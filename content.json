[{"title":"概率统计小记","date":"2017-06-22T02:53:49.000Z","path":"2017/06/22/Statistics/","text":"似然函数知乎上的一些理解wiki 最大似然估计知乎wiki 最大后验概率最大似然估计 vs 最大后验概率","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"统计学","slug":"统计学","permalink":"http://yoursite.com/tags/统计学/"}]},{"title":"统计学习方法笔记(五) —— 决策树","date":"2017-06-19T08:41:25.000Z","path":"2017/06/19/Decision-Tree/","text":"决策树是一种基本的分类与回归方法，其模型呈树形结构，在分类问题中表示基于特征对实例进行分类的过程，可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布； 其主要优点是模型具有可读性，分类速度快； 学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型，预测时，对新的数据，利用决策树模型进行分类； 决策树学习的三个主要步骤 特征选择 决策树的生成 决策树的剪枝 常用决策树算法 ID3 C4.5 CART Notes:关于损失函数最小化可以回顾第一章节的模型选择部分的内容。 决策树模型定义分类决策树模型是一种描述对实例进行分类的树形结构，其中包含两种类型的节点 内部节点：表示一个特征（属性） 叶节点：表示一个类 if-then规则集合 一条由根节点到叶节点的路径 –&gt; 一条规则 路径上内部节点的特征 –&gt; 规则的条件 叶节点的类 –&gt; 规则的结论 性质：互斥且完备 条件概率分布给定特征条件下类的条件概率分布 决策树的学习 决策树学习本质上是从训练数据集中归纳出一组分类规则，另一个角度，学习是由训练数据集估计条件概率模型 目的：得到一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力 策略：以损失函数（通常为正则化的极大似然函数）为目标函数的最小化，并在损失函数确定后，选择最优决策树 学习算法： 理论上：从所有可能的决策树中选取最优决策树，NP完全问题 实际中：采用启发式方法，近似求解（得到次最优决策树）–&gt; 递归的选择最优特征，并根据该最优特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。 主要步骤： 特征选择 决策树的生成 决策树的剪枝 Notes: 决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。 特征选择 实质：选取对于训练数据具有分类能力的特征（决定用哪个特征来划分特征空间） 常用准则 信息增益 –&gt; ID3 信息增益比 –&gt; C4.5 基尼指数 –&gt; CART Notes: 分类能力强即表示给定一个特征使得实例能较准确地被分类，及减少了实力的不确定性，掌握了更多的信息（信息增益角度的理解） 每次挑一个特征（列），根据该特征下各记录的不同取值来划分实例点（过程让我联想起k-nn中kd树的构建）–&gt; 所以一般要求特征值为离散值，或者事先对特征进行离散化处理 信息增益定义$$g(D, A)=H(D)-H(D|A)$$ $g(D, A)$即信息增益，表示得知特征$A$的信息而使得类$D$的信息的不确定性减少的程度 $H(D)$为集合$D$的经验熵 其中假设$D$是一个取有限个值的离散随机变量，概率分布为$P(X=x_i)=p_i, i=1, 2,…,n$ 熵是表示随机变量不确定性的度量，定义$H(D)=- \\sum_{i=1}^n p_ilogp_i$，熵越大，随机变量的不确定性就越大，$0 \\leq H(D) \\leq logn$ $H(D|A)$即经验条件熵表示在已知随机变量$A$（特征）的条件下随机变量$D$的不确定性$H(D|A)= \\sum_{i=1}^{n}p_iH(D|A=a_i)$ 一般将熵$H(D)$与条件熵$H(D|A)$之差称为互信息，决策树学习中的信息增益等价于训练数据集中类与特征的互信息。 小结 给定训练数据集$D$和特征$A$，经验熵$H(D)$表示对数据集$D$进行分类的不确定性，而经验条件熵$H(D|A)$表示在特征$A$给定的条件下对数据集进行分类的不确定性，因此两者之差即信息增益表示由于特征$A$而使得数据集$D$的分类的不确定性减少的程度。 对于数据集$D$而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益，信息增益大的特征具有更强的分类能力（也就是我们需要挑选的目标） 算法 输入：训练数据集$D$和特征$A$； 输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$ (1) 计算数据集$D$的经验熵$H(D)$，$$H(D)=- \\sum_{k=1}^{K}\\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|}$$ (2) 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$,$$H(D|A)= \\sum_{i=1}^{n} \\frac{|D_i|}{|D|}H(D_i)=- \\sum_{i=1}^{n} \\frac{|D_i|}{|D|} \\sum_{k=1}^{K} \\frac{|D_{ik}|}{|D_i|}log_2 \\frac{|D_{ik}|}{|D_i|}$$ (3) 计算信息增益$g(D, A)$,$$g(D|A)=H(D)-H(D|A)$$其中假设训练数据集$D$，$|D|$表示样本个数，设有$K$个类$C_k$, $|C_k|$表示属于类$C_k$的样本个数，根据特征$A$的$n$个不同的取值将训练数据集$D$划分为$n$个子集$D_1, D_2,…,D_n$，其中$|D_i|$表示子集$D_i$中的样本数，记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$ Notes:上述过程是一次特征挑选的过程，首先计算原始数据的经验熵，然后对每一个特征（轮流判断），首先根据该特征的所有不同取值将原始数据进行划分，得到的数据子集分别计算该子集的经验熵，然后按照该子集本身样本占比加权（按概率）加和起来作为经验条件熵，两相作差即为该特征对应的信息增益值，而在计算所有特征对应的信息增益之后，我们选出其中信息增益最大的特征作为分割的特征对数据集进行划分，而且后面涉及到决策树的生成过程就是一个递归的对子数据集挑选最佳特征然后划分数据集的过程。 如果觉得抽象可以参照原书P62的例子，或者参考《机器学习实战》中决策树章节的ID3算法的实现就能大概知道是什么情况了。 信息增益比 以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比可以对这一问题进行校正。 信息增益比定义特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比：$$g_R(D,A)= \\frac{g(D,A)}{H_A(D)}$$其中$$H_A(D)=- \\sum_{i=1}^{n} \\frac{|D_{i}|}{|D|}log_2 \\frac{|D_{i}|}{|D|}$$$n$为特征$A$的取值个数，$D_i$表示据特征$A$的取值将$D$分成的子集 决策树的生成ID3核心思想 在决策树的各个结点上应用信息增益准则选择特征，递归地构建决策树。 递归终止条件：所有特征的信息增益（设置信息增益的阈值来判断是否进一步划分）均很小或没有特征可以选择（没选择一个特征则后期划分子树不再使用前面使用过的特征，因为子树已经是在该特征下属于同一取值的实例集合）为止。 ID3相当于用极大似然法进行概率模型的选择。 算法 输入：训练数据集$D$和特征集$A$，阈值$\\varepsilon$； 输出：决策树$T$ (1) （叶子结点）若$D$中所有实例属于同一类$C_k$,则$T$为单节点树，并将类$C_k$作为该结点的类标记，返回$T$ (2) (终止条件之没有特征可供选择)若$A= \\emptyset$,则$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记（多数表决规则），返回$T$ (3) 否则计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$ (4) (终止条件之阈值)若$A_g$的信息增益小于阈值$\\varepsilon$，则置$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$ (5) 否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将$D$划分为若干非空子集$D_i$，并将$D_i$中实例数最大的类作为标记构建子节点，返回$T_i$ (6) 对第$i$个子节点，以$D_i$为训练集，$A- \\lbrace A_g \\rbrace$为特征集，递归地调用(1)~(5)得到子树$T_i$并返回。 C4.5C4.5算法与ID3算法类似，不同之处在于，C4.5在生成的过程中，用信息增益比来选择特征。 Notes:上述决策树的生成算法只有树的生成，而且是针对训练集构造的树，容易产生过拟合。 决策树的剪枝过拟合 过拟合产生的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树，解决该问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。 剪枝定义在决策树学习中将已生成的树进行简化的过程 实现决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现 决策树的损失函数$$C_\\alpha(T)= \\sum_{t=1}^{|T|}N_tH_t(T)+ \\alpha|T|$$其中，树$T$的叶节点数为$|T|$，叶节点$t$有$|N_t|$个样本点，其中属于$k$类的数目为$|N_{tk}|$个 其中经验熵$H_t(T)$为$$H_t(T)=- \\sum_{k=1}^{K} \\frac{|N_{tk}|}{|N_t|}log_2 \\frac{|N_{tk}|}{|N_t|}$$ 令$C(T)=\\sum_{t=1}^{|T|}N_tH_t(T)$，则损失函数可表示为$$C_\\alpha(T)=C(T)+\\alpha|T|$$$C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$表示模型的复杂度，参数$\\alpha$控制两者之间的影响 Notes: 剪枝就是当$\\alpha$确定时，选择损失函数最小的模型，及损失函数最小的子树。 利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。 算法 输入：生成算法产生的整棵树$T$，参数$\\alpha$ 输出：修剪后的子树$T\\alpha$ (1)计算每个结点的经验熵 (2)递归地从叶节点向上回缩:设叶节点回缩到其父节点之前与之后的整体树分别为$T_B$和$T_A$，如果其对应的损失函数有：$$C_{\\alpha}(T_A) \\leq C_{\\alpha}(T_B)$$则进行剪枝，即将父节点变为新的叶结点(关于这里叶节点的类别标记应该仍是以多数表决的方法)。 (3)返回(2)直至不能继续，得到损失函数最小的子树$T\\alpha$。 Notes:在向上回缩过程中判断损失函数值大小时，计算可以在局部进行，所以决策树的剪枝算法可以由一种动态规划的算法实现 CART(classification and regression tree)算法与ID3,C4.5的区别 CART假设决策树是二叉树，而ID3,C4.5生成的过程中并无此假设，这也导致了两者的根本不同，ID3,C4.5每次选择出最佳特征之后，是按照该特征的每一个取值划分子树；而CART则是对每一个特征、每一个特征的每一个取值计算基尼指数（分类树）然后从所有特征、所有特征对应的取值计算所得的基尼指数中最小的特征及特征值作为切分点来划分子树，而划分依据则是判断实例对应特征的值是否等于该选定的特征值 子树划分（特征选择）的准则不同 回归树，平方误差最小化准则 分类树，基尼指数最小化准则 CART生成回归树理解设$X, Y$分别为输入和输出变量，其中$Y$为连续变量，给定训练数据集$D= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $ 一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值，所以我们的主要目的是要构建回归树，也就是如何划分输入空间，因为一旦划分好输入空间，如将输入空间划分为$M$个单元$R_1, R_2,…,R_M$,并且在每个单元$R_m$上有一个固定的输出值$c_m$，那么回归树的模型就可以表示为$$f(x)=\\sum_{m=1}^Mc_mI(x \\in R_m)$$ 如何划分输入空间？仍然是采用启发式的方法（尝试），假设我们取$j$的某个值$s$来划分实例点，就可以根据各实例点对应该特征的取值与选定的特征值$s$比较大小来划分，即$$R_1(j, s)= \\lbrace x|x^{(j)} \\leq s \\rbrace; R_2(j, s)= \\lbrace x|x^{(j)} &gt; s \\rbrace$$ 如何选择最优切分点$s$和对应的切分变量$j$?逐一计算和比较，比较的标准是平方误差最小化，即$$min_{j,s} \\left[ min_{c_1} \\sum_{x_i \\in R_1(j,s)} (y_i-c_1)^2 + min_{c_2} \\sum_{x_i \\in R_2(j,s)} (y_i-c_2)^2 \\right]$$,这里的$c_1, c_2$就是各个区域的输出，那么各个区域的输出又是怎么得到的呢？ 如何得到各区域的输出？仍然是依照平方误差最小准则，计算$\\sum_{x_i \\in R_m}(y_i-f(x_i))^2$，所以计算得到的单元$R_m$上的$c_m$的最优值刚好是$R_m$上所有输入实例$x_i$对应的输出$y_i$的均值，即$$\\hat{c_m}=ave(y_i|x_i \\in R_m)$$ 小结一下，通过计算各区域的实例对应的输出的均值可以得到当前尝试的划分点和划分变量划分的各区域的输出，从而可以计算和比较不同的切分点和切分变量下的误差，并进一步根据平方误差最小化准则从中选取出最优的切分点和切分变量，而确定了切分点和切分变量就相当于确定了区域的一个划分，接下来针对已划分的区域进一步根据需求进行划分或者停止划分就最终将输入空间划分为多个区域，而每个区域对应有一个确定的输出值，也就是构建好了一颗回归树 算法 输入：训练数据集$D$ 输出：回归树$f(x)$在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树 (1) 选择最有切分点$s$和切分变量$j$，求解$$min_{j,s} \\left[ min_{c_1} \\sum_{x_i \\in R_1(j,s)} (y_i-c_1)^2 + min_{c_2} \\sum_{x_i \\in R_2(j,s)} (y_i-c_2)^2 \\right]$$遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小的$s, j$ (2) 用选定的$s, j$划分区域并决定相应的输出值：$$R_1(j, s)= \\lbrace x|x^{(j)} \\leq s \\rbrace; R_2(j, s)= \\lbrace x|x^{(j)} &gt; s \\rbrace$$$$\\hat{c_m}=ave(y_i|x_i \\in R_m)$$ (3) 继续对两个子区域调用步骤(1),(2)直至满足停止条件 (4) 将输入空间划分为$M$个区域$R_1, R_2,…,R_M$，生成决策树$$f(x)=\\sum_{m=1}^Mc_mI(x \\in R_m)$$ 分类树分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点 基尼指数 定义分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$,则概率分布的基尼指数定义为$$Gini(p)=\\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^Kp_k^2$$对于给定的样本集合$D$,其基尼指数为$$Gini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|}\\right)^2$$,其中，$C_k$为$D$中属于第$k$类的样本子集，$K$是类的个数 特征$A$的条件下，集合$D$的基尼指数$$Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$$，其中样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1, D_2$两部分$$D_1=\\lbrace (x, y) \\in D|A(x)=a \\rbrace; D_2=D-D_1$$ 基尼指数$Gini(D)$表示集合$D$的不确定性，基尼指数$Gini(D,A)$表示经$A=a$分割后集合$D$的不确定性，基尼指数值越大，样本集合的不确定性也就越大。 算法 输入：训练数据集$D$，停止计算的条件（如结点中样本个数小于预定阈值，或样本集的基尼指数小于预定阈值(样本基本属于同一类)，或者没有更多的特征） 输出：CART决策树从根结点开始，递归地对每个结点进行以下操作： (1) 设结点的训练数据集为$D$，计算现有特征对该数据集的基尼指数，对每一个特征$A$，对其可能的每一个取值$a$，根据样本点对$A=a$的测试为“是”或“否”将$D$分割成$D_1$和$D_2$两部分，计算$A=a$时的基尼指数。 (2) 在所有可能的特征$A$以及它们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。 (3) 对两个节点递归地调用(1),(2)直至满足停止条件 (4) 生成CART决策树 CART剪枝基本原理 首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个子树序列$\\lbrace T_0,T_1,…,T_n \\rbrace$ 然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树 剪枝过程理解 基本原理如上所述，就是由先前生成的CART决策树开始逆向剪枝得到一系列不同的子树集合，然后使用另一份验证集来对上述子树测试选出其中损失函数最小的子树 而由于我们根据损失函数最小来选取决策树，而由损失函数的定义$$C_\\alpha(T)= \\sum_{t=1}^{|T|}N_tH_t(T)+ \\alpha|T|$$而我们主要是通过调整$\\alpha$的值来权衡模型对训练集的预测能力和模型复杂度，因此对于固定的$\\alpha$必然存在使损失函数$C_\\alpha(T)$最小的子树，而极端情况下，前面根据CART生成算法得到的决策树$T_0$可认为是模型复杂度最大时的树，即此时可认为$\\alpha=0$，而当$\\alpha=+\\infty$时对应的是以根结点为单节点树的简单模型 如此一来对于每一个$\\alpha$的值，存在一个不同的子树模型，我们可以通过慢慢的从原始生成树中剪去叶节点并收集新得到的决策树，最后可以得到一个决策树的集合，而这个修剪和收集的过程，为了保证收集到完备的决策树序列，我们每次在原始生成树的基础上比较每个结点的剪枝前后的损失函数减少程度（即比较以该节点为根的节点的模型的损失与以该节点为单节点的模型的损失之差），并选取损失函数减少程度最小的一个子树部分，从原始生成树中剪去该子树得到新的子树保存到子树序列，并作为进一步剪枝的起始子树，直到最后得到以根节点为单节点模型。 算法 输入：CART算法生成的决策树$T_0$ 输出：最优决策树$T_{\\alpha}$ (1) 设$k=0, T=T_0$ (2) 设$\\alpha = +\\infty $ (3) 自下而上对各内部结点$s$计算$C(T_t), |T_t|$，以及$$g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1}$$$$\\alpha=min(\\alpha,g(t))$$其中$T_t$表示以$t$为根节点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶节点个数 (4) 对$g(t)=\\alpha$的内部节点$t$进行剪枝，剪去$g(t)$最小的子树，并对叶节点$t$以多数表决法决定其类，得到树$T$ (5) 设$k=k+1, \\alpha_k=\\alpha, T_k=T$ (6) 若$T_k$不是由根结点及两个叶结点构成的树，就返回(3)，否则令$T_k=T_n$(即最后一次剪枝得到根结点为单节点模型) (7) 采用交叉验证在剪枝得到的子树序列$T_0,T_1,…,T_n$中选取最优子树$T_{\\alpha}$ 决策树生成算法ID3的实现按照机器学习实战第三章节的内容实现了简单的ID3算法 特征选择 计算熵 计算条件熵 计算信息增益 递归构建决策树并使用matplotlib实现决策树的可视化 使用pickle序列化保存生成的决策树模型 实例：使用决策树模型预测隐形眼镜的类型具体实验代码见github","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"decision tree","slug":"decision-tree","permalink":"http://yoursite.com/tags/decision-tree/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"}]},{"title":"统计学习方法笔记(四) —— 朴素贝叶斯法","date":"2017-06-09T11:59:44.000Z","path":"2017/06/09/naive-bayes/","text":"朴素贝叶斯法是基于贝叶斯定理和特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布，然后基于此模型，对给定的输入$x$，利用贝叶斯定理求出后验概率最大的输出$y$。 关键词：贝叶斯定理、特征条件独立假设、实现简单、学习预测效率高、生成学习方法 基本方法 贝叶斯定理 特征条件独立假设 贝叶斯定理若$X$是定义在输入空间$R^n$上的随机向量，$Y$是定义在输出空间$\\lbrace c_1, c_2,…, c_K \\rbrace$上的随机变量，$P(X, Y)$是$X$和$Y$的联合概率分布，训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $由$P(X, Y)$独立同分布产生。 后验概率现对于实例$x$需要判断其所属分类，即我们需要知道x属于哪个分类的概率最大，用公式表示就是求取$$argmax_{c_k} P(Y=c_k | X=x)$$即后验概率，其中$k=1, 2,…, K$表示所有可能分类，而对于求解$P(Y=c_k | X=x)$，根据贝叶斯定理，我们需要知道训练数据的： 先验概率分布$$P(Y=c_k), k = 1,2,…,K$$ 条件概率分布，即每种分类下各个实例特征取不同值的概率 $$P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)},…,X^{(n)}=x^{(n)}|Y=c_k)$$然后根据贝叶斯定理进行计算求得后验概率 贝叶斯定理$$P(Y|X)=\\frac{P(X,Y)}{P(X)}=\\frac{P(X|Y)P(Y)}{\\sum P(Y)P(X|Y)}$$即$$P(Y=c_k|X=x)=\\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_k P(Y=c_k)P(X=x|Y=c_k)}$$但是如果直接这么求解，由于条件概率分布涉及到大量的参数，所以其估计在实际中是不可行的，所以我们在这里进行了一个大胆的假设 条件独立性假设假设用于分类的特征在类确定的条件下都是条件独立的，所以对于条件分布，我们有$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},…,X^{(n)}=x^{(n)} | Y=c_k)=\\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$这一假设使朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率 如此一来我们就可以得到最终的朴素贝叶斯分类器$$y=argmax_{c_k}P(Y=c_k) \\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$ 后验概率最大化后验概率最大化等价于期望风险最小化，证明如下 参数估计极大似然估计学习意味着估计先验概率和条件概率 先验概率的极大似然估计$$P(Y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)}{N}$$ 条件概率的极大似然估计$$P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl}, y_i=c_k)}{\\sum_{i=1}^{N}I(y_i=c_k)}$$其中假设第$j$个特征$x^{(j)}$可能取值的集合为$\\lbrace a_{j1}, a_{j2},…,a_{jS_j}$，$j=1, 2,…,n; l=1, 2,…,S_j; k=1,2,…,K; x_i^{(j)}$是第$i$个样本的第$j$个特征；$a_{jl}$为第$j$个特征可能取的第$l$个值，$I$为指示函数 算法 输入：训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $，其中$x_i=(x_i^{(1)}, x_i^{(2)},…,x_i^{(n)})^T, x_i^{(j)}$是第$i$个样本的第$j$个特征，$x_i^{(j)} \\in \\lbrace a_{j1}, a_{j2},…,a_{jS_j}$，$a_{jl}$为第$j$个特征可能取的第$l$个值，$j=1, 2,…,n; l=1, 2,…,S_j; y_i \\in \\lbrace c_1, c_2,…, c_K \\rbrace $; 实例$x$； 输出：实例$x$的分类 (1) 计算先验概率$$P(Y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)}{N}, k = 1,2,…,K$$计算条件概率$$P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl}, y_i=c_k)}{\\sum_{i=1}^{N}I(y_i=c_k)}$$ (2) 对于给定的实例$x=(x^{(1)}, x^{(2)},…,x^{(n)})^T$，计算$$P(Y=c_k)\\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$ (3) 确定实例$x$的类$$y=argmax_{c_k}P(Y=c_k)\\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$ 贝叶斯估计由于用极大似然估计可能会出现索要估计的概率值为0的情况(条件概率的某种取值的统计量为0)，会影响到后验概率的计算结果，使分类产生偏差，我们可以采取的做法是在随机变量各个取值的频数上赋予一个正数$\\lambda$，即贝叶斯估计，此时 条件概率的贝叶斯估计为$$P_{\\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl}, y_i=c_k)+ \\lambda}{\\sum_{i=1}^{N}I(y_i=c_k)+S_j \\lambda}$$ 先验概率的贝叶斯估计$$P_{\\lambda}(Y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)+\\lambda}{N+K\\lambda}$$常取$\\lambda=1$称为拉普拉斯平滑 Note:朴素贝叶斯法中假设输入变量都是条件独立的，如果假设他们之间存在概率依存关系，模型就变成了贝叶斯网络。 实现——机器学习实战按照机器学习实战第四章节的内容实现了基础的朴素贝叶斯分类器不同于k-nn和感知机将实力硬性的划分到某一类别，朴素贝叶斯分类器是返回某一实例属于某类别的概率，此外，朴素贝叶斯分类器是一种生成模型，在数据较为缺少的情况下依然有效。 本实验主要是针对文档的分类 根据文档中的词构建词集、词袋子 涉及文档的词向量的解析、提取 将文档中每一个词作为特征，计算先验概率、条件概率 实际应用中使用对数处理了下溢出等问题 实例 使用朴素贝叶斯分类器过滤侮辱性留言 使用朴素贝叶斯分类器过滤垃圾邮件 使用朴素贝叶斯分类器从个人广告中获取地域倾向具体实验代码见github","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"k-NN","slug":"k-NN","permalink":"http://yoursite.com/tags/k-NN/"}]},{"title":"剑指offer阅读笔记","date":"2017-06-09T02:33:40.000Z","path":"2017/06/09/offer/","text":"本文主要是个人针对《剑指offer》一书的一些笔记，并不涉及具体的题目或者方法，只是作为一个整体的知识点框架梳理，便于查缺补漏。 辅助网站 剑指offer第二版作者源代码（C++实现） 第一版面试题Java实现 牛客网上配套练习 第一章面试形式&amp;流程 电面 ——&gt; 远程 ——&gt; 现场 尽可能用形象化的语言把细节描述清楚 不确定问题时，主动提问 编程习惯&amp;调试能力 编程前理清思路 命名、缩进习惯 测试在前、开发在后(编程前尽可能考虑多个测试用例) 项目介绍 STAR模型 —— 简短项目背景，突出自己所做的工作和成绩 技术面5种素质 扎实的基础知识 高质量代码 分析问题时思路清晰 能优化时间、空间效率 学习、沟通能力 基础 编程语言——熟悉程度 数据结构：链表、树、栈、队列、哈希表 算法： 查找、排序：二分查找、归并排序、快排 动态规划、贪心 高质量代码 鲁棒性——检查空指针 特殊输入 边界条件 异常处理 Note: 考虑本身结构的取值范围 输入变量的可能取值 开发之前多想一些测试用例 分析&amp;思路——针对复杂问题 画图使抽象问题形象化 举例使抽象问题具体化 分解使复杂问题简单化 效率 首先要知道如何分析效率 数值各种数据结构的优缺点、并能选择合适的数据结构解决问题 熟练掌握常用的算法 软技能 沟通能力——思路、逻辑、合作 学习能力：读书、新概念 知识迁移能力 抽象建模能力 发散思维能力 关于提问（技术面） 忌 不要问与自己职位无关的问题 不要问薪水（HR面再详谈） 不要打听面试结果 荐 问与应聘职位或项目相关的问题 事先搜集应聘职位、项目背景相关信息 面试中留心面试官说过的话 第二章 —— 基础知识编程语言语言面试的3种类型 概念的理解：eg:关键字（特点及使用场合） 分析代码运行结果 写代码 数据结构 数组&amp;字符串 —— 基本 链表&amp;树 —— 常考，代码鲁棒性 栈 —— 递归 队列 —— 广度优先搜索 算法与数据操作 实现方式：循环或递归 排序&amp;查找：重点（重中之重：二分查找、归并排序、快速排序） 回溯法 —— 递归、栈、二维数组（矩阵）、迷宫问题 动态规划 分析：自上而下 ——&gt; 递归 实现：自下而上 ——&gt; 循环 涉及动态规划求解问题的四个特点 问题目标是求其最优解 整体问题的最优解依赖于各个子问题的最优解 将大问题分解为若干小问题之后，小问题之间还存在相互重叠的更小的公共子问题 自上而下分析问题，自下而上求解问题 贪婪 分解子问题时存在特殊选择 证明是最优解——需要较强的数学功底 位运算：与、或、异或、左移、右移 重要结论：将一个整数减去1后再与原来的整数做位与运算，得到的结果相当于把原整数的二进制表示中的最右边的1变成0 常考点：涉及统计二进制表示中1的个数","tags":[{"name":"Coding","slug":"Coding","permalink":"http://yoursite.com/tags/Coding/"},{"name":"剑指offer","slug":"剑指offer","permalink":"http://yoursite.com/tags/剑指offer/"},{"name":"面试","slug":"面试","permalink":"http://yoursite.com/tags/面试/"},{"name":"校招","slug":"校招","permalink":"http://yoursite.com/tags/校招/"}]},{"title":"统计学习方法笔记(三) —— K近邻","date":"2017-06-06T12:12:28.000Z","path":"2017/06/06/knn/","text":"k近邻算法概述 给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类 算法 输入：训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $，其中$x_i \\in R^n$为实例的特征向量,$y_i \\in \\lbrace c_1, c_2,…, c_K \\rbrace$ 为实例的类别,$i=1, 2, 3…, N$； 输出：实例$x$所属的类$y$ 根据所给的距离度量，在训练集中找到与$x$最近的k个点，涵盖这k个点的$x$的邻域记作$N_k(x)$； $N_k(x)$中根据分类决策规则（常用多数表决）决定$x$的类别$y$ $$y = argmax_{c_j} \\sum_{x_i \\in N_k(x)} I(y_i = c_j), i=1, 2,…, N; j=1, 2,…, K$$ 其中$I$为指示函数，当$y_i = c_j$时$I$为1，否则为0 Note: k近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型” k=1时称为最近邻算法 k近邻法没有显式的学习过程 k近邻模型——对特征空间的划分 k近邻法中，当训练集、距离度量、k值以及分类决策规则确定后，对于任何一个新的输入实例，它所属的类唯一地确定，这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。 k近邻法的三个基本要素k值的选择 k值 学习的近似误差 估计误差 特点 较小 下降 上升 整体模型变得复杂，对紧邻的实例点变得非常敏感，容易发生过拟合 较大 上升 下降 模型变得简单，但与输入实例较远的点也会对输入实例点产生影响 Note: 实际应用中，k值一般取一个比较小的数值，通常采用交叉验证法来选取最优的k值。 另外关于近似误差和估计误差，网上没有找到让我满意的答案，目前的一点理解如下： k值越小，学习的近似误差(approximation error)越小，估计误差(estimation error)越大，反之则相反 个人感觉这里有点像偏差和方差的区别，是否可以理解为近似误差即偏差，在k值较小时，选择的邻域范围较小，所以在空间内切割的“比较细”，但与此同时导致模型更加复杂，对于近邻的实例点非常敏感，而估计误差理解为方差，也就是说划分的邻域范围较大，所以平均下来根据分类决策规则可以减小邻域内的噪音点的影响，但是范围大的同时也会产生较远的点对于实例点也产生影响 搜索到的一些结果比如linian2763的博客 估计误差（estimation error）:度量预测结果与最优结果的相近程度近似误差（approximation error）:度量与最优误差之间的相近程度 此外还有stackexchange上的讨论，但是看得更加迷糊。 距离度量——实例点相似程度的反应常用距离度量 欧氏距离 $L_p$距离 Minkowski距离 其中对于$L_p$距离，设特征空间为n维实数向量空间$R^n$，$x_i, x_j \\in R^n$，$x_i = (x_i^{(1)}, x_i^{(2)},…, x_i^{(n)})^T, x_j = (x_j^{(1)}, x_j^{(2)},…, x_j^{(n)})^T$，则$x_i, x_j$的$L_p$距离定义为 $$L_p(x_i,x_j)=(\\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p)^{\\frac{1}{p}}$$ 其中$p \\geq 1$，且当$p=2$时转化为欧氏距离，$p=1$时转化为曼哈顿距离 Note: 不同的距离度量所确定的最近邻点是不同的 分类决策规则常用多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类，多数表决规则等价于经验风险最小化 实现方法——kd树(k维树) 特征空间维数大 训练数据容量大如何对训练数据进行快速k近邻搜索 线性扫描：耗时，不可行 使用特殊的结构存储训练数据，以减少计算距离的次数——kd树 构造kd树概述 kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构 Note: kd树是二叉树 kd树的每个结点对应于一个k维超矩形区域，而各实例点存在于不同的超矩形区域内，即在kd树的不同结点里 一般这个k维就是数据实例点的维度，即特征数，每次选定一个特征，然后根据范围内的实例点（记录）的该特征的值来进行二分，过程有点像快排里的分割 算法 输入：k维空间数据集$T= \\lbrace x_1, x_2,…, x_N \\rbrace $，其中$x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(k)})^T, i=1,2,…,N$； 输出：kd树 （1）开始：构造根结点，根结点对应于包含T的k维空间的超矩形区域。 选择$x^{(1)}$为坐标轴，以T中所有实例的$x^{(1)}$坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。由根结点生成深度为1的左、右结点，左子结点区域的$x^{(1)}$坐标对应的值小于切分点的子区域；右子结点区域的$x^{(1)}$坐标对应的值大于切分点的子区域。将落在切分超平面上的实例点保存在根结点。 （2）重复：对于深度为j的结点，选择$x^{(1)}$为切分的坐标轴，$l=(j \\mod k) + 1$，以该结点区域中所有实例的$x^{(1)}$坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。由该结点生成深度为j+1的左、右结点，左子结点区域的$x^{(1)}$坐标对应的值小于切分点的子区域；右子结点区域的$x^{(1)}$坐标对应的值大于切分点的子区域。将落在切分超平面上的实例点保存在该结点。 （3）直到两个区域没有实例存在时停止。 Note: 书中的方法是轮流按每个维度对某结点进行切分，各个维度（特征）可能会被重复利用来进行切分，当然此时的实例点范围不同，切分点也不同（中位数发生变化） 对于kd构造的实例，其一书中的例子比较直观，如下此外，在wiki里的一幅三维空间的配图也是非常直观 一个三维k-d树。第一次划分（红色）把根节点（白色）划分成两个节点，然后它们分别再次被划分（绿色）为两个子节点。最后这四个子节点的每一个都被划分（蓝色）为两个子节点。因为没有更进一步的划分，最后得到的八个节点称为叶子节点。 搜索kd树 利用kd树可以省去对大部分的数据点的搜索，从而减少搜索的计算量 算法 输入：已构造的kd树；目标点x； 输出：x的最近邻 （1）在kd树中找到包含目标点x的叶结点：思想有点像二分查找，从根节点出发，递归地向下访问kd树，若目标点x当前维的坐标小于切分点的坐标，则移动到左子节点，否则移动到右子节点，直到子节点为叶节点为止 （2）以此叶节点为“当前最近点” （3）递归地向上回退，在每个节点执行以下操作： （a）如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点” （b）当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。即以目标点为球心，以目标点与当前最近点间的距离为半径的球体是否与另一子结点对应的区域相交。如果相交，可能在另一个子结点对应的区域中存在更近的点。移动到另一个子结点，接着递归地进行最邻近搜索。如果不相交，则向上回退。 （4）当回退到根结点，搜索结束，此时的当前最近结点即为x的最邻近点。 Note: 若实例点随机分布，kd树搜索的平均计算复杂度是O(log N)，N为训练实例数 kd树更适用于训练实例数远大于空间维数时的k近邻搜索 关于这个算法的理解最好还是看书上的例子3.3，基本思路是一个重复二分查找和回溯的过程，另外关于kd树的搜索，有一篇很不错的文章，详见kd树的细致图文讲解 实现——机器学习实战按照机器学习实战关于k-NN的章节实现了原始的k-NN分类器，并做了几个小实例： 首先是一个基础的原理理解，对于二维空间的点按照坐标之间的距离进行分类，意图是理解k-NN的基本原理，即基于实例的学习算法，按照距离分类 在实现了原始分类器的基础上构建小应用，使用约会网站的异性数据，针对三项特征来判断新输入的异性是否有魅力 同样基于原始分类器，针对二进制点阵构成的数字图像文本文件进行识别和分类 上述三个例子都是使用的同一个原始分类器，不同之处仅仅是输入的数据有所改变，此外为了使输入数据能够被原始分类器处理，针对不同数据的特征进行了不同的处理： 针对约会网站数据由于各项特征的取值范围不同可能对于距离计算产生影响所以进行了归一化操作 对于图像文本文件将其由32*32的二进制点阵转换为1*1024的向量便于分类器处理 此外还有一些基础的文件格式化为矩阵以及可视化操作 原始的Jupyter Notebook可以参考我的github","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"k-NN","slug":"k-NN","permalink":"http://yoursite.com/tags/k-NN/"}]},{"title":"统计学习方法笔记(二) —— 感知机","date":"2017-06-03T08:53:02.000Z","path":"2017/06/03/Perceptron/","text":"概述 感知机（perceptron）是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值。感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于判别模型。感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此导入了基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。感知机预测是用学习得到的感知基模型对新的输入实例进行分类。感知机是神经网络与支持向量机的基础。关键字：二分类，线性分类模型，分离超平面，判别模型，基于误分类的损失函数，随机梯度下降法 感知机模型——分离超平面数学表达$$f(x) = sign(w \\cdot x + b)$$ 输入空间：$R^n$ 输出空间：$\\lbrace+1, -1 \\rbrace$ 假设空间：${f|f(x) = w \\cdot x + b}$，表示定义在特征空间中的所有线性分类模型或线性分类器 $w \\in R^n$，其中$w$表示权值向量，几何意义为超平面的法向量 $b \\in R$，$b$表示偏置，几何意义为超平面的截距 Note:模型学习的目的在于通过训练集求得模型的参数$w$和$b$ 几何解释 感知机的几何解释：线性方程$w \\cdot x + b = 0$ 对应特征空间$R^n$中的一个超平面$S$,其中$w$是超平面的法向量，$b$是超平面的截距，这个超平面将特征空间划分为两个部分，位于两部分的点（特征向量）分别被分为正、负两类，因此，超平面$S$称为分离超平面。 感知机的学习策略数据集的线性可分性给定一个数据集$T={(x_1, y_1), (x_2, y_2),…,(x_N, y_N)}$,其中$x_i \\in R^n$,$y_i \\in \\lbrace+1, -1 \\rbrace$,$i=1, 2, 3…, N$，若存在超平面$S$设为$w \\cdot x + b = 0$将数据集的正实例点和负实例点完全正确地划分到$S$两侧，则称数据集$T$为线性可分数据集 学习策略 目标：求得一个能将训练集正实例点和负实例点完全正确分开的分类超平面——&gt;确定$w, b$ 转化：经验损失函数最小化——基于误分类 直观思路：误分类点总数，非$w, b$的连续可导函数，不易优化 修改：误分类点到超平面$S$的总距离$$L(w, b)=-\\sum_{x_i \\in M} y_i(w \\cdot x_i + b)$$,其中$M$为误分类点的集合，损失函数$L(w, b)$是$w, b$的连续可导函数。 Note: 感知机的学习策略是在假设空间中选取是损失函数最小的模型参数$w, b$，即感知机模型。 点到平面的距离推导 范数的通俗解释 另外关于范数、规范化的理解，这篇博文写的深入浅出 感知机学习算法原始形式概述 输入：训练集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $，其中$x_i \\in R^n$,$y_i \\in \\lbrace+1, -1 \\rbrace$,$i=1, 2, 3…, N$，学习率$\\eta (0&lt; \\eta \\leq 1)$ 输出：$w, b$;感知机模型$f(x) = sign(w \\cdot x + b)$ (1) 选取初值$w_0, b_0$ (2) 在训练集中选取数据$(x_i, y_i)$ (3) if $y_i(w \\cdot x_i + b) \\leq 0$ $w \\leftarrow w + \\eta y_i x_i$ $b \\leftarrow b + \\eta y_i $ (4) 转至(2)，直至训练集中没有误分类点 几何解释当一个实例点被误分类，即位于分离超平面的错误的一侧时，则调整$w, b$的值，是分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。 随机梯度下降法 首先任意选取一个超平面$w_0, b_0$，然后用随机梯度下降法不断地极小化目标函数 $$L(w, b)= - \\sum_{x_i \\in M} y_i(w \\cdot x_i + b)$$ 极小化过程中不是一次使$M$中的所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降 若误分类点集合M固定，损失函数的梯度计算 $$\\frac{\\partial L(w,b)}{\\partial w}=- \\sum_{x_i \\in M}y_i x_i$$ $$\\frac{\\partial L(w,b)}{\\partial b}=- \\sum_{x_i \\in M}y_i$$ 随机选取一个误分类点$(x_i, y_i)$对$w, b$进行更新： $$w \\leftarrow w + \\eta y_i x_i$$ $$b \\leftarrow b + \\eta y_i $$ 通过迭代可使损失函数$L(w, b)$不断减小，直到为0。 算法的收敛性证明如下 另外可以看看其他人写的证明过程 Note: 当训练数据集线性可分时，感知机学习算法存在无穷多个解，其解由于不同的初值或不同的迭代顺序而可能有所不同。 对偶形式基本思想将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得$w, b$。 假设初始值$w_0, b_0$均为0，对误分类点$(x_i, y_i)$，通过 $$w \\leftarrow w + \\eta y_i x_i$$ $$b \\leftarrow b + \\eta y_i $$ 逐步修改$w, b$，假设修改$n$次，则$w, b$可分别表示为： $$w= \\sum_{i=1}^{N} \\alpha_i y_i x_i$$ $$b= \\sum_{i=1}^{N} \\alpha_i y_i$$ 每个实例点对应有一个$\\alpha_i$满足$\\alpha_i \\geq 0$且当$\\eta = 1$时，$\\alpha_i$就表示第$i$个实例点由于误分类而进行更新的次数。 实例点更新次数越多，意味着它距离分离超平面越接近，也就越难以正确分类，换言之，这种实例对学习结果影响最大（有点支持向量机中的支持向量的意思）。 算法 输入：线性可分的数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $，其中$x_i \\in R^n$,$y_i \\in \\lbrace +1, -1 \\rbrace$,$i=1, 2, 3…, N$，学习率$\\eta (0&lt; \\eta \\leq 1)$ 输出：$\\alpha, b$；感知机模型$f(x) = sign(\\sum_{j=1}^{N} \\alpha_j y_j x_j \\cdot x + b)$,其中$\\alpha=(alpla_1, alpla_2,…,alpla_N)^T$(1): $\\alpha \\leftarrow 0$, $b \\leftarrow 0$(2): 计算所有样本内积形成的Gram矩阵$G$， $G=[x_i \\cdot x_j ]_{N \\times N}$ (3): 在训练集中选取数据$(x_i, y_i)$，若 $y_i (\\sum_{j=1}^{N} \\alpha_j y_j x_j \\cdot x_i + b) \\leq 0$ 计算过程中可通过查$G$的值来提高效率）则更新： $$\\alpha_i \\leftarrow \\alpha_i + \\eta$$ $$b \\leftarrow b + \\eta y_i$$(4): 转至(3)直至没有误分类数据 Note:为什么要引入对偶形式？ 首先原始形式中，书中为何要使用随机梯度下降而非批量梯度下降法，个人搜索到的一篇博文里感觉说的有点道理，引用如下 用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，只有误分类的M集合里面的样本才能参与损失函数的优化。所以我们不能用最普通的批量梯度下降,只能采用随机梯度下降（SGD）或者小批量梯度下降（MBGD）。 此外，关于对偶形式的优势，小结一下： 对偶形式将权重向量$w$转化为实例$x_i$和标记$y_i$的线性组合形式，且在原书中也提到，对偶形式中的训练实例仅以内积的形式出现，所以可以预先使用Gram矩阵存储，也就是时间换空间的方法提高计算效率 书中这里应该也有点为后续介绍支持向量机做铺垫，所以这里为核函数的引入埋一个伏笔，毕竟书中提到感知机是神经网络与支持向量机的基础，而且后面书中在支持向量机部分的讲解也多次使用对偶形式的求解","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"Perceptron","slug":"Perceptron","permalink":"http://yoursite.com/tags/Perceptron/"}]},{"title":"统计学习方法笔记(一)","date":"2017-06-02T06:42:52.000Z","path":"2017/06/02/Note-StatisticalML/","text":"本系列文章是个人根据阅读李航博士的《统计学习方法》一书，辅以《机器学习实战》、scikit-learn官方文档等材料整理出来的笔记。 统计学习相关概念定义 统计学习(statistical learning)是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测和分析的一门学科 研究对象数据 研究目的对数据进行预测和分析：学习什么样的模型和如何学习模型（准确、效率） 研究方法构建模型并应用模型进行预测和分析（统计学习方法三要素） 模型：模型的假设空间，即学习模型的集合 策略：模型选择的准则 算法：模型学习的算法，求解最优模型的算法 分类 监督学习 非监督学习 半监督学习 强化学习 监督学习概览 输入空间：输入所有可能取值的集合 输出空间：输出所有可能取值的集合 特征空间： 每个具体的输入是一个实例，通常由特征向量表示，所有特征向量存在的空间称为特征空间 有时输入空间与特征空间为相同的空间，不予区分，有时为不同的空间，需要将实例从输入空间映射到特征空间，模型实际上都是定义在特征空间上 假设空间：学习模型的集合（假设要学习的模型属于某个函数集合），模型属于由输入空间到输出空间的映射的集合，这个集合即假设空间，假设空间的确定意味着学习范围的确定。 监督学习的模型 概率模型：条件概率分布$P(Y|X)$ 非概率模型：决策函数$Y=f(X)$ 联合概率分布监督学习假设输入与输出的随机变量X和Y遵循联合概率分布$P(X, Y)$(监督学习关于数据的基本假设) 监督学习分类 回归问题：输入变量与输出变量均为连续变量的预测问题。 分类问题：输出变量为有限个离散变量的预测问题。 标注问题：输入变量和输出变量均为变量序列的预测问题。 问题形式化学习与预测——训练与测试 学习：利用给定的训练数据集，通过学习（训练）得到一个模型； 预测：对于给定的测试样本中的输入，由所得到的模型给出相应的输出。 统计学习方法三要素(针对监督学习)模型学习什么样的模型，在监督学习过程中，模型指的就是所要学习的条件概率分布或决策函数。 策略（评价标准）模型选择的准则：按照什么样的准则学习或选择最优的模型 损失函数（代价函数）：一次预测的好坏的度量，常用的包括0-1损失函数、平方损失函数、绝对损失函数、对数(似然)损失函数 风险函数（期望损失）：度量平均意义下模型预测的好坏 经验风险 结构风险 将监督学习的问题转化为经验风险或结构风险函数的最优化问题（结构风险最小的模型即最优模型） Note:在模型选择中，理论上应该使用期望风险函数作为标准，但是由于输入，输出的基本假设$P(X, Y)$联合分布未知（若已知则可直接求解P(Y|X)，不用学习），所以试图使用经验风险（平均损失）来替代，但又由于现实中限于样本容量不会很大，所以要对经验风险进行矫正，于是有了结构风险，在经验风险的基础上添加正则化项来防止过拟合。 算法（求解&amp;优化）模型学习的算法，即学习模型的具体计算方法 用什么样的计算方法求解最优模型（寻找全局最优解） 如何高效实现 模型选择模型评估训练误差&amp;测试误差 训练误差：对判断给定的问题是不是一个容易学习的问题是有意义的，但若一味追求减小训练误差，会出现过拟合的情况（所选模型的复杂度比真模型高） 测试误差：反映了学习方法对未知的测试数据集的预测能力，测试误差小的方法具有更好的预测能力所以需要选择复杂度适当的模型，而模型选择常用的两种方法分别是正则化和交叉验证。 正则化结构风险最小化策略的实现，在经验风险的基础上加一个正则化项/罚项 $$ R_{srm}(f)=\\frac{1}{N}\\sum L(y_i,f(x_i))+\\lambda J(f) $$ 作用是为了选择经验风险和模型复杂度同时较小的模型(在所有可能选择的模型中，能够很好地解释已知数据并且十分简单的，才是最好的模型) 交叉验证重复的使用数据 数据集划分 训练集：训练模型，通过输入数据学习函数中的参数值，得到拟合了数据的“分类器/回归器”等 验证集：模型选择，从不同的模型中选择最佳模型 从不同类的模型中选择（比如从SVM,GBM,决策树里选择模型） 从同类模型不同超参数(hyperparameter)组合里选择最优超参数组合。 测试集：最终对学习方法的评估常用方法 简单交叉验证 S折交叉验证 留一交叉验证 Note:关于数据集划分的一点理解理解可以参看交叉验证，而关于模型选择可以进一步了解参数与超参数 学习的泛化能力定义：学习方法的泛化能力(generalization ability)是指由该方法学习到的模型对未知数据的预测能力。评价方法 理论上：泛化误差即期望风险$$ R_{exp}(f)=E_p[L(Y, f(X))]=\\int L(y,f(x))P(x,y)dxdy $$ 实际应用中：常常用测试误差衡量 泛化误差上界通过比较两种学习方法的泛化误差上界来比较其优劣（越小越好） 样本容量越大，泛化误差上界越小 假设空间越大，泛化误差上界越大 训练误差小的模型，其泛化误差也会小 生成模型与判别模型监督学习模型的一般形式 概率模型：条件概率分布$P(Y|X)$ 非概率模型：决策函数$Y=f(X)$ 监督学习方法 - 生成方法 判别方法 定义 由数据学习联合概率分布$P(X, Y)$,然后求出$P(Y&#124;X)$作为预测的模型,$P(Y&#124;X)=\\frac{P(X,Y)}{P(X)}$ 由数据直接学习决策函数$f(x)$或条件概率分布$P(Y&#124;X)$作为预测模型 特点 1.可还原出$P(X, Y)$；2.学习收敛速度更快；3.存在隐变量时仍可用 1.直接面对预测，准确率更高；2.便于数据抽象，特征定义和使用，可简化学习问题 典型模型 朴素贝叶斯法、隐马尔可夫模型 k-近邻、感知机、决策树、逻辑斯谛回归模型、最大熵模型、SVM、提升方法、条件随机场等 Note 模型表示了给定输入X产生输出Y的生成关系 判别方法关心的是对给定的输入X，应该预测什么样的输出Y 监督学习方法的应用分类问题在监督学习中，当输入变量Y取有限个离散值时，预测问题便成为分类问题 多类分类问题，包括二分类 评价指标 分类准确率 精确率&amp;召回率（二分类）——正类、负类、F1值 标注问题可以认为是分类问题的一种推广，输入观测序列，输出标记序列(状态序列) 学习&amp;标注——条件概率分布 评价指标 标注准确率 精确率、召回率 常用统计学习方法 隐马尔可夫模型 条件随机场 应用领域 信息抽取 自然语言处理 回归问题回归用于预测输入变量（自变量）和输出变量（因变量）之间的关系——函数拟合 学习&amp;预测 一元回归 vs 多元回归 线性回归 vs 非线性回归 损失函数：常用平方损失函数—— 最小二乘法","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"}]},{"title":"关于交叉验证和模型选择的一点思考","date":"2017-06-01T12:28:36.000Z","path":"2017/06/01/cv/","text":"首先按照《统计学习方法》第一章的内容，常用的模型选择方法有两种，按照结构风险最小化思想的具体实现即正则化以及数据驱动的交叉验证方法。 CV是用于模型的(评估)挑选和超参数(关于超参数和参数的区别请见另一篇memo)调整的，首先明确一点，所谓模型，指的是我们用来描述输入数据与最终需要预测的输出数据之间的关联的方法，而不是不同数据拟合的模型得到的实例，比如我们可以说一个线性回归模型，但是不会说用不同数据拟合的线性回归器为不同的模型； 为什么要用CV，这涉及到数据划分的问题，首先我们训练好一个模型后需要评估这个模型的效果，但是如果把全部数据都投入训练就没有数据来进行验证了，常规而言，可以将数据集划分为训练集和验证集比如80%训练、20%验证，但是这样会有一些问题，首先有可能你很不巧的将一些特殊数据划分到这20%的验证集里了(要么效果很好、要么效果很糟糕)，这样的话仅仅这么一组验证集的评估效果就很不稳定也不确切；此外我们划分训练集和验证集就减少了训练数据，而一般而言数据越多训练出的模型方差是越小的，从这个角度来看数据越多一般我们训练的效果更好； 那么有没有方法既可以用上所有数据进行验证和评估、最后又可以用所有数据进行训练呢？所以我们就需要交叉验证，比如五折交叉验证，假设我们仍将数据集8-2划分（80%-20%），对于五折交叉验证，我们就是针对模型进行了五次训练，每次取80%训练数据、20%验证数据，并最终保证每个数据都曾在这五次训练验证中作为20%的验证集对模型进行过评估，这样我们就可以确保我们使用了所有数据对我们的模型进行评估 为什么不用CV中得到的预测器进行预测？一般而言，在做交叉验证时确实可能出现某一组交叉验证的得分较高，我们会试图用这一组数据进行模型的拟合和最终预测，但是这种得分高只是一种表面现象，首先数据量少了，其次这一组验证集并非独立的，是在整个交叉验证中随机生成的，所以这个预测器的结果到底好不好还需要在对额外的数据进行测试，如果数据重组也许你可以用嵌套式的交叉验证进行实验，即第一步用常规内层交叉验证确定最佳模型，然后采用数据驱动的方式(外层交叉验证)拟合最佳的一组预测器 所以CV的作用是用来对不同模型（SVM和生成树等），不同参数的模型中评估各个模型的效果，而得到最佳参数组合的模型；接下来将所有训练集投入进去拟合预测得到拟合好的预测器最后对测试集进行预测 那么把所有训练集放到模型中会不会导致过拟合呢？答案是不会，过拟合产生的原因是模型过于复杂（模型的参数），而不是数据增加导致，（而不是传入参数的值），增加数据一般而言更有利于训练集的训练 补充：关于生成树模型中的early_stopping，按照《统计学习方法》一书的第12章总结部分P213所述 提升方法没有现实的正则化项，通常通过早停止(early stopping)的方法达到正则化的效果 所以early_stopping属于正则化的范畴，是另一种模型选择的具体方法。 参考资料https://stats.stackexchange.com/a/52277/152084http://scikit-learn.org/stable/modules/cross_validation.htmlhttps://stats.stackexchange.com/a/52312/152084","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"CV","slug":"CV","permalink":"http://yoursite.com/tags/CV/"},{"name":"model selection","slug":"model-selection","permalink":"http://yoursite.com/tags/model-selection/"}]},{"title":"机器学习中模型的参数和超参数","date":"2017-06-01T12:24:11.000Z","path":"2017/06/01/parameter-hyperparameter/","text":"一直以来对于机器学习中的模型训练和模型选择存在一个误区，首先机器学习力的模型通俗来说就是一个函数关系，表明输入数据到输出数据的映射，基本的假设前提是输入数据和输出数据符合某种联合概率分布，而模型训练的过程其实就是在确定函数式的具体参数值的过程，比如假设你要做一个多项式回归分析的模型，比如$f(x)=w_1x_1+w_2x_2+w_3x_3$，那么模型训练的过程中其实就是在学习对应的w的值，那么问题来了，实战中所谓的模型调参来选择模型又指的是什么呢？既然训练已经把参数都确定下来了，那我们调整的参数又是什么？原来这里有个误区在于模型中的parameter和hyperparameter的区别，按照搜集到的资料来看，其实模型中可以分为两种参数，一种是在训练过程中学习到的参数，即parameter也就是上面公式里的w，而另一种参数则是hyperparameter，这种参数是模型中学习不到的，是我们预先定义的，而模型的调参其实指的是调整hyperparameter，而且不同类型的模型的hyperparameter也不尽相同，比如SVM中的C,树模型中的深度、叶子数以及比较常规的学习率等等，这种参数是在模型训练之前预先定义的，所以关于模型的选择其实更多的指的是选择最佳的hyperparameter组合。 参考资料https://datascience.stackexchange.com/a/14234/31117https://www.quora.com/What-are-hyperparameters-in-machine-learning","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"memo","slug":"memo","permalink":"http://yoursite.com/tags/memo/"},{"name":"parameter","slug":"parameter","permalink":"http://yoursite.com/tags/parameter/"},{"name":"hyperparameter","slug":"hyperparameter","permalink":"http://yoursite.com/tags/hyperparameter/"}]},{"title":"计算机与生活","date":"2017-05-30T11:57:01.000Z","path":"2017/05/30/computer-life/","text":"阅读完吴军的《浪潮之巅》，给人的感觉颇有点当初看纪录片《互联网时代》的感觉，简单而言就是你透过作者的眼睛快速浏览了一遍计算机这个行业的发展历程；手头的这本是第二版，看出版时间也是2012年了，到现在算算有点老了，不过对于12年以前发生的一些事情还是能有一个大致的梳理； 这本书上册主要讲述了一些在历史中唱过主角的公司，关于这些公司的兴衰以及作者自己的一些分析判断，下册则更宽泛一点，除了谈谈部分公司，还科普了一些计算机工业界以及商业方面的概念，比如谈信息产业的一些规律，一些公司的商业模式甚至风险投资，金融风暴等等； 下面我想简单谈一谈自己读过这本书的一点点体会，关于计算机和人类的生活。 我粗略的将计算机的发展历程划分为四个大的阶段，分别是计算时代—自动化时代—互联网时代—数据时代；计算时代比较有代表性的是计算机ENIAC，那个时候还主要用作军事用途，用来计算导弹轨迹之类的，离走进我们的生活还比较远，第二阶段自动化时代比较有代表性的就是微机了（当然这里跳过了面向企业的工作站，主要谈与个人生活相关的），按照作者的说法，这一阶段最有代表性的是三个公司，苹果的个人电脑（一体化）和微软与Intel的联盟（WinTel体系），这一阶段计算机真正开始走入普通人的家庭生活，主要是用于协助处理一些日常的办公工作，属于单机阶段；然后是大家比较熟悉的互联网时代，这一阶段的强大之处在于将计算机连接起来，于是每个用户都不再是孤立的，随着因特网的出现，有一部分人将本地的内容放到了网上，这个时候网上的内容不多，但是比较杂乱无序，所以随之诞生了门户网站比如雅虎等等，他们将互联网上的内容分门别类从而使用户便于查看和获取信息；而随着建立网站的门槛降低，互联网上的内容越来越多，单纯靠人工分类的的门户网站已经满足不了人们的需求了，于是搜索引擎（代表是Google）应运而生，帮助用户在浩瀚的互联网中快速找到自己所需的信息； 如果说互联网时代的关键词是‘内容’；那么数据时代的关键词就是平台了。这个时候，人们已经不满足于只是从互联网上获取信息了，就像不满足于只是从电视接受固定的节目一样，用户开始有‘发出自己的声音’的诉求，也就是创作；于是各个平台相继出现，最有代表性的，早期的Blog可以允许用户自己在网上发布自己的文章，然后是Facebook、Twitter不仅为用户构建了虚拟的社交圈，而且让用户可以在这个圈子里随时随地发出自己的声音并与其他人互动，还有YouTube把平台交给用户，让用户自己去发布自己的视频和收看其他用户的视频；这一阶段的特点是‘平台’ ，比较有代表性的公司并不提供内容，而是让用户自己来做内容的创造者。 而随之而来的是数据的爆发，互联网上的信息变得更加庞杂，在不断炒来炒去的概念‘云计算’，‘大数据’的驱动下，计算机的同学们纷纷投身到数据分析/数据挖掘的工作中，希望能够从庞杂的数据中挖掘出与实际业务相关的有价值的信息；那么，下一步人们的需求在哪呢？是通过数据挖掘对一个人建立画像从而实现对每个人的私人定制服务？还是现在炙手可热的基于VR/AR的虚拟社交？按照前段时间看到的一个关于区块链的演讲，在未来每个人都可以被“数字化”，真实世界的你可以投影到虚拟世界成为一个数字化的、独一无二的你，这大概是一种趋势吧。 最后还是推荐下这本书，作为科普类的读物挺不错的。","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"IT","slug":"IT","permalink":"http://yoursite.com/tags/IT/"}]},{"title":"来来来，我教你搭个博客好不好哇","date":"2017-05-30T10:40:58.000Z","path":"2017/05/30/my-blog/","text":"先把我的博客贴一下Lancelot’s Desert 端午节两天时间宅实验室把自己搭建一个博客这么个‘历史遗留问题’解决了下，其实之前也用Hugo搭建过，最后给弄崩了，这次尝试下Hexo，发现倒是异常的顺利，一方面Hexo的整体生态比较完备，另一方面网上找到的教程也很靠谱，下面就把我搭建过程中的一些步骤和踩过的坑记录下。 基础博客搭建首先声明下，基础博客搭建基本上和我所参照的崔斯特的教程是一致的，只是在顺序和语言上重新组织了一下，因为原博主写的教程已经很简明扼要了。 准备工作 git下载 github账号 node.js 另外的话写博客最好熟悉一点markdown的基本语法，常用的指令不多而且很简单，用的熟练是非常不错的工具，这里也推荐一个markdown的不错的在线编辑器Cmd markdown即时预览的，此外，类似简书、SegmentFault这些网站也是支持MarkDown编辑的，所以如果熟悉这个语法的话还是很方便的。 下载和注册基础搭建的话首先需要下载git和node.js(npm)来进行控制台命令的一些输入，以及需要一个github账号，因为我们的博客是挂载在github上的。 在下载好git和node.js并注册好一个github账号后，首先新建一个repository，名称和你的账户名一致，后面添加.github.io域名，比如我的用户名是LancelotHolmes,那么我的repository命名就是LancelotHolmes.github.io 环境配置在完成上述操作并安装好git和node.js之后，我们选择一个路径新建一个文件夹比如我是在D盘的MyBlog，然后执行git bash，可以在开始里搜索，也可以右键然后选择git bash here,在出现的git控制台中输入之前注册的github账号相对应信息，比如1git config --global user.name \"你的账户名\" 1git config --global user.email \"注册github账号时的邮箱\" 如图 然后是安装Hexo,直接输入 1npm install -g hexo-cli 开始搭建同样在MyBlog(或者你命名的目录下)，仍然是刚刚的控制台界面，输入1hexo init blog 成功的时候会显示 1INFO Start blogging with Hexo! 接下来进入blog目录下，输入123hexo cleanhexo ghexo s 或者你也可以新建一个generate.sh脚本文件将上面三条语句写入,因为后面线下测试会多次用到，可以直接在控制台输入./generate.sh，然后在浏览器里输入http://localhost:4000/,这个时候你就可以看到一个网页的基本雏形，这是由于你的\\blog\\source\\_posts路径下已经有一个基本的markdown文件了，而且本身下载的Hexo带了一个landscape的主题文件，在路径\\blog\\themes下可以看到 配置githubSSH回到控制台，现在我们需要生成SSH，仍然是在git控制台里，输入 1ssh-keygen -t rsa -C \"Github的注册邮箱地址\" 基本是一直回车，到出现信息1Your public key has been saved in /c/Users/user/.ssh/id_rsa.pub. 在信息对应的路径下找到这个文件，打开它(我是用sublime text打开的)，在你的github界面，右上角头像里选择Settings,左侧选择SSH and GPG keys然后新建一个SSH key,名称随你定，比如我是设置的blog,内容的话把刚刚id_rsa.pub里的内容全选复制进去就好了。 站点配置在blog目录下，用sublime或者其他编辑器打开_config.yml文件，找到对应的字段修改下面的基本参数信息，这里需要注意一下存在多个_config.yml文件，一个是在blog目录下，作为站点配置文件，一般用来做一些常规的配置，另外在各个主题的目录下还有一个_config.yml文件用来进行特定主体的一些个性化设置，后面会经常用到这两个文件，另外就是下面的配置注意:之后的空格 博客基本信息1234title: 博客名称subtitle: 副标题description: 网页描述author: 作者名 推送设置（这里的repo注意修改为自己的github的对应格式）1234deploy: type: git repo: https://github.com/LancelotHolmes/LancelotHolmes.github.io.git branch: master 新建文章在控制台输入 1hexo n \"文章名称\" 这样会在\\blog\\source\\_posts目录下生成对应的markdown文件，你可以通过编辑器打开它并书写你的文章，比如保存后同样的执行./generate.sh然后打开http://localhost:4000/就可以看到你刚刚写好的的文章了。 当然我这里展示的界面略有不同，因为我这里设置主题，后面会具体介绍。 推送到github线下测试发现没有什么问题我们就可以推送到github了，输入如下命令，或者保存为脚本文件deploy.sh然后执行./deploy.sh第一次部署到github时可能会出错error deployer not found:github，可以在控制台输入,注意--save必不可少1npm install hexo-deployer-git --save 12hexo cleanhexo d -g 过程中会让你输入你的github账号和密码，推送成功后，你就可以通过在浏览器输入你的静态站点名称访问你的博客了，比如LancelotHolmes.github.io,至此一个基本的博客就搭好了，接下来你每次需要写文章只需要经过如下步骤 在blog路径下打开git bash控制台然后输入hexo n &quot;文章名&quot; 在路径\\blog\\source\\_posts中编辑对应的markdown，编辑好后保存 执行generate.sh进行线下预览（可选） 执行deploy.sh推送到github就可以通过你的站点访问啦 配置yilia主题前面也给大家看到了我的博客的截图，这里我使用的是yilia主题，目前Hexo主题里面比较热门的两大主题是Next和yilia,这里我就我所配置的一些功能和踩过的坑记录一下。主要包括一些基本的配置以及优化 基本配置基本的主题下载和安装可以直接参照yilia的github对应的教程，同样在blog目录下执行git bash控制台，输入1git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 然后修改站点配置文件，Hexo根目录下(即blog目录的)的 _config.yml1theme: yilia 然后再该文件末尾添加如下语句123456789101112131415161718jsonContent: meta: false pages: false posts: title: true date: true path: true text: true raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true 添加disqus评论目前使用的评论比较多，我早期Hugo时使用过disqus也就还是用这个了，主要是需要注册一个disque账号，然后修改主题目录下的_config.yml 文件，特别注意这里的路径是\\blog\\themes\\yilia,不再是blog下的文件，后面大部分配置都是针对这个文件1disqus: LancelotHolmes 这里的修改为你注册的disqus的short-name,yilia主题下的配置会优先覆盖blog下的配置，如果你是使用其他的评论比如多说，顺言之类的应该适时修改相应的字段后面的false为对应的值，效果如下 添加menu原始的主题menu是这样的123menu: 主页: / 随笔: /tags/随笔/ ，我们可以修改为我们所需要的‘类别’，注意由于yilia作者没有预先设置类别（category）而是把他当作tags使，所以这里配置时链接到对应的tags下的路径，修改如下1234menu: 主页: / 阅读: /tags/Reading/ 机器学习: /tags/ML/ 可以根据需要在新建文章是设置标签自动生成对应的路径，可以在\\blog\\public\\tags下看到，然后修改menu下的对应字段即可 RSS &amp; Sitemap为什么要用RSS,可以看这篇短文，主要是为了方便对你的博客感兴趣的人将你的博客添加到他的订阅列表中，一旦你有更新他可以在第一时间接收到推送。而sitemap则主要是给搜索引擎用的，方便你的站点能够被google收录，当然这里首先需要绑定一个域名，后续我们会具体介绍。这部分主要是参照voidking和magicwangs的博客，执行如下语句123npm install hexo-generator-feed --savenpm install hexo-generator-sitemap --save 然后照常的执行generate.sh，你可以在路径\\blog\\public下看到生成的文件atom.xml和sitemap.xml,接下来在主题目录下的_config.yml 文件，特别注意这里的路径是\\blog\\themes\\yilia里添加123456# SubNavsubnav: github: \"#\" weibo: \"#\" rss: /atom.xml ... 此外在blog目录下的站点配置文件里(这回是在\\blog路径下的_config.yml)添加如下语句1234567891011# Sitemapsitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xm# RSSfeed: type: atom path: atom.xml limit: 100 头像设置有采用本地图片的，也有将图片传到网上制成外链的，我就是用后面那种方法，这里给大家推荐一个不错的工具sm.ms，方便你把你的本地图片传到网上制成markdown、html、url等格式的外部链接制作好后，修改主题目录下的_config.yml 文件，注意这里的路径是\\blog\\themes\\yilia里 12#你的头像urlavatar: https://ooo.0o0.ooo/2017/05/29/592be5c575c4c.jpg 链接当然是你刚刚生成的url的链接 其他社交外链同样是修改主题目录下的_config.yml 文件，注意这里的路径是\\blog\\themes\\yilia里，位置与之前的rss的地方差不多，根据你想展示的社交平台设置，注意链接是你的社交平台的主页之类的，比如1234# SubNavsubnav: github: \"https://github.com/LancelotHolmes/\" weibo: \"http://weibo.com/2925991784/profile?topnav=1&amp;wvr=6\" 文章截断直接生成的文章在主页会全部显示，如果不处理会占据较大的篇幅，我们可以在文章的特定位置设置文章截断，这样主页展示的就是部分文字，首先仍然是修改主题目录下的_config.yml 文件，注意这里的路径是\\blog\\themes\\yilia里 1234# Content# 文章太长，截断按钮文字excerpt_link: more 然后在你的文章的md文件里你想要阶段的位置插入语句1&lt;!-- more --&gt; 例如 这样就实现了文章的截断，而需要阅读全文只需要点击相应的按钮或者标题即可 favicon这个主要是为了好玩，就是你的网页打开后的在浏览器的上面的一个小图标，比如github和我的博客的favicon 我是在freefavicon上直接选的一个，如果你有兴趣可以自己制作1616的图片就行了，将图片复制到路径\\blog\\public下，然后在*主题目录下的_config.yml 文件里修改即可 1favicon: /favicon.png 文章目录这个是作者目前没有实现的部分，但是有其他的方法，主要参照的是这个post,修改主要涉及这么两个文件 在/themes/yilia/layout/_partial/article.ejs文件里18行左右的位置插入 1234567891011121314&lt;% if (!index)&#123; %&gt; &lt;% if (toc(post.content))&#123; %&gt; &lt;div id=\"toc\" class=\"article-toc\"&gt; &lt;h2&gt;目录&lt;/h2&gt; &lt;%- toc(post.content) %&gt; &lt;/div&gt; &lt;script type=\"text/javascript\"&gt; var _article = document.getElementsByClassName('article')[0]; &lt;!-- setTimeout(\"_article.style.marginRight = '211px'\",0); --&gt; setTimeout(\"_article.className += ' article2'\",0); setTimeout(\"document.getElementById('toc').style.right = '15px'\", 0); &lt;/script&gt; &lt;% &#125; %&gt; &lt;% &#125; %&gt; 在\\themes\\yilia\\source-src\\css\\article.scss文件的末尾添加 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@media (max-width: 1099px)&#123; #toc&#123; display: none; &#125; &#125; @media (min-width: 1100px) &#123; #toc&#123; z-index: 999; background-color: #fff; padding: 0 1em; border:1px solid #ddd; position: fixed; top: 100px; right: -180px; transition: right .5s ease-in; width: 150px; h2&#123; margin-bottom:10px; &#125; ol&#123; padding-left: 0!important; &#125; line-height: 1.3em; font-size: 0.8em; float: right; .toc&#123; padding: 0; li&#123; list-style-type: none; margin: .5em 0 .5em; ol&#123; margin: .5em 0 .5em 1em; &#125; &#125; &#125; &#125; .article2&#123; margin-right: 211px; transition: margin-right .5s ease-in; &#125;&#125;.toc-item span &#123;display: table-cell;&#125;span.toc-text &#123;padding-left: 3px;&#125; 即可实现，如图如果没有效果，可以尝试在需要目录的文章头部添加如下语句123456---title: 使用机器学习识别出拍卖场中作弊的机器人用户date: 2017-05-29 19:27:51tags: [ML, Kaggle, Python]toc: true--- 以上内容基本可以搭建起一个还不错的博客了，当然如果你需要其他的一些好玩的功能比如标签云啊，浏览量、动态特效之类的就多多使用搜索引擎咯，另外，遇到什么问题大部分都能在对应的github的issues找到答案。 域名绑定最后就介绍下域名的绑定，如果你也不满足使用github的二级域名，而是想使用专属于你自己的域名，那么这部分就是为你而准备的。 这部分也是参照了不少文章，比如Setsuna和水瓶座IOSer，还有zhaozhiming下面我就简单的叙述一下。 使用工具 namesilo: 购买域名的地方，经过一番查阅感觉这个相对靠谱 DNSPod: 国内的免费DNS服务，后面将域名的dns转移到这个上面 首先要绑定域名自然需要购买一个域名，国内的话可以试试阿里云、万网，学生的话好像可以试试腾讯云的云服务器，是会送免费的.cn域名，但是国内购买的话需要去公安局备份好象，感觉比较麻烦我就是用了国外的，比如namesilo，其他的介绍可以看看zhaozhiming的对应观点，当然他和我一样也是在知乎上的一个地方看到的； 在namesilo上注册的教程可以看这个,选择好域名并付款后（支持支付宝）的基本设置可以看这个，接下来： CNAME在你的本地站点目录里的source目录下添加一个CNAME文件，不带后缀，用编辑器打开并输入你购买的域名，不要http也不要www,比如我的域名是izhaoyi.top,我就写入izhaoyi.top然后保存以后执行deploy。 DNS设置namesilo登陆namesilo之后，右上角的Manage My Domains点击进入后,选择然后下拉，选择在上面手动添加两条记录，如图然后回到域名管理界面，选中域名那一栏最前面的勾选框，然后选择上面的Change Namesevers图标最后在在NameServer1和NameServer2中填写 DNSPod 的 nameserver 地址f1g1ns1.dnspod.net，f1g1ns2.dnspod.net DNSPod同样完成注册后，在域名注册中需要手动添加你购买的域名，并添加一些记录，最终如图这里的192.30.252.153是github pages的ip地址，固定设置成这个就好，然后稍微等一段时间（也许不用等）应该就可以通过访问你的域名比如izhaoyi.top或者你之前的github站点名比如LancelotHolmes.github.io访问你的博客啦。 最后，欢迎大家来我的博客踩踩，也欢迎跟我互加友链啊~","tags":[{"name":"blog","slug":"blog","permalink":"http://yoursite.com/tags/blog/"},{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"yilia","slug":"yilia","permalink":"http://yoursite.com/tags/yilia/"},{"name":"Coding","slug":"Coding","permalink":"http://yoursite.com/tags/Coding/"}]},{"title":"使用机器学习识别出拍卖场中作弊的机器人用户(二)","date":"2017-05-29T12:52:19.000Z","path":"2017/05/29/HumenOrRobot2/","text":"原创文章，首发于SegmentFault 本文承接上一篇文章:使用机器学习识别出拍卖场中作弊的机器人用户 本项目为kaggle上Facebook举行的一次比赛，地址见数据来源，完整代码见我的github,欢迎来玩~ 代码 数据探索——Data_Exploration.ipynb 数据预处理&amp;特征工程——Feature_Engineering.ipynb &amp; Feature_Engineering2.ipynb 模型设计及评测——Model_Design.ipynb 项目数据来源 kaggle项目所需额外工具包 numpy pandas matplotlib sklearn xgboost lightgbm mlxtend: 含有聚和算法Stacking项目整体运行时间预估为60min左右，在Ubuntu系统，8G内存，运行结果见所提交的jupyter notebook文件 由于文章内容过长，所以分为两篇文章，总共包含四个部分 数据探索 数据预处理及特征工程 模型设计 评估及总结 特征工程(续)12345import numpy as npimport pandas as pdimport pickle%matplotlib inlinefrom IPython.display import display 12# bids = pd.read_csv('bids.csv')bids = pickle.load(open('bids.pkl')) 12print bids.shapedisplay(bids.head()) (7656329, 9) bid_id bidder_id auction merchandise device time country ip url 0 8dac2b259fd1c6d1120e519fb1ac14fbqvax8 ewmzr jewelry phone0 9759243157894736 us 69.166.231.58 vasstdc27m7nks3 1 668d393e858e8126275433046bbd35c6tywop aeqok furniture phone1 9759243157894736 in 50.201.125.84 jmqlhflrzwuay9c 2 aa5f360084278b35d746fa6af3a7a1a5ra3xe wa00e home goods phone2 9759243157894736 py 112.54.208.157 vasstdc27m7nks3 3 3939ac3ef7d472a59a9c5f893dd3e39fh9ofi jefix jewelry phone4 9759243157894736 in 18.99.175.133 vasstdc27m7nks3 4 8393c48eaf4b8fa96886edc7cf27b372dsibi jefix jewelry phone5 9759243157894736 in 145.138.5.37 vasstdc27m7nks3 1bidders = bids.groupby('bidder_id') 针对国家、商品单一特征多类别转换为多个独立特征进行统计1234567891011121314cates = (bids['merchandise'].unique()).tolist()countries = (bids['country'].unique()).tolist()def dummy_coun_cate(group): coun_cate = dict.fromkeys(cates, 0) coun_cate.update(dict.fromkeys(countries, 0)) for cat, value in group['merchandise'].value_counts().iteritems(): coun_cate[cat] = value for c in group['country'].unique(): coun_cate[c] = 1 coun_cate = pd.Series(coun_cate) return coun_cate 1bidder_coun_cate = bidders.apply(dummy_coun_cate) 12display(bidder_coun_cate.describe())bidder_coun_cate.to_csv('coun_cate.csv') - ad ae af ag al am an ao ar at … count 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 … mean 0.002724 0.205629 0.054774 0.001059 0.048570 0.023907 0.000303 0.036314 0.120442 0.052655 … std 0.052121 0.404191 0.227555 0.032530 0.214984 0.152770 0.017395 0.187085 0.325502 0.223362 … min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 … 25% 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 … 50% 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 … 75% 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 … max 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 … 同样的，对于每个用户需要统计他对于自己每次竞拍行为的时间间隔情况 12345678910111213141516171819def bidder_interval(group): time_diff = np.ediff1d(group['time']) bidder_interval = &#123;&#125; if len(time_diff) == 0: diff_mean = 0 diff_std = 0 diff_median = 0 diff_zeros = 0 else: diff_mean = np.mean(time_diff) diff_std = np.std(time_diff) diff_median = np.median(time_diff) diff_zeros = time_diff.shape[0] - np.count_nonzero(time_diff) bidder_interval['tmean'] = diff_mean bidder_interval['tstd'] = diff_std bidder_interval['tmedian'] = diff_median bidder_interval['tzeros'] = diff_zeros bidder_interval = pd.Series(bidder_interval) return bidder_interval 1bidder_inv = bidders.apply(bidder_interval) 12display(bidder_inv.describe())bidder_inv.to_csv('bidder_inv.csv') - tmean tmedian tstd tzeros count 6.609000e+03 6.609000e+03 6.609000e+03 6609.0 mean 2.933038e+12 1.860285e+12 3.440901e+12 122.986231 std 8.552343e+12 7.993497e+12 6.512992e+12 3190.805229 min 0.000000e+00 0.000000e+00 0.000000e+00 0.000000 25% 1.192853e+10 2.578947e+09 1.749995e+09 0.000000 50% 2.641139e+11 5.726316e+10 5.510107e+11 0.000000 75% 1.847456e+12 6.339474e+11 2.911282e+12 0.000000 max 7.610295e+13 7.610295e+13 3.800092e+13 231570.000000 按照用户-拍卖场分组进一步分析之前的统计是按照用户进行分组，针对各个用户从整体上针对竞标行为统计其各项特征，下面根据拍卖场来对用户进一步细分，看一看每个用户在不同拍卖场的行为模式,类似上述按照用户分组来统计各个用户的各项特征，针对用户-拍卖场结对分组进行统计以下特征 基本计数统计，针对各个用户在各个拍卖场统计设备、国家、ip、url、商品类别、竞标次数等特征的数目作为新的特征 时间间隔统计：统计各个用户在各个拍卖场每次竞拍的时间间隔的 均值、方差、中位数和0值 针对商品类别、国家进一步转化为多类别进行统计 123456789101112131415161718192021222324252627282930313233343536def auc_features_count(group): time_diff = np.ediff1d(group['time']) if len(time_diff) == 0: diff_mean = 0 diff_std = 0 diff_median = 0 diff_zeros = 0 else: diff_mean = np.mean(time_diff) diff_std = np.std(time_diff) diff_median = np.median(time_diff) diff_zeros = time_diff.shape[0] - np.count_nonzero(time_diff) row = dict.fromkeys(cates, 0) row.update(dict.fromkeys(countries, 0)) row['devices_c'] = group['device'].unique().shape[0] row['countries_c'] = group['country'].unique().shape[0] row['ip_c'] = group['ip'].unique().shape[0] row['url_c'] = group['url'].unique().shape[0]# row['merch_c'] = group['merchandise'].unique().shape[0] row['bids_c'] = group.shape[0] row['tmean'] = diff_mean row['tstd'] = diff_std row['tmedian'] = diff_median row['tzeros'] = diff_zeros for cat, value in group['merchandise'].value_counts().iteritems(): row[cat] = value for c in group['country'].unique(): row[c] = 1 row = pd.Series(row) return row 1bidder_auc = bids.groupby(['bidder_id', 'auction']).apply(auc_features_count) 1bidder_auc.to_csv('bids_auc.csv') 1print bidder_auc.shape (382336, 218) 模型设计与参数评估合并特征对之前生成的各项特征进行合并产生最终的特征空间 1234import numpy as npimport pandas as pd%matplotlib inlinefrom IPython.display import display 首先将之前根据用户分组的统计特征合并起来，然后将其与按照用户-拍卖场结对分组的特征合并起来，最后加上时间特征，分别于训练集、测试集连接生成后续进行训练和预测的特征数据文件 123456789101112131415161718192021222324def merge_data(): train = pd.read_csv('train.csv') test = pd.read_csv('test.csv') time_differences = pd.read_csv('tdiff.csv', index_col=0) bids_auc = pd.read_csv('bids_auc.csv') bids_auc = bids_auc.groupby('bidder_id').mean() bidders = pd.read_csv('cnt_bidder.csv', index_col=0) country_cate = pd.read_csv('coun_cate.csv', index_col=0) bidder_inv = pd.read_csv('bidder_inv.csv', index_col=0) bidders = bidders.merge(country_cate, right_index=True, left_index=True) bidders = bidders.merge(bidder_inv, right_index=True, left_index=True) bidders = bidders.merge(bids_auc, right_index=True, left_index=True) bidders = bidders.merge(time_differences, right_index=True, left_index=True) train = train.merge(bidders, left_on='bidder_id', right_index=True) train.to_csv('train_full.csv', index=False) test = test.merge(bidders, left_on='bidder_id', right_index=True) test.to_csv('test_full.csv', index=False) 1merge_data() 1234train_full = pd.read_csv('train_full.csv')test_full = pd.read_csv('test_full.csv')print train_full.shapeprint test_full.shape (1983, 445) (4626, 444) 123456789train_full['outcome'] = train_full['outcome'].astype(int)ytrain = train_full['outcome']train_full.drop('outcome', 1, inplace=True)test_ids = test_full['bidder_id']labels = ['payment_account', 'address', 'bidder_id']train_full.drop(labels=labels, axis=1, inplace=True)test_full.drop(labels=labels, axis=1, inplace=True) 设计交叉验证模型选择根据之前的分析，由于当前的数据集中存在正负例不均衡的问题，所以考虑选取了RandomForestClassfier, GradientBoostingClassifier, xgboost, lightgbm等四种模型来针对数据及进行训练和预测，确定最终模型的基本思路如下： 对四个模型分别使用评价函数roc_auc进行交叉验证并绘制auc曲线，对各个模型的多轮交叉验证得分取平均值并输出 根据得分确定最终选用的一个或多个模型 若最后发现一个模型的表现大幅度优于其他所有模型，则选择该模型进一步调参 若最后发现多个模型表现都不错，则进行模型的集成，得到聚合模型 使用GridSearchCV来从人为设定的参数列表中选择最佳的参数组合确定最终的模型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from scipy import interpimport matplotlib.pyplot as pltfrom itertools import cycle# from sklearn.cross_validation import StratifiedKFoldfrom sklearn.model_selection import StratifiedKFoldfrom sklearn.metrics import roc_auc_score, roc_curve, aucdef kfold_plot(train, ytrain, model):# kf = StratifiedKFold(y=ytrain, n_folds=5) kf = StratifiedKFold(n_splits=5) scores = [] mean_tpr = 0.0 mean_fpr = np.linspace(0, 1, 100) exe_time = [] colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue']) lw = 2 i=0 for (train_index, test_index), color in zip(kf.split(train, ytrain), colors): X_train, X_test = train.iloc[train_index], train.iloc[test_index] y_train, y_test = ytrain.iloc[train_index], ytrain.iloc[test_index] begin_t = time.time() predictions = model(X_train, X_test, y_train) end_t = time.time() exe_time.append(round(end_t-begin_t, 3))# model = model# model.fit(X_train, y_train) # predictions = model.predict_proba(X_test)[:, 1] scores.append(roc_auc_score(y_test.astype(float), predictions)) fpr, tpr, thresholds = roc_curve(y_test, predictions) mean_tpr += interp(mean_fpr, fpr, tpr) mean_tpr[0] = 0.0 roc_auc = auc(fpr, tpr) plt.plot(fpr, tpr, lw=lw, color=color, label='ROC fold %d (area = %0.2f)' % (i, roc_auc)) i += 1 plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k', label='Luck') mean_tpr /= kf.get_n_splits(train, ytrain) mean_tpr[-1] = 1.0 mean_auc = auc(mean_fpr, mean_tpr) plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--', label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw) plt.xlim([-0.05, 1.05]) plt.ylim([-0.05, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Receiver operating characteristic') plt.legend(loc='lower right') plt.show() # print 'scores: ', scores print 'mean scores: ', np.mean(scores) print 'mean model process time: ', np.mean(exe_time), 's' return scores, np.mean(scores), np.mean(exe_time) 收集各个模型进行交叉验证的结果包括每轮交叉验证的auc得分、auc的平均得分以及模型的训练时间 123dct_scores = &#123;&#125;mean_score = &#123;&#125;mean_time = &#123;&#125; RandomForestClassifier12from sklearn.model_selection import GridSearchCVimport time 12345678910from sklearn.ensemble import RandomForestClassifierdef forest_model(X_train, X_test, y_train):# begin_t = time.time() model = RandomForestClassifier(n_estimators=160, max_features=35, max_depth=8, random_state=7) model.fit(X_train, y_train) # end_t = time.time()# print 'train time of forest model: ',round(end_t-begin_t, 3), 's' predictions = model.predict_proba(X_test)[:, 1] return predictions 12dct_scores['forest'], mean_score['forest'], mean_time['forest'] = kfold_plot(train_full, ytrain, forest_model)# kfold_plot(train_full, ytrain, model_forest) mean scores: 0.909571935157 mean model process time: 0.643 s 123456from sklearn.ensemble import GradientBoostingClassifierdef gradient_model(X_train, X_test, y_train): model = GradientBoostingClassifier(n_estimators=200, random_state=7, max_depth=5, learning_rate=0.03) model.fit(X_train, y_train) predictions = model.predict_proba(X_test)[:, 1] return predictions 1dct_scores['gbm'], mean_score['gbm'], mean_time['gbm'] = kfold_plot(train_full, ytrain, gradient_model) mean scores: 0.911847771023 mean model process time: 4.1948 s 123456789import xgboost as xgbdef xgboost_model(X_train, X_test, y_train): X_train = xgb.DMatrix(X_train.values, label=y_train.values) X_test = xgb.DMatrix(X_test.values) params = &#123;'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': 1, 'seed': 7, 'max_depth': 6, 'eta': 0.01&#125; model = xgb.train(params, X_train, 600) predictions = model.predict(X_test) return predictions /home/lancelot/anaconda2/envs/udacity/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20. &quot;This module will be removed in 0.20.&quot;, DeprecationWarning) 1dct_scores['xgboost'], mean_score['xgboost'], mean_time['xgboost'] = kfold_plot(train_full, ytrain, xgboost_model) mean scores: 0.915372340426 mean model process time: 3.1482 s 1234567import lightgbm as lgbdef lightgbm_model(X_train, X_test, y_train): X_train = lgb.Dataset(X_train.values, y_train.values) params = &#123;'objective': 'binary', 'metric': &#123;'auc'&#125;, 'learning_rate': 0.01, 'max_depth': 6, 'seed': 7&#125; model = lgb.train(params, X_train, num_boost_round=600) predictions = model.predict(X_test) return predictions 1dct_scores['lgbm'], mean_score['lgbm'], mean_time['lgbm'] = kfold_plot(train_full, ytrain, lightgbm_model) mean scores: 0.921512158055 mean model process time: 0.3558 s 模型比较比较四个模型在交叉验证机上的roc_auc平均得分和模型训练的时间 123456789101112131415def plot_model_comp(title, y_label, dct_result): data_source = dct_result.keys() y_pos = np.arange(len(data_source)) # model_auc = [0.910, 0.912, 0.915, 0.922] model_auc = dct_result.values() barlist = plt.bar(y_pos, model_auc, align='center', alpha=0.5) # get the index of highest score max_val = max(model_auc) idx = model_auc.index(max_val) barlist[idx].set_color('r') plt.xticks(y_pos, data_source) plt.ylabel(y_label) plt.title(title) plt.show() print 'The highest auc score is &#123;0&#125; of model: &#123;1&#125;'.format(max_val, data_source[idx]) 1plot_model_comp('Model Performance', 'roc-auc score', mean_score) The highest auc score is 0.921512158055 of model: lgbm 123456789101112131415def plot_time_comp(title, y_label, dct_result): data_source = dct_result.keys() y_pos = np.arange(len(data_source)) # model_auc = [0.910, 0.912, 0.915, 0.922] model_auc = dct_result.values() barlist = plt.bar(y_pos, model_auc, align='center', alpha=0.5) # get the index of highest score min_val = min(model_auc) idx = model_auc.index(min_val) barlist[idx].set_color('r') plt.xticks(y_pos, data_source) plt.ylabel(y_label) plt.title(title) plt.show() print 'The shortest time is &#123;0&#125; of model: &#123;1&#125;'.format(min_val, data_source[idx]) 1plot_time_comp('Time of Building Model', 'time(s)', mean_time) The shortest time is 0.3558 of model: lgbm 12345678910111213141516171819auc_forest = dct_scores['forest']auc_gb = dct_scores['gbm']auc_xgb = dct_scores['xgboost']auc_lgb = dct_scores['lgbm']print 'std of forest auc score: ',np.std(auc_forest)print 'std of gbm auc score: ',np.std(auc_gb)print 'std of xgboost auc score: ',np.std(auc_xgb)print 'std of lightgbm auc score: ',np.std(auc_lgb)data_source = ['roc-fold-1', 'roc-fold-2', 'roc-fold-3', 'roc-fold-4', 'roc-fold-5']y_pos = np.arange(len(data_source))plt.plot(y_pos, auc_forest, 'b-', label='forest')plt.plot(y_pos, auc_gb, 'r-', label='gbm')plt.plot(y_pos, auc_xgb, 'y-', label='xgboost')plt.plot(y_pos, auc_lgb, 'g-', label='lightgbm')plt.title('roc-auc score of each epoch')plt.xlabel('epoch')plt.ylabel('roc-auc score')plt.legend()plt.show() std of forest auc score: 0.0413757504568 std of gbm auc score: 0.027746291638 std of xgboost auc score: 0.0232931322563 std of lightgbm auc score: 0.0287156755513 单从5次交叉验证的各模型roc-auc得分来看，xgboost的得分相对比较稳定 聚合模型由上面的模型比较可以发现，四个模型的经过交叉验证的表现都不错，但是综合而言，xgboost和lightgbm更胜一筹，而且两者的训练时间也相对更短一些，所以接下来考虑进行模型的聚合，思路如下： 先通过GridSearchCV分别针对四个模型在整个训练集上进行调参获得最佳的子模型 针对子模型使用 stacking: 第三方库mlxtend里的stacking方法对子模型进行聚合得到聚合模型，并采用之前相同的cv方法对该模型进行打分评价 voting: 使用sklearn内置的VotingClassifier进行四个模型的聚合 最终对聚合模型在一次进行cv验证评分，根据结果确定最终的模型 先通过交叉验证针对模型选择参数组合 12345678910def choose_xgb_model(X_train, y_train): tuned_params = [&#123;'objective': ['binary:logistic'], 'learning_rate': [0.01, 0.03, 0.05], 'n_estimators': [100, 150, 200], 'max_depth':[4, 6, 8]&#125;] begin_t = time.time() clf = GridSearchCV(xgb.XGBClassifier(seed=7), tuned_params, scoring='roc_auc') clf.fit(X_train, y_train) end_t = time.time() print 'train time: ',round(end_t-begin_t, 3), 's' print 'current best parameters of xgboost: ',clf.best_params_ return clf.best_estimator_ 1bst_xgb = choose_xgb_model(train_full, ytrain) train time: 48.141 s current best parameters of xgboost: {&apos;n_estimators&apos;: 150, &apos;objective&apos;: &apos;binary:logistic&apos;, &apos;learning_rate&apos;: 0.05, &apos;max_depth&apos;: 4} 12345678910def choose_lgb_model(X_train, y_train): tuned_params = [&#123;'objective': ['binary'], 'learning_rate': [0.01, 0.03, 0.05], 'n_estimators': [100, 150, 200], 'max_depth':[4, 6, 8]&#125;] begin_t = time.time() clf = GridSearchCV(lgb.LGBMClassifier(seed=7), tuned_params, scoring='roc_auc') clf.fit(X_train, y_train) end_t = time.time() print 'train time: ',round(end_t-begin_t, 3), 's' print 'current best parameters of lgb: ',clf.best_params_ return clf.best_estimator_ 1bst_lgb = choose_lgb_model(train_full, ytrain) train time: 12.543 s current best parameters of lgb: {&apos;n_estimators&apos;: 150, &apos;objective&apos;: &apos;binary&apos;, &apos;learning_rate&apos;: 0.05, &apos;max_depth&apos;: 4} 先使用stacking集成两个综合表现最佳的模型lgb和xgb，此处元分类器使用较为简单的LR模型来在已经训练好了并且经过参数选择的模型上进一步优化预测结果 12345678910from mlxtend.classifier import StackingClassifierfrom sklearn import linear_modeldef stacking_model(X_train, X_test, y_train): lr = linear_model.LogisticRegression(random_state=7) sclf = StackingClassifier(classifiers=[bst_xgb, bst_lgb], use_probas=True, average_probas=False, meta_classifier=lr) sclf.fit(X_train, y_train) predictions = sclf.predict_proba(X_test)[:, 1] return predictions 1dct_scores['stacking_1'], mean_score['stacking_1'], mean_time['stacking_1'] = kfold_plot(train_full, ytrain, stacking_model) mean scores: 0.92157674772 mean model process time: 0.7022 s 可以看到相对之前的得分最高的模型lightgbm，将lightgbm与xgboost经过stacking集成并且使用lr作为元分类器得到的auc得分有轻微的提升，接下来考虑进一步加入另外的RandomForest和GBDT模型看看增加一点模型的差异性使用Stacking是不是会有所提升 123456789def choose_forest_model(X_train, y_train): tuned_params = [&#123;'n_estimators': [100, 150, 200], 'max_features': [8, 15, 30], 'max_depth':[4, 8, 10]&#125;] begin_t = time.time() clf = GridSearchCV(RandomForestClassifier(random_state=7), tuned_params, scoring='roc_auc') clf.fit(X_train, y_train) end_t = time.time() print 'train time: ',round(end_t-begin_t, 3), 's' print 'current best parameters: ',clf.best_params_ return clf.best_estimator_ 1bst_forest = choose_forest_model(train_full, ytrain) train time: 42.201 s current best parameters: {&apos;max_features&apos;: 15, &apos;n_estimators&apos;: 150, &apos;max_depth&apos;: 8} 12345678910def choose_gradient_model(X_train, y_train): tuned_params = [&#123;'n_estimators': [100, 150, 200], 'learning_rate': [0.03, 0.05, 0.07], 'min_samples_leaf': [8, 15, 30], 'max_depth':[4, 6, 8]&#125;] begin_t = time.time() clf = GridSearchCV(GradientBoostingClassifier(random_state=7), tuned_params, scoring='roc_auc') clf.fit(X_train, y_train) end_t = time.time() print 'train time: ',round(end_t-begin_t, 3), 's' print 'current best parameters: ',clf.best_params_ return clf.best_estimator_ 1bst_gradient = choose_gradient_model(train_full, ytrain) train time: 641.872 s current best parameters: {&apos;n_estimators&apos;: 100, &apos;learning_rate&apos;: 0.03, &apos;max_depth&apos;: 8, &apos;min_samples_leaf&apos;: 30} 1234567def stacking_model2(X_train, X_test, y_train): lr = linear_model.LogisticRegression(random_state=7) sclf = StackingClassifier(classifiers=[bst_xgb, bst_forest, bst_gradient, bst_lgb], use_probas=True, average_probas=False, meta_classifier=lr) sclf.fit(X_train, y_train) predictions = sclf.predict_proba(X_test)[:, 1] return predictions 1dct_scores['stacking_2'], mean_score['stacking_2'], mean_time['stacking_2'] = kfold_plot(train_full, ytrain, stacking_model2) mean scores: 0.92686550152 mean model process time: 4.0878 s 可以看到四个模型的聚合效果比用两个模型的stacking聚合效果要好不少，接下来尝试使用voting对四个模型进行聚合 12345678from sklearn.ensemble import VotingClassifierdef voting_model(X_train, X_test, y_train): vclf = VotingClassifier(estimators=[('xgb', bst_xgb), ('rf', bst_forest), ('gbm',bst_gradient), ('lgb', bst_lgb)], voting='soft', weights=[2, 1, 1, 2]) vclf.fit(X_train, y_train) predictions = vclf.predict_proba(X_test)[:, 1] return predictions 1dct_scores['voting'], mean_score['voting'], mean_time['voting'] = kfold_plot(train_full, ytrain, voting_model) mean scores: 0.926889564336 mean model process time: 4.055 s 再次比较单模型与集成模型的得分 1plot_model_comp('Model Performance', 'roc-auc score', mean_score) The highest auc score is 0.926889564336 of model: voting 由上可以看到最终通过voting将四个模型进行聚合可以得到得分最高的模型，确定为最终用来预测的模型 综合模型，对测试文件进行最终预测12345678910111213141516# predict(train_full, test_full, y_train)def submit(X_train, X_test, y_train, test_ids): predictions = voting_model(X_train, X_test, y_train) sub = pd.read_csv('sampleSubmission.csv') result = pd.DataFrame() result['bidder_id'] = test_ids result['outcome'] = predictions sub = sub.merge(result, on='bidder_id', how='left') # Fill missing values with mean mean_pred = np.mean(predictions) sub.fillna(mean_pred, inplace=True) sub.drop('prediction', 1, inplace=True) sub.to_csv('result.csv', index=False, header=['bidder_id', 'prediction']) 1submit(train_full, test_full, ytrain, test_ids) 最终结果提交到kaggle上进行评分，得分如下 以上就是整个完整的流程，当然还有很多模型可以尝试，很多聚合方法也可以使用，此外，特征工程部分还有很多空间可以挖掘，就留给大家去探索啦~ 参考资料 Chen, K. T., Pao, H. K. K., &amp; Chang, H. C. (2008, October). Game bot identification based on manifold learning. In Proceedings of the 7th ACM SIGCOMM Workshop on Network and System Support for Games (pp. 21-26). ACM. Alayed, H., Frangoudes, F., &amp; Neuman, C. (2013, August). Behavioral-based cheating detection in online first person shooters using machine learning techniques. In Computational Intelligence in Games (CIG), 2013 IEEE Conference on (pp. 1-8). IEEE. https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot/data http://stats.stackexchange.com/a/132832/152084 https://en.wikipedia.org/wiki/Receiver_operating_characteristic https://en.wikipedia.org/wiki/Random_forest https://en.wikipedia.org/wiki/Gradient_boosting https://xgboost.readthedocs.io/en/latest//parameter.html#parameters-for-tree-booster https://github.com/Microsoft/LightGBM https://en.wikipedia.org/wiki/Receiver_operating_characteristic http://stackoverflow.com/questions/29530232/python-pandas-check-if-any-value-is-nan-in-dataframe http://pandas.pydata.org/pandas-docs/stable/missing_data.html http://stackoverflow.com/a/18272653/6653189 http://www.cnblogs.com/jasonfreak/p/5720137.html kaggle ensembling guide","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"使用机器学习识别出拍卖场中作弊的机器人用户","date":"2017-05-29T11:27:51.000Z","path":"2017/05/29/HumenOrRobot/","text":"原创文章，首发于SegmentFault 本项目为kaggle上Facebook举行的一次比赛，地址见数据来源，完整代码见我的github,欢迎来玩~ 代码 数据探索——Data_Exploration.ipynb 数据预处理&amp;特征工程——Feature_Engineering.ipynb &amp; Feature_Engineering2.ipynb 模型设计及评测——Model_Design.ipynb 项目数据来源 kaggle项目所需额外工具包 numpy pandas matplotlib sklearn xgboost lightgbm mlxtend: 含有聚和算法Stacking项目整体运行时间预估为60min左右，在Ubuntu系统，8G内存，运行结果见所提交的jupyter notebook文件 由于文章内容过长，所以分为两篇文章，总共包含四个部分 数据探索 数据预处理及特征工程 模型设计 评估及总结 数据探索1234import numpy as npimport pandas as pd%matplotlib inlinefrom IPython.display import display 123df_bids = pd.read_csv('bids.csv', low_memory=False)df_train = pd.read_csv('train.csv')df_test = pd.read_csv('test.csv') 1df_bids.head(3) bid_id bidder_id auction merchandise device time country ip url 0 0 8dac2b259fd1c6d1120e519fb1ac14fbqvax8 ewmzr jewelry phone0 9759243157894736 us 69.166.231.58 vasstdc27m7nks3 1 1 668d393e858e8126275433046bbd35c6tywop aeqok furniture phone1 9759243157894736 in 50.201.125.84 jmqlhflrzwuay9c 2 2 aa5f360084278b35d746fa6af3a7a1a5ra3xe wa00e home goods phone2 9759243157894736 py 112.54.208.157 vasstdc27m7nks3 12df_train.head(3)# df_train.dtypes bidder_id payment_account address outcome 0 91a3c57b13234af24875c56fb7e2b2f4rb56a a3d2de7675556553a5f08e4c88d2c228754av a3d2de7675556553a5f08e4c88d2c228vt0u4 0.0 1 624f258b49e77713fc34034560f93fb3hu3jo a3d2de7675556553a5f08e4c88d2c228v1sga ae87054e5a97a8f840a3991d12611fdcrfbq3 0.0 2 1c5f4fc669099bfbfac515cd26997bd12ruaj a3d2de7675556553a5f08e4c88d2c2280cybl 92520288b50f03907041887884ba49c0cl0pd 0.0 异常数据检测1234# 查看各表格中是否存在空值print 'Is there any missing value in bids?',df_bids.isnull().any().any()print 'Is there any missing value in train?',df_train.isnull().any().any()print 'Is there any missing value in test?',df_test.isnull().any().any() Is there any missing value in bids? True Is there any missing value in train? False Is there any missing value in test? False 整个对三个数据集进行空值判断，发现用户数据训练集和测试集均无缺失数据，而在竞标行为数据集中存在缺失值的情况，下面便针对bids数据进一步寻找缺失值 123# nan_rows = df_bids[df_bids.isnull().T.any().T]# print nan_rowspd.isnull(df_bids).any() bid_id False bidder_id False auction False merchandise False device False time False country True ip False url False dtype: bool 1234missing_country = df_bids['country'].isnull().sum().sum()print 'No. of missing country: ', missing_countrynormal_country = df_bids['country'].notnull().sum().sum()print 'No. of normal country: ', normal_country No. of missing country: 8859 No. of normal country: 7647475 123456789import matplotlib.pyplot as pltlabels = ['unknown', 'normal']sizes = [missing_country, normal_country]explode = (0.1, 0)fig1, ax1 = plt.subplots()ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)ax1.axis('equal')plt.title('Distribution of missing countries vs. normal countries')plt.show() 综合上述的分析可以发现，在竞标行为用户的country一栏属性中存在很少一部分用户行为是没有country记录的，在预处理部分可以针对这部分缺失数据进行填充操作，有两种思路： 针对原始行为数据按照用户分组后，看看每个对应的用户竞标时经常所位于的国家信息，对缺失值填充常驻国家 针对原始行为数据按照用户分组后，按时间顺序对每组用户中的缺失值前向或后向填充相邻的国家信息 12345678# 查看各个数据的记录数# 看看数据的id是否是唯一标识print df_bids.shape[0]print len(df_bids['bid_id'].unique())print df_train.shape[0]print len(df_train['bidder_id'].unique())print df_test.shape[0]print len(df_test['bidder_id'].unique()) 7656334 7656334 2013 2013 4700 4700 12345678# 简单统计各项基本特征（类别特征）的数目（除去时间）print 'total bidder in bids: ', len(df_bids['bidder_id'].unique())print 'total auction in bids: ', len(df_bids['auction'].unique())print 'total merchandise in bids: ', len(df_bids['merchandise'].unique())print 'total device in bids: ', len(df_bids['device'].unique())print 'total country in bids: ', len(df_bids['country'].unique())print 'total ip in bids: ', len(df_bids['ip'].unique())print 'total url in bids: ', len(df_bids['url'].unique()) total bidder in bids: 6614 total auction in bids: 15051 total merchandise in bids: 10 total device in bids: 7351 total country in bids: 200 total ip in bids: 2303991 total url in bids: 1786351 由上述基本特征可以看到： 竞标行为中的用户总数少于训练集+测试集的用户数，也就是说并不是一一对应的，接下来验证下竞标行为数据中的用户是否完全来自训练集和测试集 商品类别和国家的种类相对其他特征较少，可以作为天然的类别特征提取出来进行处理，而其余的特征可能更多的进行计数统计 12345lst_all_users = (df_train['bidder_id'].unique()).tolist() + (df_test['bidder_id'].unique()).tolist()print 'total bidders of train and test set',len(lst_all_users)lst_bidder = (df_bids['bidder_id'].unique()).tolist()print 'total bidders in bids set',len(lst_bidder)print 'Is bidders in bids are all from train+test set? ',set(lst_bidder).issubset(set(lst_all_users)) total bidders of train and test set 6713 total bidders in bids set 6614 Is bidders in bids are all from train+test set? True 123456lst_nobids = [i for i in lst_all_users if i not in lst_bidder]print 'No. of bidders never bid: ',len(lst_nobids)lst_nobids_train = [i for i in lst_nobids if i in (df_train['bidder_id'].unique()).tolist()]lst_nobids_test = [i for i in lst_nobids if i in (df_test['bidder_id'].unique()).tolist()]print 'No. of bidders never bid in train set: ',len(lst_nobids_train)print 'No. of bidders never bid in test set: ',len(lst_nobids_test) No. of bidders never bid: 99 No. of bidders never bid in train set: 29 No. of bidders never bid in test set: 70 12345678data_source = ['train', 'test']y_pos = np.arange(len(data_source))num_never_bids = [len(lst_nobids_train), len(lst_nobids_test)]plt.bar(y_pos, num_never_bids, align='center', alpha=0.5)plt.xticks(y_pos, data_source)plt.ylabel('bidders no bids')plt.title('Source of no bids bidders')plt.show() 1print df_train[(df_train['bidder_id'].isin(lst_nobids_train)) &amp; (df_train['outcome']==1.0)] Empty DataFrame Columns: [bidder_id, payment_account, address, outcome] Index: [] 由上述计算可知存在99个竞标者无竞标记录，其中29位来自训练集，70位来自测试集，而且这29位来自训练集的竞标者未被标记为机器人用户，所以可以针对测试集中的这70位用户后续标记为人类或者取平均值处理 12# check the partition of bots in trainprint (df_train[df_train['outcome'] == 1].shape[0]*1.0) / df_train.shape[0] * 100,'%' 5.11674118231 % 训练集中的标记为机器人的用户占所有用户数目约5% 12df_train.groupby('outcome').size().plot(labels=['Human', 'Robot'], kind='pie', autopct='%.2f', figsize=(4, 4), title='Distribution of Human vs. Robots', legend=True) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f477135c5d0&gt; 由上述训练集中的正负例分布可以看到本数据集正负例比例失衡，所以后续考虑使用AUC（不受正负例比例影响）作为评价指标，此外尽量采用Gradient Boosting族模型来进行训练 数据预处理与特征工程12345import numpy as npimport pandas as pdimport pickle%matplotlib inlinefrom IPython.display import display 123bids = pd.read_csv('bids.csv')train = pd.read_csv('train.csv')test = pd.read_csv('test.csv') 处理缺失数据针对前面数据探索部分所发现的竞标行为数据中存在的国家属性缺失问题，考虑使用针对原始行为数据按照用户分组后，按时间顺序对每组用户中的缺失值前向或后向填充相邻的国家信息的方法来进行缺失值的填充处理 1display(bids.head(3)) bid_id bidder_id auction merchandise device time country ip url 0 0 8dac2b259fd1c6d1120e519fb1ac14fbqvax8 ewmzr jewelry phone0 9759243157894736 us 69.166.231.58 vasstdc27m7nks3 1 1 668d393e858e8126275433046bbd35c6tywop aeqok furniture phone1 9759243157894736 in 50.201.125.84 jmqlhflrzwuay9c 2 2 aa5f360084278b35d746fa6af3a7a1a5ra3xe wa00e home goods phone2 9759243157894736 py 112.54.208.157 vasstdc27m7nks3 12# pd.algos.is_monotonic_int64(bids.time.values, True)[0]print 'Is the time monotonically non-decreasing? ', pd.Index(bids['time']).is_monotonic Is the time monotonically non-decreasing? False 123# bidder_group = bids.sort_values(['bidder_id', 'time']).groupby('bidder_id')bids['country'] = bids.sort_values(['bidder_id', 'time']).groupby('bidder_id')['country'].ffill()bids['country'] = bids.sort_values(['bidder_id', 'time']).groupby('bidder_id')['country'].bfill() 1display(bids.head(3)) bid_id bidder_id auction merchandise device time country ip url 0 0 8dac2b259fd1c6d1120e519fb1ac14fbqvax8 ewmzr jewelry phone0 9759243157894736 us 69.166.231.58 vasstdc27m7nks3 1 1 668d393e858e8126275433046bbd35c6tywop aeqok furniture phone1 9759243157894736 in 50.201.125.84 jmqlhflrzwuay9c 2 2 aa5f360084278b35d746fa6af3a7a1a5ra3xe wa00e home goods phone2 9759243157894736 py 112.54.208.157 vasstdc27m7nks3 12print 'Is there any missing value in bids?',bids.isnull().any().any()# pickle.dump(bids, open('bids.pkl', 'w')) Is there any missing value in bids? True 1234missing_country = bids['country'].isnull().sum().sum()print 'No. of missing country: ', missing_countrynormal_country = bids['country'].notnull().sum().sum()print 'No. of normal country: ', normal_country No. of missing country: 5 No. of normal country: 7656329 12nan_rows = bids[bids.isnull().T.any().T]print nan_rows bid_id bidder_id auction \\ 1351177 1351177 f3ab8c9ecc0d021ebc81e89f20c8267bn812w jefix 2754184 2754184 88ef9cfdbec4c9e33f6c2e0b512e7a01dp2p2 cc5fs 2836631 2836631 29b8af2fea3881ef61911612372dac41vczqv jqx39 3125892 3125892 df20f216cbb0b0df5a7b2e94b16a7853iyw9g jqx39 5153748 5153748 5e05ec450e2dd64d7996a08bbbca4f126nzzk jqx39 merchandise device time country \\ 1351177 office equipment phone84 9767200789473684 NaN 2754184 mobile phone150 9633363947368421 NaN 2836631 jewelry phone72 9634034894736842 NaN 3125892 books and music phone106 9635755105263157 NaN 5153748 mobile phone267 9645270210526315 NaN ip url 1351177 80.211.119.111 g9pgdfci3yseml5 2754184 20.67.240.88 ctivbfq55rktail 2836631 149.210.107.205 vasstdc27m7nks3 3125892 26.23.62.59 ac9xlqtfg0cx5c5 5153748 145.7.194.40 0em0vg1f0zuxonw 1234# print bids[bids['bid_id']==1351177]nan_bidder = nan_rows['bidder_id'].values.tolist()# print nan_bidderprint bids[bids['bidder_id'].isin(nan_bidder)] bid_id bidder_id auction \\ 1351177 1351177 f3ab8c9ecc0d021ebc81e89f20c8267bn812w jefix 2754184 2754184 88ef9cfdbec4c9e33f6c2e0b512e7a01dp2p2 cc5fs 2836631 2836631 29b8af2fea3881ef61911612372dac41vczqv jqx39 3125892 3125892 df20f216cbb0b0df5a7b2e94b16a7853iyw9g jqx39 5153748 5153748 5e05ec450e2dd64d7996a08bbbca4f126nzzk jqx39 merchandise device time country \\ 1351177 office equipment phone84 9767200789473684 NaN 2754184 mobile phone150 9633363947368421 NaN 2836631 jewelry phone72 9634034894736842 NaN 3125892 books and music phone106 9635755105263157 NaN 5153748 mobile phone267 9645270210526315 NaN ip url 1351177 80.211.119.111 g9pgdfci3yseml5 2754184 20.67.240.88 ctivbfq55rktail 2836631 149.210.107.205 vasstdc27m7nks3 3125892 26.23.62.59 ac9xlqtfg0cx5c5 5153748 145.7.194.40 0em0vg1f0zuxonw 在对整体数据的部分用户缺失国家的按照各个用户分组后在时间上前向和后向填充后，仍然存在5个用户缺失了国家信息，结果发现这5个用户是仅有一次竞标行为，下面看看这5个用户还有什么特征 1234lst_nan_train = [i for i in nan_bidder if i in (train['bidder_id'].unique()).tolist()]lst_nan_test = [i for i in nan_bidder if i in (test['bidder_id'].unique()).tolist()]print 'No. of bidders 1 bid in train set: ',len(lst_nan_train)print 'No. of bidders 1 bid in test set: ',len(lst_nan_test) No. of bidders 1 bid in train set: 1 No. of bidders 1 bid in test set: 4 1print train[train['bidder_id']==lst_nan_train[0]]['outcome'] 546 0.0 Name: outcome, dtype: float64 由于这5个用户仅有一次竞标行为，而且其中1个用户来自训练集，4个来自测试集，由训练集用户的标记为人类，加上行为数太少，所以考虑对这5个用户的竞标行为数据予以舍弃，特别对测试集的4个用户后续操作类似之前对无竞标行为的用户，预测值填充最终模型的平均预测值 123bid_to_drop = nan_rows.index.values.tolist()# print bid_to_dropbids.drop(bids.index[bid_to_drop], inplace=True) 12print 'Is there any missing value in bids?',bids.isnull().any().any()pickle.dump(bids, open('bids.pkl', 'w')) Is there any missing value in bids? False 统计基本的计数特征根据前面的数据探索，由于数据集大部分由类别数据或者离散型数据构成，所以首先针对竞标行为数据按照竞标者分组统计其各项属性的数目，比如使用设备种类，参与竞标涉及国家，ip种类等等 123# group by bidder to do some statisticsbidders = bids.groupby('bidder_id')# pickle.dump(bids, open('bidders.pkl', 'w')) 1234567891011121314# print bidders['device'].count()def feature_count(group): dct_cnt = &#123;&#125; dct_cnt['devices_c'] = group['device'].unique().shape[0] dct_cnt['countries_c'] = group['country'].unique().shape[0] dct_cnt['ip_c'] = group['ip'].unique().shape[0] dct_cnt['url_c'] = group['url'].unique().shape[0] dct_cnt['auction_c'] = group['auction'].unique().shape[0] dct_cnt['auc_mean'] = np.mean(group['auction'].value_counts()) # bids_c/auction_c# dct_cnt['dev_mean'] = np.mean(group['device'].value_counts()) # bids_c/devices_c dct_cnt['merch_c'] = group['merchandise'].unique().shape[0] dct_cnt['bids_c'] = group.shape[0] dct_cnt = pd.Series(dct_cnt) return dct_cnt 1cnt_bidder = bidders.apply(feature_count) 123display(cnt_bidder.describe())# cnt_bidder.to_csv('cnt_bidder.csv')# print cnt_bidder[cnt_bidder['merch_c']==2] - auc_mean auction_c bids_c countries_c devices_c ip_c merch_c url_c count 6609.000000 6609.000000 6609.000000 6609.000000 6609.000000 6609.000000 6609.000000 6609.000000 mean 6.593493 57.850810 1158.470117 12.733848 73.492359 544.507187 1.000151 290.964140 std 30.009242 131.814053 9596.595169 22.556570 172.171106 3370.730666 0.012301 2225.912425 min 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 25% 1.000000 2.000000 3.000000 1.000000 2.000000 2.000000 1.000000 1.000000 50% 1.677419 10.000000 18.000000 3.000000 8.000000 12.000000 1.000000 5.000000 75% 4.142857 47.000000 187.000000 12.000000 57.000000 111.000000 1.000000 36.000000 max 1327.366667 1726.000000 515033.000000 178.000000 2618.000000 111918.000000 2.000000 81376.000000 特征相关性在对竞标行为数据按照用户分组后，对数据集中的每一个产品特征构建一个散布矩阵（scatter matrix），来看看各特征之间的相关性 12# 对于数据中的每一对特征构造一个散布矩阵pd.scatter_matrix(cnt_bidder, alpha = 0.3, figsize = (16,10), diagonal = 'kde'); 在针对竞标行为数据按照竞标用户进行分组基本统计后由上表可以看出，此时并未考虑时间戳的情形下，有以下基本结论： 由各项统计的最大值与中位值，75%值的比较可以看到除了商品类别一项，其他的几项多少都存在一些异常数值，或许可以作为异常行为进行观察 各特征的倾斜度很大，考虑对特征进行取对数的操作，并再次输出散布矩阵看看相关性。 商品类别计数这一特征的方差很小，而且从中位数乃至75%的统计来看，大多数用户仅对同一类别商品进行拍卖，而且因为前面数据探索部分发现商品类别本身适合作为类别数据，所以考虑分多个类别进行单独统计，而在计数特征中舍弃该特征。 1cnt_bidder.drop('merch_c', axis=1, inplace=True) 1cnt_bidder = np.log(cnt_bidder) 1pd.scatter_matrix(cnt_bidder, alpha = 0.3, figsize = (16,10), diagonal = 'kde'); 由上面的散布矩阵可以看到，个行为特征之间并没有表现出很强的相关性，虽然其中的ip计数和竞标计数，设备计数在进行对数操作处理之后表现出轻微的正相关性，但是由于是在做了对数操作之后才体现，而且从图中可以看到并非很强的相关性，所以保留这三个特征。 针对前述的异常行为，先从原train数据集中的机器人、人类中分别挑选几个样本进行追踪观察他们在按照bidders分组后的统计结果，对比看看 1cnt_bidder.to_csv('cnt_bidder.csv') 12345678# trace samples,first 2 bots, last 2 humenindices = ['9434778d2268f1fa2a8ede48c0cd05c097zey','aabc211b4cf4d29e4ac7e7e361371622pockb', 'd878560888b11447e73324a6e263fbd5iydo1','91a3c57b13234af24875c56fb7e2b2f4rb56a']# build a DataFrame for the choosed indicessamples = pd.DataFrame(cnt_bidder.loc[indices], columns = cnt_bidder.keys()).reset_index(drop = True)print \"Chosen samples of training dataset:(first 2 bots, last 2 humen)\"display(samples) Chosen samples of training dataset:(first 2 bots, last 2 humen) auc_mean auction_c bids_c countries_c devices_c ip_c url_c 0 1 2 3 3.190981 5.594711 8.785692 4.174387 6.011267 8.147578 7.557995 2.780432 4.844187 7.624619 2.639057 3.178054 5.880533 1.609438 0.287682 1.098612 1.386294 1.098612 1.386294 1.386294 0.000000 0.287682 2.890372 3.178054 1.791759 2.639057 2.995732 0.000000 使用seaborn来对上面四个例子的热力图进行可视化，看看percentile的情况 123456789101112import matplotlib.pyplot as pltimport seaborn as sns# look at percentile rankspcts = 100. * cnt_bidder.rank(axis=0, pct=True).loc[indices].round(decimals=3)print pcts# visualize percentiles with heatmapsns.heatmap(pcts, yticklabels=['robot 1', 'robot 2', 'human 1', 'human 2'], annot=True, linewidth=.1, vmax=99, fmt='.1f', cmap='YlGnBu')plt.title('Percentile ranks of\\nsamples\\' feature statistics')plt.xticks(rotation=45, ha='center'); auc_mean auction_c bids_c \\ bidder_id 9434778d2268f1fa2a8ede48c0cd05c097zey 94.9 94.6 97.0 aabc211b4cf4d29e4ac7e7e361371622pockb 92.4 87.2 92.3 d878560888b11447e73324a6e263fbd5iydo1 39.8 30.4 30.2 91a3c57b13234af24875c56fb7e2b2f4rb56a 39.8 60.2 53.0 countries_c devices_c ip_c url_c bidder_id 9434778d2268f1fa2a8ede48c0cd05c097zey 95.4 95.6 96.7 97.4 aabc211b4cf4d29e4ac7e7e361371622pockb 77.3 63.8 84.8 50.3 d878560888b11447e73324a6e263fbd5iydo1 48.8 38.7 34.2 13.4 91a3c57b13234af24875c56fb7e2b2f4rb56a 63.7 56.8 56.2 13.4 由上面的热力图对比可以看到，机器人的各项统计指标除去商品类别上的统计以外，均比人类用户要高，所以考虑据此设计基于基本统计指标规则的基准模型，其中最显著的特征差异应该是在auc_mean一项即用户在各个拍卖场的平均竞标次数，不妨先按照异常值处理的方法来找出上述基础统计中的异常情况 设计朴素分类器由于最终目的是从竞标者中寻找到机器人用户，而根据常识，机器人用户的各项竞标行为的操作应该比人类要频繁许多，所以可以从异常值检验的角度来设计朴素分类器，根据之前针对不同用户统计的基本特征计数情况，可以先针对每一个特征找出其中的疑似异常用户列表，最后整合各个特征生成的用户列表，认为超过多个特征异常的用户为机器人用户。 123456789101112# find the outliers for each featurelst_outlier = []for feature in cnt_bidder.keys(): # percentile 25th Q1 = np.percentile(cnt_bidder[feature], 25) # percentile 75th Q3 = np.percentile(cnt_bidder[feature], 75) step = 1.5 * (Q3 - Q1) # show outliers # print \"Data points considered outliers for the feature '&#123;&#125;':\".format(feature) display(cnt_bidder[~((cnt_bidder[feature] &gt;= Q1 - step) &amp; (cnt_bidder[feature] &lt;= Q3 + step))]) lst_outlier += cnt_bidder[~((cnt_bidder[feature] &gt;= Q1 - step) &amp; (cnt_bidder[feature] &lt;= Q3 + step))].index.values.tolist() 再找到各种特征的所有可能作为‘异常值’的用户id之后，可以对其做一个基本统计，进一步找出其中超过某几个特征值均异常的用户，经过测试，考虑到原始train集合里bots用户不到5%，所以最终确定以不低于1个特征值均异常的用户作为异常用户的一个假设，由此与train集合里的用户进行交叉，可以得到一个用户子集，可以作为朴素分类器的一个操作方法。 12345# print len(lst_outlier)from collections import Counterfreq_outlier = dict(Counter(lst_outlier))perhaps_outlier = [i for i in freq_outlier if freq_outlier[i] &gt;= 1]print len(perhaps_outlier) 214 123# basic_pred = test[test['bidder_id'].isin(perhaps_outlier)]['bidder_id'].tolist()train_pred = train[train['bidder_id'].isin(perhaps_outlier)]['bidder_id'].tolist()print len(train_pred) 76 设计评价指标根据前面数据探索知本实验中的数据集的正负例比例约为19:1，有些失衡，所以考虑使用auc这种不受正负例比例影响的评价指标作为衡量标准，现针对所涉及的朴素分类器在原始训练集上的表现得到一个基准得分 1234567from sklearn.metrics import roc_auc_scorey_true = train['outcome']naive_pred = pd.DataFrame(columns=['bidder_id', 'prediction'])naive_pred['bidder_id'] = train['bidder_id']naive_pred['prediction'] = np.where(naive_pred['bidder_id'].isin(train_pred), 1.0, 0.0)basic_pred = naive_pred['prediction']print roc_auc_score(y_true, basic_pred) 0.54661464952 在经过上述对基本计数特征的统计之后，目前尚未针对非类别特征：时间戳进行处理，而在之前的数据探索过程中，针对商品类别和国家这两个类别属性，可以将原始的单一特征转换为多个特征分别统计，此外，在上述分析过程中，我们发现针对用户分组可以进一步对于拍卖场进行分组统计。 对时间戳进行处理 针对商品类别、国家转换为多个类别分别进行统计 按照用户-拍卖场进行分组进一步统计 对时间戳进行处理主要是分析各个竞标行为的时间间隔，即统计竞标行为表中在同一拍卖场的各个用户之间的竞标行为间隔 然后针对每个用户对其他用户的时间间隔计算 时间间隔均值 时间间隔最大值 时间间隔最小值 1234567891011121314151617181920212223from collections import defaultdictdef generate_timediff(): bids_grouped = bids.groupby('auction') bds = defaultdict(list) last_row = None for bids_auc in bids_grouped: for i, row in bids_auc[1].iterrows(): if last_row is None: last_row = row continue time_difference = row['time'] - last_row['time'] bds[row['bidder_id']].append(time_difference) last_row = row df = [] for key in bds.keys(): df.append(&#123;'bidder_id': key, 'mean': np.mean(bds[key]), 'min': np.min(bds[key]), 'max': np.max(bds[key])&#125;) pd.DataFrame(df).to_csv('tdiff.csv', index=False) 1generate_timediff() 后续内容请移步使用机器学习识别出拍卖场中作弊的机器人用户(二)","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"平坦世界","date":"2017-05-29T08:30:23.000Z","path":"2017/05/29/世界是平的/","text":"这次给大家推荐一本书——“The world is flat”（《世界是平的》），刚刚看完了一遍，感觉这本书的视角很广阔，让我看到自己受限于地理或者意识因素所看不到的一些其实就实实在在发生在身边的变化，私以为，作为即将毕业的大学生而言，是很有必要一读的。 【这本书面世其实快10年了，但是书中所述的一些变化我们或经历过，或忽略掉了，所以即使放在现在读，也是很有前瞻性的。特别由于作者本身是记者，写作风格还是颇为轻松地，所以，读起来感觉像是跟着作者开着上帝视角实现了所谓“世界那么大，我想去看看”的成就一般。】 此书整体大概是由世界变平坦的种种例证、现象到作者所认为使世界变平坦的一些因素再到各个“成员”大至国家公司、小至每一个个体在这平坦世界里的角色和相互影响，最后是平坦世界所带来的或即将造成的可能负面影响和威胁来逐层叙述的。我想先就宏观角度简单记叙下书中的一些观点，然后谈一下书中所说而我也感觉到的身边的一些变化，最后就自己而言随意谈一下想法。 就宏观而言，书中让我印象最深的是全球化的重中体现，首先是全球合作的一些案例，这个曾经在《互联网时代》里也看到过，比如现在的波音飞机的组装以及各零件的制造是分散到世界各个国家各个地区去做的，然后又井然有序的由各个地方汇聚到一个点组装成型，书中还举得一个例子是沃尔玛的生产线，和前者类似，也是一种像分布式的结构，这种做法极大地提高了效率并降低了成本，而其中最让我目眩神迷的是这么个巨大的整体居然能够这么一致的展开协同工作；另外一点是关于离岸外包，这里就不得不提到一个国家是印度，或者说印度的城市班加罗尔，看了这本书我感觉像亲身去看一下，所谓外包，就是将本国家的一些相对而言中低端而要耗费大量人力的基础工作交给其他国家去做，这里你也许会想到所谓的”Made in China”，诚然，中国也是外包的一个很好的例子，换做以前我会觉得嗤之以鼻，感觉我们国家就老是像给美国之类的国家打工一样，其实，分析一下，就目前而言，这个是双赢的，美国那边自不用说，花一分美国工人的钱在中国或者印度雇佣到6~8个同样水平的技术工人，极大地降低了成本，而且这里面不得不说有个很有意思的东西是——时差，因为有些工作在美国的白天又美国人做，晚上就交给印度人（印度正在白天）做，极大地提升了效率，这个我在电影《贫民窟的百万富翁》里看到过的一个场景就是印度人通过远程监控摄像头替美国人监控停车场让我印象深刻，那么对于印度和中国这些国家呢，好处在哪？其一就是增多了工作岗位，就印度而言的话，由于现在印度培养了大量的理工学生，而毕业后以前只能去国外谋出路，否则在国内一般只能成为出租车司机，而经过外包，他们可以在本土就进行软件开发的工作，虽然工资不如美国人，但是相对于自己本身而言，他们的生活已经得到不小的提升了，另外，通过这种形式，印度和中国还可以学习一些国外的技术，甚至是抢占国外的市场，书中后面有个例子是中国生产的埃及传统的灯几乎快要垄断埃及的市场，因为在技术上有所突破，而且价格也低廉。 当然，以上只是从书中看到的知识，也许我以后还需要实地考察一番看看实际情况是否真如作者所述，不过，对于我们国家，我还是希望抱有乐观的态度去看他的发展，相信他会越变越好。 就身边的所见所闻而言，我印象比较深的是一个 距离 的问题，和书中作者一次下飞机乘出租车的经历一般，在乘车那段时间里，司机一直在通过蓝牙耳机聊天，车上开着导航，播放着电影，而作者在自己的笔记本上整理文章以及收发e-mail，用作者的话说，在那一个小时里他们同时做了很多事，却几乎没有交流，作者甚至猜想，也许司机正和远在另一个国度的父母通话呢。这大概就是目前发生在我们身边的一个尴尬境地了，一方面，技术的发展拉近了我们的距离，而另一面却又使我们的距离变得遥远，它拉近了我们和处在不同空间的亲人之间的距离，却又在此时此地的就在你眼前的我面前树起了一道屏障。我想起了高中语文老师发的一篇文章里对动车、高铁上的年轻人乘客们的描述，确确实实的感受到这一真切的现象，大家一上车就戴上耳机，或插入ipad，或插入iphone，而彼此之间却没有交流，这和绿皮火车上的情景完全不同，也是值得我们反思的一件事。 最后我想简单谈一下自己的一些想法。首先关于教育和竞争方面，这里还得扯下印度，书中说道，班加罗尔的接线员晚上为美国（白天）的乘客们进行咨询和失物找回工作，晚上还会自己学习一些知识，攻读一些学位什么的，顿时感到一种压迫感。从前经常听到政治老师说美国家长告诫自己的孩子说快吃饭，不然中国和印度的孩子就把你的饭给吃了或者快努力学习，中国或印度的孩子快把你的饭碗抢走了之类的。而现在是大家都站在几乎同一个平台上竞争，前面我们需要追赶美国，而同时，印度的青年们却也在旁虎视眈眈。而身处计算机专业这一日新月异，竞争更加残酷的环境下，我一面感到威胁，一面感到兴奋，兴奋的是，试想一下你即将站在一个大舞台上，和来自不同城市乃至不同国家的人角逐，而威胁在于，你们本身的水平是不一样的，就我而言，在我将要踏上这个舞台的那一天，我不仅会和同班同学竞争，稍远一点还有来自全国各大高校的强者，再远一点还有世界其他角落的高手出没，而要想不被碾成炮灰，或者说不被这一变平的趋势所冲倒的话，我就得不断地去吸收和学习新的技术、新的知识，还要进一步强化自己的全局意识，做到让自己所做的事无可替代，也就不会被”外包”掉。另外一点就是之前实习时听到负责人说现在的联系方式太多了，又是电话、又是微信、QQ的，反而造成了联系上的障碍，以前我们只有电话时一般就通过电话联系，而现在常常是不同人有不同的习惯，你得把这些一股脑全开着，否则搞不好会错过重要讯息，我常常在想，世界是多元化的，但本意是方便我们交流的工具最后却慢慢成了束缚我们的枷锁，从这个角度来看，人类到底是进步了，还是退化了？","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"Economy","slug":"Economy","permalink":"http://yoursite.com/tags/Economy/"}]},{"title":"面壁者，我是你的破壁人","date":"2017-05-29T08:28:53.000Z","path":"2017/05/29/三体/","text":"对一个人的评判若不结合他所处的时代都是不公允的。 终于看完了《三体》全集，这部科幻小说给人的感觉真像作者自己命名的“地球往事”，像一本历史书，以宇宙为坐标，以光年为刻度，读罢，借用一句话“科幻小说虽然尽是对于未来的想象，但我们探索的一直是人的内心”。 情节上我就不剧透太多了，我想谈谈《三体》里的几个角色。 首先是罗辑，面壁者、执剑人、守墓人是他不同时期的身份，他是一个充满传奇色彩的人，用他自己的话是一个及时行乐的人，对其他的人、其他的事都不怎么上心的人。但是我看到的是他受人胁迫而终于接受面壁者身份时的无奈和挣扎；在冬眠复苏时被人当做笑话时的淡然、洒脱，再一次被推上历史转折点时的坚韧以及作为执剑人半个多世纪面壁时的狠厉。我得承认全书给我印象最深的是《黑暗森林》最高潮的部分，当罗辑站在自己挖掘的坟墓边对着虚空中的智子亮出最后底牌的那一刻；我真的有一种作为人类终于得救了、终于又有了希望的感觉。命运喜欢捉弄罗辑，在他吊儿郎当、无所事事时对他施以面壁者的诅咒，在他拯救了全人类时却让人类对他报以敌视；但罗辑不在乎这些，他只在乎自己所爱的那个女孩的幸福。 然后是《死神永生》里的女主角程心。之前有看一个《x分钟读完三体》的短片，当时发现弹幕上对于程心大多数是一片骂声，随处可见诸如“程圣母毁灭世界”的字眼。不得不说在看《死神永生》的中后段我也一直对于程心是恨得咬牙切齿的，实话说，我讨厌她的不作为和自以为是爱与善的化身所做的所有决定，讨厌她毁灭了地球、害人类灭亡，但后来一想，我其实是开了上帝视角在看这个角色，其实回过头来看，程心是当前普通人类的典型，甚至，是大众中向善的群体的典型，她一直秉持着自己的责任，逆来顺受；她是勇敢的，敢于牺牲自己，但是，她的能力是不够的。整部小说看下来程心大部分时间处于冬眠状态，而经常是其他人帮她做足了准备，然后突然一下把她推到全人类命运决策的位置，可是却没有想过她准备好了没有，结果她只能依据自己当前的认识做出最佳选择，然后成了公众的替罪羊，其实换位思考，我们估计在那个节点做的更差；不信你把程心放到不同的时间段看公众对于她作为执剑人的那几分钟的态度的不停反复就可以看出来了，大众只是需要一个为自己顶包的人，然后好像自己就可以置身事外了。 如果说程心更多的依靠感性来做判断，那么就不得不提到维德了，这个极端理性到冷酷的男人。最开始他对于程心的捉弄确实让我很不爽，从发射云天明的大脑一直到后面他不择手段的想要暗杀程心竞争执剑人。但读到后面你会发现维德其实是个表里如一的人，对于自己认定的事他能坚持到底，不择手段的坚持到底。 所以不存在这么一个人，把他放到任何一个时空他的判断都是对的，或者说大众对于个体的判断其实也不能代表什么，一个人只需坚持自己所相信的，在乎自己所爱的，到最后一刻，就够了。 各位面壁者们，准备好做自己的破壁人了么？","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"Novel","slug":"Novel","permalink":"http://yoursite.com/tags/Novel/"}]},{"title":"平凡的世界，不平凡的凡人","date":"2017-05-29T08:27:42.000Z","path":"2017/05/29/平凡的世界/","text":"一直想读这套书，在临近毕业时还专门从朋友那收藏了一套，借着在MEB的机会终于是忙里偷闲的看完了，虽意犹未尽，却也感到一种平和和满足。 如果要用一句话概括，《平凡的世界》写的是爱情和面包。 书中描述的爱情种类比较多，我就简单挑给我印象最深的三个来说说吧。首先是润叶和少安的爱情，那句诗里写的“郎骑竹马来，绕床弄青梅”大抵描述的就是这么一种爱情。少安和润叶是两小无猜的一对童年玩伴，随着两家各自发展出的家庭背景的差异却也慢慢在双方之间产生了巨大的鸿沟，一个留校任教成了城里人，另一个却回家务农，给家里分担压力。诚然，你可以说是少安主观上的懦弱导致了最终两人的分离和三个家庭的阴影和痛苦，但是个人认为，在那么一种背景下，就一个男人而言是不得不考虑这些东西的。书中虽说如果两人身份对调也许就没什么问题，但我觉得即使两人身份对调而彼此相爱也还是要考虑双方的家庭背景，这里我倒不是在推崇门当户对的老观念，只是客观的分析一下，毕竟，我一直认为，爱情是建立在一定的物质基础之上，不是简单的彼此相爱，毕竟，爱情可能产生在一瞬间，但是维系这份感情却需要长期的努力，也就需要一定的物质作为基础，而若双方各自条件不对等，时间长了或多或少会产生并积累一些问题。由此看来，古人所推崇的门当户对确有一定道理。 当然，我不否认灰姑娘和王子的童话故事，但是即使在灰姑娘的故事里，灰姑娘也是有仙子为她做后盾的而且她本身有相对应的素质作为支撑。 扯远了，其实书中倒也有这么一对，就是少平和晓霞。我对这一对的定义是充满理想和浪漫主义色彩的情侣，两人由最初的共同爱好或说理想发展出深厚的友谊，继而在不同的人生道路上产生的共鸣引导出爱情，说实话，看到这里我已经觉得有点过于理想化了，到后来晓霞拒绝高富帅依然坚定地爱着少平时，我也深深被打动，从心理上我是希望看到这一对能有美好的结局的，我原希望会是少平从煤炭场最后走出来完成逆袭然后迎娶白富美的励志故事（原谅程序员的屌丝气息），不过感觉依作者的套路势必会再给这对有情人制作一些波折，可是，万万没想到，情节发展居然是晓霞的牺牲，当时感觉很难接受这一事实，毕竟这朴实的书中的一朵充满浪漫主义色彩的绚丽花朵竟以这种方式枯萎。 最后一对就是润叶和向前，很难用爱情定义这一对，因为故事前段一直是向前的单恋，即使两人的结合也是润叶的自我牺牲和自我放逐，结果是长期的互相折磨到最后，然后转折点在向前的事故所带来的残疾，润叶因为内疚而突然回头，两人复合（个人觉得此处作者不够深入），然后两人开始真正的婚姻生活，不过也很难说是爱情，连文中也提及此时的润叶更像是一个虔诚的修女，给予向前的除了妻子的责任另外确是一种变相的母性关怀，姑且称之为——怜悯或者同情吧，其实我是很喜欢润叶这么个姑娘的，敢爱敢恨，在和少安的爱情中表现出勇敢，却又太懂事，这样矛盾的性格造成她最终的屈服，与其说她是嫁给了一个自己不爱的男人，倒不如说她是嫁给了生活。但我又不能怪罪向前，爱情这东西说不清道不明的，很多时候其实就是，我喜欢你然而我并不能理解你为何不喜欢我，其结果至少有一人受伤。 以上是对书中描述的几种爱情的粗浅理解，当然，作为一个纯理论家，请相信，我说的每一句话都是谎话。 还是来谈谈另一条线——面包吧。 全书主要描述了双水村这么个平凡的小天地里一个又一个像你像他像那路边野花的平凡人的平凡而又不平凡的生活。总体来说，大的背景是变化莫测的，从文革末的动乱年代到改革开放的小康生活，立足在这么一个大环境下，每个人的命运都不能孤立的来看，刚开始看这本书的时候我常常随着故事的情节发展给故事中的人下定义是好是坏是对是错，后来随着作者时不时的客观分析才发现自己的浅薄，其实就像我们所生活中的生活，它不像我们做的卷子上的题目，没有什么绝对的对错，每个人只是在给他演出的那么一段镜头里或优雅谢幕或落荒而逃，导演却是生活。也许我仍不能从简单的评判一个人是好人坏人，他做的一件事是对的还是错的这么一种观念里走出来，但是从这本书里我看到了生活这辆无缰马车上的形形色色人物的身不由己，同时，我也看到他们的奋斗，一次一次被生活喊cut却又一次一次带着微笑或者含着泪水伤痕累累的跑龙套，在这场戏里，没有主角，或者说每个人都是主角，轮番上场。 每一个人物都有其鲜明特色又有其局限性，之前有一朋友说他觉得书中把任务刻画得太脸谱化了，我觉得是有这么一点，但是大体而言路遥对人物的刻画还是很鲜明，很丰满而有血有肉的。我欣赏少安在物质生活追求上的不屈不挠精神，却又为他有时的偏执扼腕，对于他在爱情上的胆怯以及一意孤行而叹息；我喜欢少平这么一个脚踏实地的为生活战斗，为自己为他人着想的‘精神贵族’。他太可爱了，看这本书真的就是看着这个少年的成长，看着他咀嚼生活，消化生活，不过我希望他能更勇敢，有时能多为自己考虑一点，勇敢的追逐自己的幸福；至于晓霞，自不用说，感觉书中几乎刻画成女神一般的存在了，可惜也由于她的冒险精神和舍己为人的精神而香消玉殒；润叶，真是一个让人忍不住心疼却又忍不住尊敬的女性，她是那么为大家着想，甚至于可以牺牲自己；还有孙玉厚，这个伟大的老父亲，虽然书中没太多专门的笔墨，但是仍可以看见少安、少平甚至兰花从他身上传承的坚韧的基因，也许从早期发狠供玉亭念书也能窥见一二，另外，这位老人在儿子追求自己目标时默默支持，在孩子们遭受不幸时却总是挺身而出，实在令人感动；其他还有用情专一的向前，逛鬼王满银，懂事的兰香等等，像你我身边的每一个人一样在这个平凡的世界里献上了这精彩的演出，然后帷幕缓缓落下，他们又消失在人群中，从此你我看到的身边的每一个人都似曾相识。 以上大概就是这粗略的第一次阅读《平凡的世界》的一点不吐不快的想法，虽然书有一点点小瑕疵比如各个主角的结局实在难以让我这么一个习惯了happy ending的人满意，比如中间外星人的情节有点乱入的感觉，不过还是强力推荐这本书，怎么说呢，在看这本书的过程中，我常常有一种踏实感，活着的踏实感，在我合上书页，回想起书中一些情节时情不自禁的感受到生活，从某种程度上，它减轻了我若有若无的虚无感。","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"Novel","slug":"Novel","permalink":"http://yoursite.com/tags/Novel/"}]},{"title":"韶华倾负，难舍皮囊","date":"2017-05-29T08:22:21.000Z","path":"2017/05/29/韶华倾负，难舍皮囊/","text":"利用在动车上的四个小时看完了《皮囊》（除去中间15分钟和邻座美女搭讪的时间），感觉是很不错的一本书，不同的章节写的不同的故事，如果你有时间不妨通篇阅读一下，如果没有时间不妨听我说一说。 个人觉得，前五章的主题关于生离，关于死别。 第一章和书同名是为《皮囊》，讲述的是99岁的阿太。 我们的生命本来多轻盈，都是被这肉体和各种欲望的污浊给拖住。阿太，我记住了。“肉体是拿来用的，不是拿来伺候的。” 没文化的神婆阿太穷极一生都在利用自己的那副皮囊，甚至要求周围的人也学会去利用好这幅皮囊，所以她把年幼的孩子扔进海里让他学游泳，所以她即使白发人送黑发人也保持一副让人不解的嘲弄表情，或许在她看来，失掉这皮囊，灵魂最终的归宿反而是自由吧。 但这并不意味着阿太是那种不在乎生死的人，相反，我觉得她是站在更高的角度看待生死这个问题的人，而这也许隐隐约约影响着身边的人。所以作者后来写下这么一句话： 从小我就喜欢闻泥土的味道，也因此其实从小我不怕死，一直觉得死是回家，是入土。我反而觉得生才是问题，人学会站立，是任性地想脱离这土地，因此不断向上攀爬，不断抓取任何理由——欲望、理想、追求。然而，我们终究需要脚踏着黄土。在我看来，生是更激烈的索取，或许太激烈的生活本身就是一种任性。 然后是第二章——《母亲的房子》写的是母亲，房子还有爱情。 每个人都会有自己的执着，而那种执着最后就物化成某种具体的事物，然后进一步，有的化成妥协后的一道疤痕，有的成为穷极一生的执念。而文中的母亲显然是后者，而她所执着的是那所房子，是年轻时的父亲曾允诺的房子，以一种执着的近乎任性的方式。从最开始的一修再修，这所母亲执意要修建的房子联系着一家人的生活和命运，从拮据的生活到后来周围人的不解最后甚至家人的埋怨，母亲一度支撑不下去却又不甘心，最难过的日子里甚至一家人想要一起求死，直到后来身为“不合格的一家之主”的我终于理解了母亲： 事实上，知道母亲坚持要建好这房子的那一刻。我才明白过来，前两次建房，为的不是她或者我的脸面，而是父亲的脸面——她想让父亲发起的这个家庭看上去是那么健全和完整。 这是母亲从老没表达过，也不可能说出口的爱情。 另外，关于母亲的故事在《我的神明朋友》里会继续讲述，你会看到更多母亲身上的坚韧。 然后《残疾》讲述的是父亲中风后的家里的种种艰辛和一家人和生活的抗争。 不同于以前语文课本里父亲那种高大、沉默的形象，《皮囊》里的父亲首先是一个中风而偏瘫了的父亲，而后是一个曾经混黑社会呼风唤雨的混混头子，中风前后的心理落差造成了父亲接下来的一系列变化，他从假装坚强，希望靠对时间的严苛要求和每天的活动恢复到以前的健壮身体，于是一家人默契的相互演着戏，却也过了一段幸福的时间，直到被一场意料之中的台风摧毁。 虽然知道根本不是台风的错。那结局是注定的，生活中很多事情，该来的会来，不以这个形式，就会以那样的形式。但把事情简单归咎于我们无能为力的某个点，会让我们的内心可以稍微自我安慰一下，所以，我至今仍愿意诅咒那次台风。 那场台风刮倒的不仅仅是父亲，更打碎了父亲伪装的坚强和一家人匆忙写就的生活的剧本。自此，父亲进入了第二个状态，先是放弃了坚强，呈现出一副自暴自弃的样子期盼着死亡。而后又进入下一阶段，甚至于舍弃了父亲的身份，像个不懂事的孩子一样撒娇、任性，却不再期盼死亡。 但是父亲的故事并没有完，在《重症病房里的圣诞节》，作者以在重症病房里的看护家属的视角半写实半轻描淡写地描述了退去浮华身份后的种种生离死别。 疾病在不同的地方找到了他们，即使他们当时身处不同的生活，但疾病一眼看出他们共同的地方，统一把他们赶到这么一个地方圈养。 在这一章里，作者刻画了几个不同的人物，有乐观的病人，有刻意显得刻薄的医护人员，有早熟的同龄孩子。在医院这么一个特殊的小世界里，赤裸的生长着。有几个事件的刻画让我印象深刻，比如‘我’对于医院里电梯的描写，对于走过一间间病房核查之前的病人是否还在；比如和同龄的孩子交流，发现相同点，大家拥有相同的眼睛，那是经历过生命消逝后才看得到的眼睛，所以会被一眼看透，而无法交到同龄的朋友，因为他们只有一次也只能有一次痛彻心扉的谈话；比如乐观的病友最后还是被夺走生命后‘我’心态上的变化，印象最深的是那个为父亲在圣诞节违禁燃放烟花的年轻人，只因他父亲要进行手术了，然后他被三个保安团团围住…… 比如，在帮父亲换输液瓶时，会发觉他手上密密麻麻的针孔，找不到哪一寸可以用来插针；比如医生会时常拿着两种药让我选择，这个是进口的贵点的，这个是国产的便宜的，你要哪种？我问了问进口的价钱，想了很久。“国产的会有副作用吗？”“会，吃完后会有疼痛，进口的就不会。”我算了算剩下的钱和可能要住院的时间，“还是国产的吧。” 然后看着父亲疼痛了一个晚上，怎么都睡不着。 在这里，你一不小心留出空当，就会被悲伤占领——这是疾病最廉价、最恼人的雇佣兵。 这种笔触没在重症病房待过的人是写不出来的，那种切实的压抑感和空闲下来被对未来的恐惧和迷茫逼到绝路的走投无路感。 接下来的几章个人觉得是关于青春和梦想的。 首先《张美丽》这一章从这么一个略显俗气的名字开始，从迷信的桃色传说开始记叙了传说中自杀的殉情张美丽，而后记叙了现实中被夹在理想和世俗之间压垮的张美丽，同样是一副让荷尔蒙萌动的青春期少年燥起来的皮囊，演绎了不同的故事。这故事一面让我想起了《搜索》这部电影里舆论的可怕，人们对于未知的事物的不理解演化成的嫉妒和驱逐和以讹传讹造成的悲剧上演。另一面看到的是被遗忘的先驱们，为了自己的理想而头破血流，然后若干年后在残次不齐的字里行间被遗忘或变成谈资。只是人们一同忘掉的，是现在所呈现在眼前的似曾相识。 接下来《阿小和阿小》、《天才文展》记叙的算是童年和成长吧。 关于城市，那是不在城市长大的孩子们小时候的天堂，是他们在电视屏幕上看到的模样，而两个阿小，一个土生土长在小城镇；另一个为即将开始的大城市生活过度而寄居在小城镇。同样的名字，也沉浸在同样的幻想里，只是他们误读了城市，他们以为的城市就真的是电视屏幕上的那样，数不过来的高楼，四六分的香港发饰，衬衫，洁白的牙齿……于是，他们开始注重外表上的模仿和行为上的接近，在我看来，这正是一种青春期的迷茫，对于寻找自我定位时的种种探索，追求那些很酷很与众不同的感觉。但是结局却不尽如意： 大部分人都困倦到睡着了——他们都是一早七点准时在家门口等着这车到市区，他们出发前各自化妆、精心穿着，等着到这城市的各个角落，扮演起维修工、洗碗工、电器行销售、美发店小弟……时间一到，又仓皇地一路小跑赶这趟车，搭一两个小时回所谓的家，准备第二天的演出。 他们都是这城市的组成部分。而这城市，曾经是我们在小镇以为的，最美的天堂。他们是我们曾经认为的，活在天堂里的人。 而《天才文展》里的文展则代表着一种有远大抱负却囿于家庭环境的早熟的年轻人，也许我们都碰到过像这样的人，以一种居高临下的姿态看着你，对于自我有着严苛的要求，希望通过一项项计划通来向别人证明自己，来狠狠地扇曾经看不起自己的人一耳光。 我才明白，那封信里，我向文展说的“小时候的玩伴真该一起聚聚了”，真是个天真的提议。每个人都已经过上不同的生活，不同的生活让许多人在这个时空里没法相处在共同的状态中，除非等彼此都老了，年迈再次抹去其他，构成我们每个人最重要的标志，或许那时候的聚会才能成真。 然而结局却是悲剧的，‘我’最终过上了文展所希冀的生活而遭到文展的嫉妒和排斥，令人吃惊的是‘我’却也产生了一些负罪感。就像电视剧里经常碰到的桥段，“如果不是因为当初XXX，你现在所有的一切本该是我的。”其实这何尝不是自我欺骗呢，就算自我假设的前提条件不成立，结局却不见得会不一样，失败者把失败原因归结到自己以外的事物上注定还是会重蹈覆辙。 然后是《厚朴》描述的这么一个可爱的男生，英文名又是hope，伴着激情和叛逆的形象出场，不得不说让我想起身边一些充满朝气和正能量的人，平时还是很羡慕他们的，不过厚朴是不同的，开始我也很希望看看他所谓的“务虚”能走出一片天地来，俗话说就是理想主义者，因为我感觉自己更类似于作者。不过可惜的是最后结局让人痛心，像作者所形容的 不清楚真实的标准时，越用力就越让人觉得可笑。 不合时宜的东西，如果自己虚弱，终究会成为人们嘲笑的对象，但有力量了，或坚持久了，或许反而能成为众人追捧的魅力和个性——让我修正自己想法，产生这个判断的，是厚朴。 他以为自己做着摧毁一切规矩的事情，但其实一直活在规矩里。我以为自己战战兢兢地以活在规矩里为生活方式，但其实却对规矩有着将其彻底摧毁的欲望。 所以厚朴所表现出来的其实是一种沉浸在自己构造的幻想世界里而最终难以回到现实，他不愿接受自己失败了的现实，只是表现的叛逆。但话说回来我是希望能看到一种理想主义者最终实现理想的故事，我相信如果有那么一定会是精彩的。 说到这里我甚至有一种奇怪的感觉，是否厚朴其实就是作者的另一面，只是最终发现自己其实是在规则里画着圈而随着屈服而死掉的那部分。 最后几章主要简单谈了些关于城市、理想之类的话题，印象较之之前的几章倒是没那么深了，截取一段如下 只是我觉得城市不好。特别是中国的城市不好。厦门和中国大部分城市的建设都有个基础——人家国外的城市是怎么样的，以及人们该怎么被组织的，然后再依据这样的标准建设。中国近代的城市不是长出来的，不是培植出来的，不是催生出来的，而是一种安排。因为初期必然要混乱，所以中国的城市也表现出强大的秩序意识，人要干吗，路要怎么样。生长在这样环境里的人，除了维护秩序或者反抗秩序，似乎也难接受第二层次的思维了。 回头来看，几篇文章倒确实很契合《皮囊》这么一个主题，就像开头所述的，灵魂离不开皮囊，无论你如何讨厌自己的这幅皮囊，你的灵魂也得附庸其下。皮囊往小的说就是阿太所谓的驱壳和父亲半瘫的残疾，往大了说是一个人所处的位置，他背后的故事，就像母亲的那所房子，阿小的香港梦，厚朴的架子鼓。过去我常常看不懂电视剧里有些人的无可奈何，一厢情愿的认为这么容易的事换做是我直接三下五除二就解决了，实际情况却是不要以好坏善恶对错来划分人群，你看到的只是他当前展现给你的，你看不到的是他这一举动后面的挣扎，人是不能孤立和剥离来看的，当他站在你面前时，你得看到他的背景。 我常对朋友说，理解是对他人最大的善举。当你坐在一个人面前，听他开口说话，看得到各种复杂、精密的境况和命运，如何最终雕刻出这样的性格、思想、做法、长相，这才是理解。而有了这样的眼睛，你才算真正“看见”那个人，也才会发觉，这世界最美的风景，是一个个活出各自模样和体系的人。 如果每个人都是一段程序的话，那么我们在和不同的人交往的过程中就有意无意的修改了他们的代码，从此也许他们身上就带着你的痕迹；而身边亲近的人，更像是写进你基因里的代码段，在某个特定的时间里被激活，让你似曾相识，让你视线模糊。 无论如何，请带着这副皮囊好好生活。","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"Novel","slug":"Novel","permalink":"http://yoursite.com/tags/Novel/"}]}]