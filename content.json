[{"title":"关于交叉验证和模型选择的一点思考","date":"2017-06-01T12:28:36.000Z","path":"2017/06/01/cv/","text":"首先按照《统计学习方法》第一章的内容，常用的模型选择方法有两种，按照结构风险最小化思想的具体实现即正则化以及数据驱动的交叉验证方法。 CV是用于模型的(评估)挑选和超参数(关于超参数和参数的区别请见另一篇memo)调整的，首先明确一点，所谓模型，指的是我们用来描述输入数据与最终需要预测的输出数据之间的关联的方法，而不是不同数据拟合的模型得到的实例，比如我们可以说一个线性回归模型，但是不会说用不同数据拟合的线性回归器为不同的模型； 为什么要用CV，这涉及到数据划分的问题，首先我们训练好一个模型后需要评估这个模型的效果，但是如果把全部数据都投入训练就没有数据来进行验证了，常规而言，可以将数据集划分为训练集和验证集比如80%训练、20%验证，但是这样会有一些问题，首先有可能你很不巧的将一些特殊数据划分到这20%的验证集里了(要么效果很好、要么效果很糟糕)，这样的话仅仅这么一组验证集的评估效果就很不稳定也不确切；此外我们划分训练集和验证集就减少了训练数据，而一般而言数据越多训练出的模型方差是越小的，从这个角度来看数据越多一般我们训练的效果更好； 那么有没有方法既可以用上所有数据进行验证和评估、最后又可以用所有数据进行训练呢？所以我们就需要交叉验证，比如五折交叉验证，假设我们仍将数据集8-2划分（80%-20%），对于五折交叉验证，我们就是针对模型进行了五次训练，每次取80%训练数据、20%验证数据，并最终保证每个数据都曾在这五次训练验证中作为20%的验证集对模型进行过评估，这样我们就可以确保我们使用了所有数据对我们的模型进行评估 为什么不用CV中得到的预测器进行预测？一般而言，在做交叉验证时确实可能出现某一组交叉验证的得分较高，我们会试图用这一组数据进行模型的拟合和最终预测，但是这种得分高只是一种表面现象，首先数据量少了，其次这一组验证集并非独立的，是在整个交叉验证中随机生成的，所以这个预测器的结果到底好不好还需要在对额外的数据进行测试，如果数据重组也许你可以用嵌套式的交叉验证进行实验，即第一步用常规内层交叉验证确定最佳模型，然后采用数据驱动的方式(外层交叉验证)拟合最佳的一组预测器 所以CV的作用是用来对不同模型（SVM和生成树等），不同参数的模型中评估各个模型的效果，而得到最佳参数组合的模型；接下来将所有训练集投入进去拟合预测得到拟合好的预测器最后对测试集进行预测 那么把所有训练集放到模型中会不会导致过拟合呢？答案是不会，过拟合产生的原因是模型过于复杂（模型的参数），而不是数据增加导致，（而不是传入参数的值），增加数据一般而言更有利于训练集的训练 补充：关于生成树模型中的early_stopping，按照《统计学习方法》一书的第12章总结部分P213所述 提升方法没有现实的正则化项，通常通过早停止(early stopping)的方法达到正则化的效果 所以early_stopping属于正则化的范畴，是另一种模型选择的具体方法。 参考资料https://stats.stackexchange.com/a/52277/152084http://scikit-learn.org/stable/modules/cross_validation.htmlhttps://stats.stackexchange.com/a/52312/152084","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"CV","slug":"CV","permalink":"http://yoursite.com/tags/CV/"},{"name":"model selection","slug":"model-selection","permalink":"http://yoursite.com/tags/model-selection/"}]},{"title":"机器学习中模型的参数和超参数","date":"2017-06-01T12:24:11.000Z","path":"2017/06/01/parameter-hyperparameter/","text":"一直以来对于机器学习中的模型训练和模型选择存在一个误区，首先机器学习力的模型通俗来说就是一个函数关系，表明输入数据到输出数据的映射，基本的假设前提是输入数据和输出数据符合某种联合概率分布，而模型训练的过程其实就是在确定函数式的具体参数值的过程，比如假设你要做一个多项式回归分析的模型，比如$f(x)=w_1x_1+w_2x_2+w_3x_3$，那么模型训练的过程中其实就是在学习对应的w的值，那么问题来了，实战中所谓的模型调参来选择模型又指的是什么呢？既然训练已经把参数都确定下来了，那我们调整的参数又是什么？原来这里有个误区在于模型中的parameter和hyperparameter的区别，按照搜集到的资料来看，其实模型中可以分为两种参数，一种是在训练过程中学习到的参数，即parameter也就是上面公式里的w，而另一种参数则是hyperparameter，这种参数是模型中学习不到的，是我们预先定义的，而模型的调参其实指的是调整hyperparameter，而且不同类型的模型的hyperparameter也不尽相同，比如SVM中的C,树模型中的深度、叶子数以及比较常规的学习率等等，这种参数是在模型训练之前预先定义的，所以关于模型的选择其实更多的指的是选择最佳的hyperparameter组合。 参考资料https://datascience.stackexchange.com/a/14234/31117https://www.quora.com/What-are-hyperparameters-in-machine-learning","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"memo","slug":"memo","permalink":"http://yoursite.com/tags/memo/"},{"name":"parameter","slug":"parameter","permalink":"http://yoursite.com/tags/parameter/"},{"name":"hyperparameter","slug":"hyperparameter","permalink":"http://yoursite.com/tags/hyperparameter/"}]},{"title":"计算机与生活","date":"2017-05-30T11:57:01.000Z","path":"2017/05/30/computer-life/","text":"阅读完吴军的《浪潮之巅》，给人的感觉颇有点当初看纪录片《互联网时代》的感觉，简单而言就是你透过作者的眼睛快速浏览了一遍计算机这个行业的发展历程；手头的这本是第二版，看出版时间也是2012年了，到现在算算有点老了，不过对于12年以前发生的一些事情还是能有一个大致的梳理； 这本书上册主要讲述了一些在历史中唱过主角的公司，关于这些公司的兴衰以及作者自己的一些分析判断，下册则更宽泛一点，除了谈谈部分公司，还科普了一些计算机工业界以及商业方面的概念，比如谈信息产业的一些规律，一些公司的商业模式甚至风险投资，金融风暴等等； 下面我想简单谈一谈自己读过这本书的一点点体会，关于计算机和人类的生活。 我粗略的将计算机的发展历程划分为四个大的阶段，分别是计算时代—自动化时代—互联网时代—数据时代；计算时代比较有代表性的是计算机ENIAC，那个时候还主要用作军事用途，用来计算导弹轨迹之类的，离走进我们的生活还比较远，第二阶段自动化时代比较有代表性的就是微机了（当然这里跳过了面向企业的工作站，主要谈与个人生活相关的），按照作者的说法，这一阶段最有代表性的是三个公司，苹果的个人电脑（一体化）和微软与Intel的联盟（WinTel体系），这一阶段计算机真正开始走入普通人的家庭生活，主要是用于协助处理一些日常的办公工作，属于单机阶段；然后是大家比较熟悉的互联网时代，这一阶段的强大之处在于将计算机连接起来，于是每个用户都不再是孤立的，随着因特网的出现，有一部分人将本地的内容放到了网上，这个时候网上的内容不多，但是比较杂乱无序，所以随之诞生了门户网站比如雅虎等等，他们将互联网上的内容分门别类从而使用户便于查看和获取信息；而随着建立网站的门槛降低，互联网上的内容越来越多，单纯靠人工分类的的门户网站已经满足不了人们的需求了，于是搜索引擎（代表是Google）应运而生，帮助用户在浩瀚的互联网中快速找到自己所需的信息； 如果说互联网时代的关键词是‘内容’；那么数据时代的关键词就是平台了。这个时候，人们已经不满足于只是从互联网上获取信息了，就像不满足于只是从电视接受固定的节目一样，用户开始有‘发出自己的声音’的诉求，也就是创作；于是各个平台相继出现，最有代表性的，早期的Blog可以允许用户自己在网上发布自己的文章，然后是Facebook、Twitter不仅为用户构建了虚拟的社交圈，而且让用户可以在这个圈子里随时随地发出自己的声音并与其他人互动，还有YouTube把平台交给用户，让用户自己去发布自己的视频和收看其他用户的视频；这一阶段的特点是‘平台’ ，比较有代表性的公司并不提供内容，而是让用户自己来做内容的创造者。 而随之而来的是数据的爆发，互联网上的信息变得更加庞杂，在不断炒来炒去的概念‘云计算’，‘大数据’的驱动下，计算机的同学们纷纷投身到数据分析/数据挖掘的工作中，希望能够从庞杂的数据中挖掘出与实际业务相关的有价值的信息；那么，下一步人们的需求在哪呢？是通过数据挖掘对一个人建立画像从而实现对每个人的私人定制服务？还是现在炙手可热的基于VR/AR的虚拟社交？按照前段时间看到的一个关于区块链的演讲，在未来每个人都可以被“数字化”，真实世界的你可以投影到虚拟世界成为一个数字化的、独一无二的你，这大概是一种趋势吧。 最后还是推荐下这本书，作为科普类的读物挺不错的。","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"IT","slug":"IT","permalink":"http://yoursite.com/tags/IT/"}]},{"title":"来来来，我教你搭个博客好不好哇","date":"2017-05-30T10:40:58.000Z","path":"2017/05/30/my-blog/","text":"先把我的博客贴一下Lancelot’s Desert 端午节两天时间宅实验室把自己搭建一个博客这么个‘历史遗留问题’解决了下，其实之前也用Hugo搭建过，最后给弄崩了，这次尝试下Hexo，发现倒是异常的顺利，一方面Hexo的整体生态比较完备，另一方面网上找到的教程也很靠谱，下面就把我搭建过程中的一些步骤和踩过的坑记录下。 基础博客搭建首先声明下，基础博客搭建基本上和我所参照的崔斯特的教程是一致的，只是在顺序和语言上重新组织了一下，因为原博主写的教程已经很简明扼要了。 准备工作 git下载 github账号 node.js 另外的话写博客最好熟悉一点markdown的基本语法，常用的指令不多而且很简单，用的熟练是非常不错的工具，这里也推荐一个markdown的不错的在线编辑器Cmd markdown即时预览的，此外，类似简书、SegmentFault这些网站也是支持MarkDown编辑的，所以如果熟悉这个语法的话还是很方便的。 下载和注册基础搭建的话首先需要下载git和node.js(npm)来进行控制台命令的一些输入，以及需要一个github账号，因为我们的博客是挂载在github上的。 在下载好git和node.js并注册好一个github账号后，首先新建一个repository，名称和你的账户名一致，后面添加.github.io域名，比如我的用户名是LancelotHolmes,那么我的repository命名就是LancelotHolmes.github.io 环境配置在完成上述操作并安装好git和node.js之后，我们选择一个路径新建一个文件夹比如我是在D盘的MyBlog，然后执行git bash，可以在开始里搜索，也可以右键然后选择git bash here,在出现的git控制台中输入之前注册的github账号相对应信息，比如1git config --global user.name \"你的账户名\" 1git config --global user.email \"注册github账号时的邮箱\" 如图 然后是安装Hexo,直接输入 1npm install -g hexo-cli 开始搭建同样在MyBlog(或者你命名的目录下)，仍然是刚刚的控制台界面，输入1hexo init blog 成功的时候会显示 1INFO Start blogging with Hexo! 接下来进入blog目录下，输入123hexo cleanhexo ghexo s 或者你也可以新建一个generate.sh脚本文件将上面三条语句写入,因为后面线下测试会多次用到，可以直接在控制台输入./generate.sh，然后在浏览器里输入http://localhost:4000/,这个时候你就可以看到一个网页的基本雏形，这是由于你的\\blog\\source\\_posts路径下已经有一个基本的markdown文件了，而且本身下载的Hexo带了一个landscape的主题文件，在路径\\blog\\themes下可以看到 配置githubSSH回到控制台，现在我们需要生成SSH，仍然是在git控制台里，输入 1ssh-keygen -t rsa -C \"Github的注册邮箱地址\" 基本是一直回车，到出现信息1Your public key has been saved in /c/Users/user/.ssh/id_rsa.pub. 在信息对应的路径下找到这个文件，打开它(我是用sublime text打开的)，在你的github界面，右上角头像里选择Settings,左侧选择SSH and GPG keys然后新建一个SSH key,名称随你定，比如我是设置的blog,内容的话把刚刚id_rsa.pub里的内容全选复制进去就好了。 站点配置在blog目录下，用sublime或者其他编辑器打开_config.yml文件，找到对应的字段修改下面的基本参数信息，这里需要注意一下存在多个_config.yml文件，一个是在blog目录下，作为站点配置文件，一般用来做一些常规的配置，另外在各个主题的目录下还有一个_config.yml文件用来进行特定主体的一些个性化设置，后面会经常用到这两个文件，另外就是下面的配置注意:之后的空格 博客基本信息1234title: 博客名称subtitle: 副标题description: 网页描述author: 作者名 推送设置（这里的repo注意修改为自己的github的对应格式）1234deploy: type: git repo: https://github.com/LancelotHolmes/LancelotHolmes.github.io.git branch: master 新建文章在控制台输入 1hexo n \"文章名称\" 这样会在\\blog\\source\\_posts目录下生成对应的markdown文件，你可以通过编辑器打开它并书写你的文章，比如保存后同样的执行./generate.sh然后打开http://localhost:4000/就可以看到你刚刚写好的的文章了。 当然我这里展示的界面略有不同，因为我这里设置主题，后面会具体介绍。 推送到github线下测试发现没有什么问题我们就可以推送到github了，输入如下命令，或者保存为脚本文件deploy.sh然后执行./deploy.sh第一次部署到github时可能会出错error deployer not found:github，可以在控制台输入,注意--save必不可少1npm install hexo-deployer-git --save 12hexo cleanhexo d -g 过程中会让你输入你的github账号和密码，推送成功后，你就可以通过在浏览器输入你的静态站点名称访问你的博客了，比如LancelotHolmes.github.io,至此一个基本的博客就搭好了，接下来你每次需要写文章只需要经过如下步骤 在blog路径下打开git bash控制台然后输入hexo n &quot;文章名&quot; 在路径\\blog\\source\\_posts中编辑对应的markdown，编辑好后保存 执行generate.sh进行线下预览（可选） 执行deploy.sh推送到github就可以通过你的站点访问啦 配置yilia主题前面也给大家看到了我的博客的截图，这里我使用的是yilia主题，目前Hexo主题里面比较热门的两大主题是Next和yilia,这里我就我所配置的一些功能和踩过的坑记录一下。主要包括一些基本的配置以及优化 基本配置基本的主题下载和安装可以直接参照yilia的github对应的教程，同样在blog目录下执行git bash控制台，输入1git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 然后修改站点配置文件，Hexo根目录下(即blog目录的)的 _config.yml1theme: yilia 然后再该文件末尾添加如下语句123456789101112131415161718jsonContent: meta: false pages: false posts: title: true date: true path: true text: true raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true 添加disqus评论目前使用的评论比较多，我早期Hugo时使用过disqus也就还是用这个了，主要是需要注册一个disque账号，然后修改主题目录下的_config.yml 文件，特别注意这里的路径是\\blog\\themes\\yilia,不再是blog下的文件，后面大部分配置都是针对这个文件1disqus: LancelotHolmes 这里的修改为你注册的disqus的short-name,yilia主题下的配置会优先覆盖blog下的配置，如果你是使用其他的评论比如多说，顺言之类的应该适时修改相应的字段后面的false为对应的值，效果如下 添加menu原始的主题menu是这样的123menu: 主页: / 随笔: /tags/随笔/ ，我们可以修改为我们所需要的‘类别’，注意由于yilia作者没有预先设置类别（category）而是把他当作tags使，所以这里配置时链接到对应的tags下的路径，修改如下1234menu: 主页: / 阅读: /tags/Reading/ 机器学习: /tags/ML/ 可以根据需要在新建文章是设置标签自动生成对应的路径，可以在\\blog\\public\\tags下看到，然后修改menu下的对应字段即可 RSS &amp; Sitemap为什么要用RSS,可以看这篇短文，主要是为了方便对你的博客感兴趣的人将你的博客添加到他的订阅列表中，一旦你有更新他可以在第一时间接收到推送。而sitemap则主要是给搜索引擎用的，方便你的站点能够被google收录，当然这里首先需要绑定一个域名，后续我们会具体介绍。这部分主要是参照voidking和magicwangs的博客，执行如下语句123npm install hexo-generator-feed --savenpm install hexo-generator-sitemap --save 然后照常的执行generate.sh，你可以在路径\\blog\\public下看到生成的文件atom.xml和sitemap.xml,接下来在主题目录下的_config.yml 文件，特别注意这里的路径是\\blog\\themes\\yilia里添加123456# SubNavsubnav: github: \"#\" weibo: \"#\" rss: /atom.xml ... 此外在blog目录下的站点配置文件里(这回是在\\blog路径下的_config.yml)添加如下语句1234567891011# Sitemapsitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xm# RSSfeed: type: atom path: atom.xml limit: 100 头像设置有采用本地图片的，也有将图片传到网上制成外链的，我就是用后面那种方法，这里给大家推荐一个不错的工具sm.ms，方便你把你的本地图片传到网上制成markdown、html、url等格式的外部链接制作好后，修改主题目录下的_config.yml 文件，注意这里的路径是\\blog\\themes\\yilia里 12#你的头像urlavatar: https://ooo.0o0.ooo/2017/05/29/592be5c575c4c.jpg 链接当然是你刚刚生成的url的链接 其他社交外链同样是修改主题目录下的_config.yml 文件，注意这里的路径是\\blog\\themes\\yilia里，位置与之前的rss的地方差不多，根据你想展示的社交平台设置，注意链接是你的社交平台的主页之类的，比如1234# SubNavsubnav: github: \"https://github.com/LancelotHolmes/\" weibo: \"http://weibo.com/2925991784/profile?topnav=1&amp;wvr=6\" 文章截断直接生成的文章在主页会全部显示，如果不处理会占据较大的篇幅，我们可以在文章的特定位置设置文章截断，这样主页展示的就是部分文字，首先仍然是修改主题目录下的_config.yml 文件，注意这里的路径是\\blog\\themes\\yilia里 1234# Content# 文章太长，截断按钮文字excerpt_link: more 然后在你的文章的md文件里你想要阶段的位置插入语句1&lt;!-- more --&gt; 例如 这样就实现了文章的截断，而需要阅读全文只需要点击相应的按钮或者标题即可 favicon这个主要是为了好玩，就是你的网页打开后的在浏览器的上面的一个小图标，比如github和我的博客的favicon 我是在freefavicon上直接选的一个，如果你有兴趣可以自己制作1616的图片就行了，将图片复制到路径\\blog\\public下，然后在*主题目录下的_config.yml 文件里修改即可 1favicon: /favicon.png 文章目录这个是作者目前没有实现的部分，但是有其他的方法，主要参照的是这个post,修改主要涉及这么两个文件 在/themes/yilia/layout/_partial/article.ejs文件里18行左右的位置插入 1234567891011121314&lt;% if (!index)&#123; %&gt; &lt;% if (toc(post.content))&#123; %&gt; &lt;div id=\"toc\" class=\"article-toc\"&gt; &lt;h2&gt;目录&lt;/h2&gt; &lt;%- toc(post.content) %&gt; &lt;/div&gt; &lt;script type=\"text/javascript\"&gt; var _article = document.getElementsByClassName('article')[0]; &lt;!-- setTimeout(\"_article.style.marginRight = '211px'\",0); --&gt; setTimeout(\"_article.className += ' article2'\",0); setTimeout(\"document.getElementById('toc').style.right = '15px'\", 0); &lt;/script&gt; &lt;% &#125; %&gt; &lt;% &#125; %&gt; 在\\themes\\yilia\\source-src\\css\\article.scss文件的末尾添加 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@media (max-width: 1099px)&#123; #toc&#123; display: none; &#125; &#125; @media (min-width: 1100px) &#123; #toc&#123; z-index: 999; background-color: #fff; padding: 0 1em; border:1px solid #ddd; position: fixed; top: 100px; right: -180px; transition: right .5s ease-in; width: 150px; h2&#123; margin-bottom:10px; &#125; ol&#123; padding-left: 0!important; &#125; line-height: 1.3em; font-size: 0.8em; float: right; .toc&#123; padding: 0; li&#123; list-style-type: none; margin: .5em 0 .5em; ol&#123; margin: .5em 0 .5em 1em; &#125; &#125; &#125; &#125; .article2&#123; margin-right: 211px; transition: margin-right .5s ease-in; &#125;&#125;.toc-item span &#123;display: table-cell;&#125;span.toc-text &#123;padding-left: 3px;&#125; 即可实现，如图如果没有效果，可以尝试在需要目录的文章头部添加如下语句123456---title: 使用机器学习识别出拍卖场中作弊的机器人用户date: 2017-05-29 19:27:51tags: [ML, Kaggle, Python]toc: true--- 以上内容基本可以搭建起一个还不错的博客了，当然如果你需要其他的一些好玩的功能比如标签云啊，浏览量、动态特效之类的就多多使用搜索引擎咯，另外，遇到什么问题大部分都能在对应的github的issues找到答案。 域名绑定最后就介绍下域名的绑定，如果你也不满足使用github的二级域名，而是想使用专属于你自己的域名，那么这部分就是为你而准备的。 这部分也是参照了不少文章，比如Setsuna和水瓶座IOSer，还有zhaozhiming下面我就简单的叙述一下。 使用工具 namesilo: 购买域名的地方，经过一番查阅感觉这个相对靠谱 DNSPod: 国内的免费DNS服务，后面将域名的dns转移到这个上面 首先要绑定域名自然需要购买一个域名，国内的话可以试试阿里云、万网，学生的话好像可以试试腾讯云的云服务器，是会送免费的.cn域名，但是国内购买的话需要去公安局备份好象，感觉比较麻烦我就是用了国外的，比如namesilo，其他的介绍可以看看zhaozhiming的对应观点，当然他和我一样也是在知乎上的一个地方看到的； 在namesilo上注册的教程可以看这个,选择好域名并付款后（支持支付宝）的基本设置可以看这个，接下来： CNAME在你的本地站点目录里的source目录下添加一个CNAME文件，不带后缀，用编辑器打开并输入你购买的域名，不要http也不要www,比如我的域名是izhaoyi.top,我就写入izhaoyi.top然后保存以后执行deploy。 DNS设置namesilo登陆namesilo之后，右上角的Manage My Domains点击进入后,选择然后下拉，选择在上面手动添加两条记录，如图然后回到域名管理界面，选中域名那一栏最前面的勾选框，然后选择上面的Change Namesevers图标最后在在NameServer1和NameServer2中填写 DNSPod 的 nameserver 地址f1g1ns1.dnspod.net，f1g1ns2.dnspod.net DNSPod同样完成注册后，在域名注册中需要手动添加你购买的域名，并添加一些记录，最终如图这里的192.30.252.153是github pages的ip地址，固定设置成这个就好，然后稍微等一段时间（也许不用等）应该就可以通过访问你的域名比如izhaoyi.top或者你之前的github站点名比如LancelotHolmes.github.io访问你的博客啦。 最后，欢迎大家来我的博客踩踩，也欢迎跟我互加友链啊~","tags":[{"name":"blog","slug":"blog","permalink":"http://yoursite.com/tags/blog/"},{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"yilia","slug":"yilia","permalink":"http://yoursite.com/tags/yilia/"},{"name":"Coding","slug":"Coding","permalink":"http://yoursite.com/tags/Coding/"}]},{"title":"使用机器学习识别出拍卖场中作弊的机器人用户(二)","date":"2017-05-29T12:52:19.000Z","path":"2017/05/29/HumenOrRobot2/","text":"本文承接上一篇文章:使用机器学习识别出拍卖场中作弊的机器人用户 本项目为kaggle上Facebook举行的一次比赛，地址见数据来源，完整代码见我的github,欢迎来玩~ 代码 数据探索——Data_Exploration.ipynb 数据预处理&amp;特征工程——Feature_Engineering.ipynb &amp; Feature_Engineering2.ipynb 模型设计及评测——Model_Design.ipynb 项目数据来源 kaggle项目所需额外工具包 numpy pandas matplotlib sklearn xgboost lightgbm mlxtend: 含有聚和算法Stacking项目整体运行时间预估为60min左右，在Ubuntu系统，8G内存，运行结果见所提交的jupyter notebook文件 由于文章内容过长，所以分为两篇文章，总共包含四个部分 数据探索 数据预处理及特征工程 模型设计 评估及总结 特征工程(续)12345import numpy as npimport pandas as pdimport pickle%matplotlib inlinefrom IPython.display import display 12# bids = pd.read_csv('bids.csv')bids = pickle.load(open('bids.pkl')) 12print bids.shapedisplay(bids.head()) (7656329, 9) bid_id bidder_id auction merchandise device time country ip url 0 1 2 3 4 0 8dac2b259fd1c6d1120e519fb1ac14fbqvax8 ewmzr jewelry phone0 9759243157894736 us 69.166.231.58 vasstdc27m7nks3 1 668d393e858e8126275433046bbd35c6tywop aeqok furniture phone1 9759243157894736 in 50.201.125.84 jmqlhflrzwuay9c 2 aa5f360084278b35d746fa6af3a7a1a5ra3xe wa00e home goods phone2 9759243157894736 py 112.54.208.157 vasstdc27m7nks3 3 3939ac3ef7d472a59a9c5f893dd3e39fh9ofi jefix jewelry phone4 9759243157894736 in 18.99.175.133 vasstdc27m7nks3 4 8393c48eaf4b8fa96886edc7cf27b372dsibi jefix jewelry phone5 9759243157894736 in 145.138.5.37 vasstdc27m7nks3 1bidders = bids.groupby('bidder_id') 针对国家、商品单一特征多类别转换为多个独立特征进行统计1234567891011121314cates = (bids['merchandise'].unique()).tolist()countries = (bids['country'].unique()).tolist()def dummy_coun_cate(group): coun_cate = dict.fromkeys(cates, 0) coun_cate.update(dict.fromkeys(countries, 0)) for cat, value in group['merchandise'].value_counts().iteritems(): coun_cate[cat] = value for c in group['country'].unique(): coun_cate[c] = 1 coun_cate = pd.Series(coun_cate) return coun_cate 1bidder_coun_cate = bidders.apply(dummy_coun_cate) 12display(bidder_coun_cate.describe())bidder_coun_cate.to_csv('coun_cate.csv') - ad ae af ag al am an ao ar at … count 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 … mean 0.002724 0.205629 0.054774 0.001059 0.048570 0.023907 0.000303 0.036314 0.120442 0.052655 … std 0.052121 0.404191 0.227555 0.032530 0.214984 0.152770 0.017395 0.187085 0.325502 0.223362 … min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 … 25% 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 … 50% 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 … 75% 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 … max 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 … 同样的，对于每个用户需要统计他对于自己每次竞拍行为的时间间隔情况 12345678910111213141516171819def bidder_interval(group): time_diff = np.ediff1d(group['time']) bidder_interval = &#123;&#125; if len(time_diff) == 0: diff_mean = 0 diff_std = 0 diff_median = 0 diff_zeros = 0 else: diff_mean = np.mean(time_diff) diff_std = np.std(time_diff) diff_median = np.median(time_diff) diff_zeros = time_diff.shape[0] - np.count_nonzero(time_diff) bidder_interval['tmean'] = diff_mean bidder_interval['tstd'] = diff_std bidder_interval['tmedian'] = diff_median bidder_interval['tzeros'] = diff_zeros bidder_interval = pd.Series(bidder_interval) return bidder_interval 1bidder_inv = bidders.apply(bidder_interval) 12display(bidder_inv.describe())bidder_inv.to_csv('bidder_inv.csv') - tmean tmedian tstd tzeros count 6.609000e+03 6.609000e+03 6.609000e+03 6609.0 mean 2.933038e+12 1.860285e+12 3.440901e+12 122.986231 std 8.552343e+12 7.993497e+12 6.512992e+12 3190.805229 min 0.000000e+00 0.000000e+00 0.000000e+00 0.000000 25% 1.192853e+10 2.578947e+09 1.749995e+09 0.000000 50% 2.641139e+11 5.726316e+10 5.510107e+11 0.000000 75% 1.847456e+12 6.339474e+11 2.911282e+12 0.000000 max 7.610295e+13 7.610295e+13 3.800092e+13 231570.000000 按照用户-拍卖场分组进一步分析之前的统计是按照用户进行分组，针对各个用户从整体上针对竞标行为统计其各项特征，下面根据拍卖场来对用户进一步细分，看一看每个用户在不同拍卖场的行为模式,类似上述按照用户分组来统计各个用户的各项特征，针对用户-拍卖场结对分组进行统计以下特征 基本计数统计，针对各个用户在各个拍卖场统计设备、国家、ip、url、商品类别、竞标次数等特征的数目作为新的特征 时间间隔统计：统计各个用户在各个拍卖场每次竞拍的时间间隔的 均值、方差、中位数和0值 针对商品类别、国家进一步转化为多类别进行统计 123456789101112131415161718192021222324252627282930313233343536def auc_features_count(group): time_diff = np.ediff1d(group['time']) if len(time_diff) == 0: diff_mean = 0 diff_std = 0 diff_median = 0 diff_zeros = 0 else: diff_mean = np.mean(time_diff) diff_std = np.std(time_diff) diff_median = np.median(time_diff) diff_zeros = time_diff.shape[0] - np.count_nonzero(time_diff) row = dict.fromkeys(cates, 0) row.update(dict.fromkeys(countries, 0)) row['devices_c'] = group['device'].unique().shape[0] row['countries_c'] = group['country'].unique().shape[0] row['ip_c'] = group['ip'].unique().shape[0] row['url_c'] = group['url'].unique().shape[0]# row['merch_c'] = group['merchandise'].unique().shape[0] row['bids_c'] = group.shape[0] row['tmean'] = diff_mean row['tstd'] = diff_std row['tmedian'] = diff_median row['tzeros'] = diff_zeros for cat, value in group['merchandise'].value_counts().iteritems(): row[cat] = value for c in group['country'].unique(): row[c] = 1 row = pd.Series(row) return row 1bidder_auc = bids.groupby(['bidder_id', 'auction']).apply(auc_features_count) 1bidder_auc.to_csv('bids_auc.csv') 1print bidder_auc.shape (382336, 218) 模型设计与参数评估合并特征对之前生成的各项特征进行合并产生最终的特征空间 1234import numpy as npimport pandas as pd%matplotlib inlinefrom IPython.display import display 首先将之前根据用户分组的统计特征合并起来，然后将其与按照用户-拍卖场结对分组的特征合并起来，最后加上时间特征，分别于训练集、测试集连接生成后续进行训练和预测的特征数据文件 123456789101112131415161718192021222324def merge_data(): train = pd.read_csv('train.csv') test = pd.read_csv('test.csv') time_differences = pd.read_csv('tdiff.csv', index_col=0) bids_auc = pd.read_csv('bids_auc.csv') bids_auc = bids_auc.groupby('bidder_id').mean() bidders = pd.read_csv('cnt_bidder.csv', index_col=0) country_cate = pd.read_csv('coun_cate.csv', index_col=0) bidder_inv = pd.read_csv('bidder_inv.csv', index_col=0) bidders = bidders.merge(country_cate, right_index=True, left_index=True) bidders = bidders.merge(bidder_inv, right_index=True, left_index=True) bidders = bidders.merge(bids_auc, right_index=True, left_index=True) bidders = bidders.merge(time_differences, right_index=True, left_index=True) train = train.merge(bidders, left_on='bidder_id', right_index=True) train.to_csv('train_full.csv', index=False) test = test.merge(bidders, left_on='bidder_id', right_index=True) test.to_csv('test_full.csv', index=False) 1merge_data() 1234train_full = pd.read_csv('train_full.csv')test_full = pd.read_csv('test_full.csv')print train_full.shapeprint test_full.shape (1983, 445) (4626, 444) 123456789train_full['outcome'] = train_full['outcome'].astype(int)ytrain = train_full['outcome']train_full.drop('outcome', 1, inplace=True)test_ids = test_full['bidder_id']labels = ['payment_account', 'address', 'bidder_id']train_full.drop(labels=labels, axis=1, inplace=True)test_full.drop(labels=labels, axis=1, inplace=True) 设计交叉验证模型选择根据之前的分析，由于当前的数据集中存在正负例不均衡的问题，所以考虑选取了RandomForestClassfier, GradientBoostingClassifier, xgboost, lightgbm等四种模型来针对数据及进行训练和预测，确定最终模型的基本思路如下： 对四个模型分别使用评价函数roc_auc进行交叉验证并绘制auc曲线，对各个模型的多轮交叉验证得分取平均值并输出 根据得分确定最终选用的一个或多个模型 若最后发现一个模型的表现大幅度优于其他所有模型，则选择该模型进一步调参 若最后发现多个模型表现都不错，则进行模型的集成，得到聚合模型 使用GridSearchCV来从人为设定的参数列表中选择最佳的参数组合确定最终的模型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354from scipy import interpimport matplotlib.pyplot as pltfrom itertools import cycle# from sklearn.cross_validation import StratifiedKFoldfrom sklearn.model_selection import StratifiedKFoldfrom sklearn.metrics import roc_auc_score, roc_curve, aucdef kfold_plot(train, ytrain, model):# kf = StratifiedKFold(y=ytrain, n_folds=5) kf = StratifiedKFold(n_splits=5) scores = [] mean_tpr = 0.0 mean_fpr = np.linspace(0, 1, 100) exe_time = [] colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue']) lw = 2 i=0 for (train_index, test_index), color in zip(kf.split(train, ytrain), colors): X_train, X_test = train.iloc[train_index], train.iloc[test_index] y_train, y_test = ytrain.iloc[train_index], ytrain.iloc[test_index] begin_t = time.time() predictions = model(X_train, X_test, y_train) end_t = time.time() exe_time.append(round(end_t-begin_t, 3))# model = model# model.fit(X_train, y_train) # predictions = model.predict_proba(X_test)[:, 1] scores.append(roc_auc_score(y_test.astype(float), predictions)) fpr, tpr, thresholds = roc_curve(y_test, predictions) mean_tpr += interp(mean_fpr, fpr, tpr) mean_tpr[0] = 0.0 roc_auc = auc(fpr, tpr) plt.plot(fpr, tpr, lw=lw, color=color, label='ROC fold %d (area = %0.2f)' % (i, roc_auc)) i += 1 plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k', label='Luck') mean_tpr /= kf.get_n_splits(train, ytrain) mean_tpr[-1] = 1.0 mean_auc = auc(mean_fpr, mean_tpr) plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--', label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw) plt.xlim([-0.05, 1.05]) plt.ylim([-0.05, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Receiver operating characteristic') plt.legend(loc='lower right') plt.show() print 'mean scores: ',np.mean(scores) print 'mean model process time: ',np.mean(exe_time), 's' RandomForestClassifier12from sklearn.model_selection import GridSearchCVimport time 1234567from sklearn.ensemble import RandomForestClassifierdef forest_model(X_train, X_test, y_train): model = RandomForestClassifier(n_estimators=160, max_features=35, max_depth=8, random_state=7) model.fit(X_train, y_train) predictions = model.predict_proba(X_test)[:, 1] return predictions 12kfold_plot(train_full, ytrain, forest_model)# kfold_plot(train_full, ytrain, model_forest) mean scores: 0.909571935157 mean model process time: 0.6372 s 123456from sklearn.ensemble import GradientBoostingClassifierdef gradient_model(X_train, X_test, y_train): model = GradientBoostingClassifier(n_estimators=200, random_state=7, max_depth=5, learning_rate=0.03) model.fit(X_train, y_train) predictions = model.predict_proba(X_test)[:, 1] return predictions 1kfold_plot(train_full, ytrain, gradient_model) mean scores: 0.911847771023 mean model process time: 4.007 s 123456789import xgboost as xgbdef xgboost_model(X_train, X_test, y_train): X_train = xgb.DMatrix(X_train.values, label=y_train.values) X_test = xgb.DMatrix(X_test.values) params = &#123;'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': 1, 'seed': 7, 'max_depth': 6, 'eta': 0.01&#125; model = xgb.train(params, X_train, 600) predictions = model.predict(X_test) return predictions 1kfold_plot(train_full, ytrain, xgboost_model) mean scores: 0.915372340426 mean model process time: 4.855 s 1234567import lightgbm as lgbdef lightgbm_model(X_train, X_test, y_train): X_train = lgb.Dataset(X_train.values, y_train.values) params = &#123;'objective': 'binary', 'metric': &#123;'auc'&#125;, 'learning_rate': 0.01, 'max_depth': 6, 'seed': 7&#125; model = lgb.train(params, X_train, num_boost_round=600) predictions = model.predict(X_test) return predictions 1kfold_plot(train_full, ytrain, lightgbm_model) mean scores: 0.921512158055 mean model process time: 0.4236 s 模型比较比较四个模型在交叉验证机上的roc_auc平均得分和模型训练的时间 123456789data_source = ['forest', 'gradient boosting', 'xgboost', 'lightgbm']y_pos = np.arange(len(data_source))model_auc = [0.910, 0.912, 0.915, 0.922]barlist = plt.bar(y_pos, model_auc, align='center', alpha=0.5)barlist[3].set_color('r')plt.xticks(y_pos, data_source)plt.ylabel('roc-auc score')plt.title('Model Performance')plt.show() 123456789data_source = ['forest', 'gradient boosting', 'xgboost', 'lightgbm']y_pos = np.arange(len(data_source))model_auc = [0.6372,4.007, 4.855, 0.4236]barlist = plt.bar(y_pos, model_auc, align='center', alpha=0.5)barlist[3].set_color('r')plt.xticks(y_pos, data_source)plt.ylabel('time(s)')plt.title('Time of Building Model')plt.show() 12345678910111213141516171819auc_forest = [0.87, 0.93, 0.94, 0.86, 0.96]auc_gb = [0.89, 0.91, 0.93, 0.87, 0.95]auc_xgb = [0.89,0.92, 0.93, 0.89, 0.95]auc_lgb = [0.90, 0.93, 0.94, 0.88, 0.96]print 'std of forest auc score: ',np.std(auc_forest)print 'std of gbm auc score: ',np.std(auc_gb)print 'std of xgboost auc score: ',np.std(auc_xgb)print 'std of lightgbm auc score: ',np.std(auc_lgb)data_source = ['roc-fold-1', 'roc-fold-2', 'roc-fold-3', 'roc-fold-4', 'roc-fold-5']y_pos = np.arange(len(data_source))plt.plot(y_pos, auc_forest, 'b-', label='forest')plt.plot(y_pos, auc_gb, 'r-', label='gbm')plt.plot(y_pos, auc_xgb, 'y-', label='xgboost')plt.plot(y_pos, auc_lgb, 'g-', label='lightgbm')plt.title('roc-auc score of each epoch')plt.xlabel('epoch')plt.ylabel('roc-auc score')plt.legend()plt.show() std of forest auc score: 0.0396988664826 std of gbm auc score: 0.0282842712475 std of xgboost auc score: 0.0233238075794 std of lightgbm auc score: 0.0285657137142 单从5次交叉验证的各模型roc-auc得分来看，xgboost的得分相对比较稳定 聚合模型由上面的模型比较可以发现，四个模型的经过交叉验证的表现都不错，但是综合而言，xgboost和lightgbm更胜一筹，而且两者的训练时间也相对更短一些，所以接下来考虑进行模型的聚合，思路如下： 先通过GridSearchCV分别针对四个模型在整个训练集上进行调参获得最佳的子模型 针对子模型使用 stacking: 第三方库mlxtend里的stacking方法对子模型进行聚合得到聚合模型，并采用之前相同的cv方法对该模型进行打分评价 voting: 使用sklearn内置的VotingClassifier进行四个模型的聚合 最终对聚合模型在一次进行cv验证评分，根据结果确定最终的模型 先通过交叉验证针对模型选择参数组合 12345678910def choose_xgb_model(X_train, y_train): tuned_params = [&#123;'objective': ['binary:logistic'], 'learning_rate': [0.01, 0.03, 0.05], 'n_estimators': [100, 150, 200], 'max_depth':[4, 6, 8]&#125;] begin_t = time.time() clf = GridSearchCV(xgb.XGBClassifier(seed=7), tuned_params, scoring='roc_auc') clf.fit(X_train, y_train) end_t = time.time() print 'train time: ',round(end_t-begin_t, 3), 's' print 'current best parameters of xgboost: ',clf.best_params_ return clf.best_estimator_ 1bst_xgb = choose_xgb_model(train_full, ytrain) train time: 86.216 s current best parameters of xgboost: {&apos;n_estimators&apos;: 150, &apos;objective&apos;: &apos;binary:logistic&apos;, &apos;learning_rate&apos;: 0.05, &apos;max_depth&apos;: 4} 12345678910def choose_lgb_model(X_train, y_train): tuned_params = [&#123;'objective': ['binary'], 'learning_rate': [0.01, 0.03, 0.05], 'n_estimators': [100, 150, 200], 'max_depth':[4, 6, 8]&#125;] begin_t = time.time() clf = GridSearchCV(lgb.LGBMClassifier(seed=7), tuned_params, scoring='roc_auc') clf.fit(X_train, y_train) end_t = time.time() print 'train time: ',round(end_t-begin_t, 3), 's' print 'current best parameters of lgb: ',clf.best_params_ return clf.best_estimator_ 1bst_lgb = choose_lgb_model(train_full, ytrain) train time: 16.602 s current best parameters of lgb: {&apos;n_estimators&apos;: 150, &apos;objective&apos;: &apos;binary&apos;, &apos;learning_rate&apos;: 0.05, &apos;max_depth&apos;: 4} 先使用stacking聚合两个综合表现最佳的模型lgb和xgb 1234567from mlxtend.classifier import StackingClassifierdef stacking_model(X_train, X_test, y_train): sclf = StackingClassifier(classifiers=[bst_xgb], use_probas=True, average_probas=False, meta_classifier=bst_lgb) sclf.fit(X_train, y_train) predictions = sclf.predict_proba(X_test)[:, 1] return predictions 1kfold_plot(train_full, ytrain, stacking_model) mean scores: 0.880479989868 mean model process time: 0.8142 s 然而两个单独表现最佳的模型经过stacking的聚合模型表现反而不如之前的任何一个单一模型，考虑对四个模型进行stacking聚和操作 123456789def choose_forest_model(X_train, y_train): tuned_params = [&#123;'n_estimators': [100, 150, 200], 'max_features': [8, 15, 30], 'max_depth':[4, 8, 10]&#125;] begin_t = time.time() clf = GridSearchCV(RandomForestClassifier(random_state=7), tuned_params, scoring='roc_auc') clf.fit(X_train, y_train) end_t = time.time() print 'train time: ',round(end_t-begin_t, 3), 's' print 'current best parameters: ',clf.best_params_ return clf.best_estimator_ 1bst_forest = choose_forest_model(train_full, ytrain) train time: 42.852 s current best parameters: {&apos;max_features&apos;: 15, &apos;n_estimators&apos;: 150, &apos;max_depth&apos;: 8} 12345678910def choose_gradient_model(X_train, y_train): tuned_params = [&#123;'n_estimators': [100, 150, 200], 'learning_rate': [0.03, 0.05, 0.07], 'min_samples_leaf': [8, 15, 30], 'max_depth':[4, 6, 8]&#125;] begin_t = time.time() clf = GridSearchCV(GradientBoostingClassifier(random_state=7), tuned_params, scoring='roc_auc') clf.fit(X_train, y_train) end_t = time.time() print 'train time: ',round(end_t-begin_t, 3), 's' print 'current best parameters: ',clf.best_params_ return clf.best_estimator_ 1bst_gradient = choose_gradient_model(train_full, ytrain) train time: 632.815 s current best parameters: {&apos;n_estimators&apos;: 100, &apos;learning_rate&apos;: 0.03, &apos;max_depth&apos;: 8, &apos;min_samples_leaf&apos;: 30} 123456def stacking_model2(X_train, X_test, y_train): sclf = StackingClassifier(classifiers=[bst_xgb, bst_forest, bst_gradient], use_probas=True, average_probas=False, meta_classifier=bst_lgb) sclf.fit(X_train, y_train) predictions = sclf.predict_proba(X_test)[:, 1] return predictions 1kfold_plot(train_full, ytrain, stacking_model2) mean scores: 0.899170466059 mean model process time: 4.1236 s 可以看到四个模型的聚合效果比用两个模型的stacking聚合效果要好一点，但是相比较单一模型仍然效果不好，接下来考虑使用voting对四个模型进行聚合 12345678from sklearn.ensemble import VotingClassifierdef voting_model(X_train, X_test, y_train): vclf = VotingClassifier(estimators=[('xgb', bst_xgb), ('rf', bst_forest), ('gbm',bst_gradient), ('lgb', bst_lgb)], voting='soft', weights=[2, 1, 1, 2]) vclf.fit(X_train, y_train) predictions = vclf.predict_proba(X_test)[:, 1] return predictions 1kfold_plot(train_full, ytrain, voting_model) mean scores: 0.926889564336 mean model process time: 4.2736 s 由上可以看到最终通过voting将四个模型进行聚合可以得到得分最高的模型，确定为最终模型 综合模型，对测试文件进行最终预测12345678910111213141516# predict(train_full, test_full, y_train)def submit(X_train, X_test, y_train, test_ids): predictions = voting_model(X_train, X_test, y_train) sub = pd.read_csv('sampleSubmission.csv') result = pd.DataFrame() result['bidder_id'] = test_ids result['outcome'] = predictions sub = sub.merge(result, on='bidder_id', how='left') # Fill missing values with mean mean_pred = np.mean(predictions) sub.fillna(mean_pred, inplace=True) sub.drop('prediction', 1, inplace=True) sub.to_csv('result.csv', index=False, header=['bidder_id', 'prediction']) 1submit(train_full, test_full, ytrain, test_ids) 最终结果提交到kaggle上进行评分，得分如下 以上就是整个完整的流程，当然还有很多模型可以尝试，很多聚合方法也可以使用，此外，特征工程部分还有很多空间可以挖掘，就留给大家去探索啦~ 参考资料1 Chen, K. T., Pao, H. K. K., &amp; Chang, H. C. (2008, October). Game bot identification based on manifold learning. In Proceedings of the 7th ACM SIGCOMM Workshop on Network and System Support for Games (pp. 21-26). ACM.2 Alayed, H., Frangoudes, F., &amp; Neuman, C. (2013, August). Behavioral-based cheating detection in online first person shooters using machine learning techniques. In Computational Intelligence in Games (CIG), 2013 IEEE Conference on (pp. 1-8). IEEE.3 https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot/data4 http://stats.stackexchange.com/a/132832/1520845 https://en.wikipedia.org/wiki/Receiver_operating_characteristic6 https://en.wikipedia.org/wiki/Random_forest7 https://en.wikipedia.org/wiki/Gradient_boosting8 https://xgboost.readthedocs.io/en/latest//parameter.html#parameters-for-tree-booster9 https://github.com/Microsoft/LightGBM10 https://en.wikipedia.org/wiki/Receiver_operating_characteristic11 http://stackoverflow.com/questions/29530232/python-pandas-check-if-any-value-is-nan-in-dataframe[12] http://pandas.pydata.org/pandas-docs/stable/missing_data.html[13] http://stackoverflow.com/a/18272653/6653189[14] http://www.cnblogs.com/jasonfreak/p/5720137.html","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"使用机器学习识别出拍卖场中作弊的机器人用户","date":"2017-05-29T11:27:51.000Z","path":"2017/05/29/HumenOrRobot/","text":"本项目为kaggle上Facebook举行的一次比赛，地址见数据来源，完整代码见我的github,欢迎来玩~ 代码 数据探索——Data_Exploration.ipynb 数据预处理&amp;特征工程——Feature_Engineering.ipynb &amp; Feature_Engineering2.ipynb 模型设计及评测——Model_Design.ipynb 项目数据来源 kaggle项目所需额外工具包 numpy pandas matplotlib sklearn xgboost lightgbm mlxtend: 含有聚和算法Stacking项目整体运行时间预估为60min左右，在Ubuntu系统，8G内存，运行结果见所提交的jupyter notebook文件 由于文章内容过长，所以分为两篇文章，总共包含四个部分 数据探索 数据预处理及特征工程 模型设计 评估及总结 数据探索1234import numpy as npimport pandas as pd%matplotlib inlinefrom IPython.display import display 123df_bids = pd.read_csv('bids.csv', low_memory=False)df_train = pd.read_csv('train.csv')df_test = pd.read_csv('test.csv') 1df_bids.head(3) bid_id bidder_id auction merchandise device time country ip url 0 0 8dac2b259fd1c6d1120e519fb1ac14fbqvax8 ewmzr jewelry phone0 9759243157894736 us 69.166.231.58 vasstdc27m7nks3 1 1 668d393e858e8126275433046bbd35c6tywop aeqok furniture phone1 9759243157894736 in 50.201.125.84 jmqlhflrzwuay9c 2 2 aa5f360084278b35d746fa6af3a7a1a5ra3xe wa00e home goods phone2 9759243157894736 py 112.54.208.157 vasstdc27m7nks3 12df_train.head(3)# df_train.dtypes bidder_id payment_account address outcome 0 91a3c57b13234af24875c56fb7e2b2f4rb56a a3d2de7675556553a5f08e4c88d2c228754av a3d2de7675556553a5f08e4c88d2c228vt0u4 0.0 1 624f258b49e77713fc34034560f93fb3hu3jo a3d2de7675556553a5f08e4c88d2c228v1sga ae87054e5a97a8f840a3991d12611fdcrfbq3 0.0 2 1c5f4fc669099bfbfac515cd26997bd12ruaj a3d2de7675556553a5f08e4c88d2c2280cybl 92520288b50f03907041887884ba49c0cl0pd 0.0 异常数据检测1234# 查看各表格中是否存在空值print 'Is there any missing value in bids?',df_bids.isnull().any().any()print 'Is there any missing value in train?',df_train.isnull().any().any()print 'Is there any missing value in test?',df_test.isnull().any().any() Is there any missing value in bids? True Is there any missing value in train? False Is there any missing value in test? False 整个对三个数据集进行空值判断，发现用户数据训练集和测试集均无缺失数据，而在竞标行为数据集中存在缺失值的情况，下面便针对bids数据进一步寻找缺失值 123# nan_rows = df_bids[df_bids.isnull().T.any().T]# print nan_rowspd.isnull(df_bids).any() bid_id False bidder_id False auction False merchandise False device False time False country True ip False url False dtype: bool 1234missing_country = df_bids['country'].isnull().sum().sum()print 'No. of missing country: ', missing_countrynormal_country = df_bids['country'].notnull().sum().sum()print 'No. of normal country: ', normal_country No. of missing country: 8859 No. of normal country: 7647475 123456789import matplotlib.pyplot as pltlabels = ['unknown', 'normal']sizes = [missing_country, normal_country]explode = (0.1, 0)fig1, ax1 = plt.subplots()ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)ax1.axis('equal')plt.title('Distribution of missing countries vs. normal countries')plt.show() 综合上述的分析可以发现，在竞标行为用户的country一栏属性中存在很少一部分用户行为是没有country记录的，在预处理部分可以针对这部分缺失数据进行填充操作，有两种思路： 针对原始行为数据按照用户分组后，看看每个对应的用户竞标时经常所位于的国家信息，对缺失值填充常驻国家 针对原始行为数据按照用户分组后，按时间顺序对每组用户中的缺失值前向或后向填充相邻的国家信息 12345678# 查看各个数据的记录数# 看看数据的id是否是唯一标识print df_bids.shape[0]print len(df_bids['bid_id'].unique())print df_train.shape[0]print len(df_train['bidder_id'].unique())print df_test.shape[0]print len(df_test['bidder_id'].unique()) 7656334 7656334 2013 2013 4700 4700 12345678# 简单统计各项基本特征（类别特征）的数目（除去时间）print 'total bidder in bids: ', len(df_bids['bidder_id'].unique())print 'total auction in bids: ', len(df_bids['auction'].unique())print 'total merchandise in bids: ', len(df_bids['merchandise'].unique())print 'total device in bids: ', len(df_bids['device'].unique())print 'total country in bids: ', len(df_bids['country'].unique())print 'total ip in bids: ', len(df_bids['ip'].unique())print 'total url in bids: ', len(df_bids['url'].unique()) total bidder in bids: 6614 total auction in bids: 15051 total merchandise in bids: 10 total device in bids: 7351 total country in bids: 200 total ip in bids: 2303991 total url in bids: 1786351 由上述基本特征可以看到： 竞标行为中的用户总数少于训练集+测试集的用户数，也就是说并不是一一对应的，接下来验证下竞标行为数据中的用户是否完全来自训练集和测试集 商品类别和国家的种类相对其他特征较少，可以作为天然的类别特征提取出来进行处理，而其余的特征可能更多的进行计数统计 12345lst_all_users = (df_train['bidder_id'].unique()).tolist() + (df_test['bidder_id'].unique()).tolist()print 'total bidders of train and test set',len(lst_all_users)lst_bidder = (df_bids['bidder_id'].unique()).tolist()print 'total bidders in bids set',len(lst_bidder)print 'Is bidders in bids are all from train+test set? ',set(lst_bidder).issubset(set(lst_all_users)) total bidders of train and test set 6713 total bidders in bids set 6614 Is bidders in bids are all from train+test set? True 123456lst_nobids = [i for i in lst_all_users if i not in lst_bidder]print 'No. of bidders never bid: ',len(lst_nobids)lst_nobids_train = [i for i in lst_nobids if i in (df_train['bidder_id'].unique()).tolist()]lst_nobids_test = [i for i in lst_nobids if i in (df_test['bidder_id'].unique()).tolist()]print 'No. of bidders never bid in train set: ',len(lst_nobids_train)print 'No. of bidders never bid in test set: ',len(lst_nobids_test) No. of bidders never bid: 99 No. of bidders never bid in train set: 29 No. of bidders never bid in test set: 70 12345678data_source = ['train', 'test']y_pos = np.arange(len(data_source))num_never_bids = [len(lst_nobids_train), len(lst_nobids_test)]plt.bar(y_pos, num_never_bids, align='center', alpha=0.5)plt.xticks(y_pos, data_source)plt.ylabel('bidders no bids')plt.title('Source of no bids bidders')plt.show() 1print df_train[(df_train['bidder_id'].isin(lst_nobids_train)) &amp; (df_train['outcome']==1.0)] Empty DataFrame Columns: [bidder_id, payment_account, address, outcome] Index: [] 由上述计算可知存在99个竞标者无竞标记录，其中29位来自训练集，70位来自测试集，而且这29位来自训练集的竞标者未被标记为机器人用户，所以可以针对测试集中的这70位用户后续标记为人类或者取平均值处理 12# check the partition of bots in trainprint (df_train[df_train['outcome'] == 1].shape[0]*1.0) / df_train.shape[0] * 100,'%' 5.11674118231 % 训练集中的标记为机器人的用户占所有用户数目约5% 12df_train.groupby('outcome').size().plot(labels=['Human', 'Robot'], kind='pie', autopct='%.2f', figsize=(4, 4), title='Distribution of Human vs. Robots', legend=True) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f477135c5d0&gt; 由上述训练集中的正负例分布可以看到本数据集正负例比例失衡，所以后续考虑使用AUC（不受正负例比例影响）作为评价指标，此外尽量采用Gradient Boosting族模型来进行训练 数据预处理与特征工程12345import numpy as npimport pandas as pdimport pickle%matplotlib inlinefrom IPython.display import display 123bids = pd.read_csv('bids.csv')train = pd.read_csv('train.csv')test = pd.read_csv('test.csv') 处理缺失数据针对前面数据探索部分所发现的竞标行为数据中存在的国家属性缺失问题，考虑使用针对原始行为数据按照用户分组后，按时间顺序对每组用户中的缺失值前向或后向填充相邻的国家信息的方法来进行缺失值的填充处理 1display(bids.head(3)) bid_id bidder_id auction merchandise device time country ip url 0 0 8dac2b259fd1c6d1120e519fb1ac14fbqvax8 ewmzr jewelry phone0 9759243157894736 us 69.166.231.58 vasstdc27m7nks3 1 1 668d393e858e8126275433046bbd35c6tywop aeqok furniture phone1 9759243157894736 in 50.201.125.84 jmqlhflrzwuay9c 2 2 aa5f360084278b35d746fa6af3a7a1a5ra3xe wa00e home goods phone2 9759243157894736 py 112.54.208.157 vasstdc27m7nks3 12# pd.algos.is_monotonic_int64(bids.time.values, True)[0]print 'Is the time monotonically non-decreasing? ', pd.Index(bids['time']).is_monotonic Is the time monotonically non-decreasing? False 123# bidder_group = bids.sort_values(['bidder_id', 'time']).groupby('bidder_id')bids['country'] = bids.sort_values(['bidder_id', 'time']).groupby('bidder_id')['country'].ffill()bids['country'] = bids.sort_values(['bidder_id', 'time']).groupby('bidder_id')['country'].bfill() 1display(bids.head(3)) bid_id bidder_id auction merchandise device time country ip url 0 0 8dac2b259fd1c6d1120e519fb1ac14fbqvax8 ewmzr jewelry phone0 9759243157894736 us 69.166.231.58 vasstdc27m7nks3 1 1 668d393e858e8126275433046bbd35c6tywop aeqok furniture phone1 9759243157894736 in 50.201.125.84 jmqlhflrzwuay9c 2 2 aa5f360084278b35d746fa6af3a7a1a5ra3xe wa00e home goods phone2 9759243157894736 py 112.54.208.157 vasstdc27m7nks3 12print 'Is there any missing value in bids?',bids.isnull().any().any()# pickle.dump(bids, open('bids.pkl', 'w')) Is there any missing value in bids? True 1234missing_country = bids['country'].isnull().sum().sum()print 'No. of missing country: ', missing_countrynormal_country = bids['country'].notnull().sum().sum()print 'No. of normal country: ', normal_country No. of missing country: 5 No. of normal country: 7656329 12nan_rows = bids[bids.isnull().T.any().T]print nan_rows bid_id bidder_id auction \\ 1351177 1351177 f3ab8c9ecc0d021ebc81e89f20c8267bn812w jefix 2754184 2754184 88ef9cfdbec4c9e33f6c2e0b512e7a01dp2p2 cc5fs 2836631 2836631 29b8af2fea3881ef61911612372dac41vczqv jqx39 3125892 3125892 df20f216cbb0b0df5a7b2e94b16a7853iyw9g jqx39 5153748 5153748 5e05ec450e2dd64d7996a08bbbca4f126nzzk jqx39 merchandise device time country \\ 1351177 office equipment phone84 9767200789473684 NaN 2754184 mobile phone150 9633363947368421 NaN 2836631 jewelry phone72 9634034894736842 NaN 3125892 books and music phone106 9635755105263157 NaN 5153748 mobile phone267 9645270210526315 NaN ip url 1351177 80.211.119.111 g9pgdfci3yseml5 2754184 20.67.240.88 ctivbfq55rktail 2836631 149.210.107.205 vasstdc27m7nks3 3125892 26.23.62.59 ac9xlqtfg0cx5c5 5153748 145.7.194.40 0em0vg1f0zuxonw 1234# print bids[bids['bid_id']==1351177]nan_bidder = nan_rows['bidder_id'].values.tolist()# print nan_bidderprint bids[bids['bidder_id'].isin(nan_bidder)] bid_id bidder_id auction \\ 1351177 1351177 f3ab8c9ecc0d021ebc81e89f20c8267bn812w jefix 2754184 2754184 88ef9cfdbec4c9e33f6c2e0b512e7a01dp2p2 cc5fs 2836631 2836631 29b8af2fea3881ef61911612372dac41vczqv jqx39 3125892 3125892 df20f216cbb0b0df5a7b2e94b16a7853iyw9g jqx39 5153748 5153748 5e05ec450e2dd64d7996a08bbbca4f126nzzk jqx39 merchandise device time country \\ 1351177 office equipment phone84 9767200789473684 NaN 2754184 mobile phone150 9633363947368421 NaN 2836631 jewelry phone72 9634034894736842 NaN 3125892 books and music phone106 9635755105263157 NaN 5153748 mobile phone267 9645270210526315 NaN ip url 1351177 80.211.119.111 g9pgdfci3yseml5 2754184 20.67.240.88 ctivbfq55rktail 2836631 149.210.107.205 vasstdc27m7nks3 3125892 26.23.62.59 ac9xlqtfg0cx5c5 5153748 145.7.194.40 0em0vg1f0zuxonw 在对整体数据的部分用户缺失国家的按照各个用户分组后在时间上前向和后向填充后，仍然存在5个用户缺失了国家信息，结果发现这5个用户是仅有一次竞标行为，下面看看这5个用户还有什么特征 1234lst_nan_train = [i for i in nan_bidder if i in (train['bidder_id'].unique()).tolist()]lst_nan_test = [i for i in nan_bidder if i in (test['bidder_id'].unique()).tolist()]print 'No. of bidders 1 bid in train set: ',len(lst_nan_train)print 'No. of bidders 1 bid in test set: ',len(lst_nan_test) No. of bidders 1 bid in train set: 1 No. of bidders 1 bid in test set: 4 1print train[train['bidder_id']==lst_nan_train[0]]['outcome'] 546 0.0 Name: outcome, dtype: float64 由于这5个用户仅有一次竞标行为，而且其中1个用户来自训练集，4个来自测试集，由训练集用户的标记为人类，加上行为数太少，所以考虑对这5个用户的竞标行为数据予以舍弃，特别对测试集的4个用户后续操作类似之前对无竞标行为的用户，预测值填充最终模型的平均预测值 123bid_to_drop = nan_rows.index.values.tolist()# print bid_to_dropbids.drop(bids.index[bid_to_drop], inplace=True) 12print 'Is there any missing value in bids?',bids.isnull().any().any()pickle.dump(bids, open('bids.pkl', 'w')) Is there any missing value in bids? False 统计基本的计数特征根据前面的数据探索，由于数据集大部分由类别数据或者离散型数据构成，所以首先针对竞标行为数据按照竞标者分组统计其各项属性的数目，比如使用设备种类，参与竞标涉及国家，ip种类等等 123# group by bidder to do some statisticsbidders = bids.groupby('bidder_id')# pickle.dump(bids, open('bidders.pkl', 'w')) 1234567891011121314# print bidders['device'].count()def feature_count(group): dct_cnt = &#123;&#125; dct_cnt['devices_c'] = group['device'].unique().shape[0] dct_cnt['countries_c'] = group['country'].unique().shape[0] dct_cnt['ip_c'] = group['ip'].unique().shape[0] dct_cnt['url_c'] = group['url'].unique().shape[0] dct_cnt['auction_c'] = group['auction'].unique().shape[0] dct_cnt['auc_mean'] = np.mean(group['auction'].value_counts()) # bids_c/auction_c# dct_cnt['dev_mean'] = np.mean(group['device'].value_counts()) # bids_c/devices_c dct_cnt['merch_c'] = group['merchandise'].unique().shape[0] dct_cnt['bids_c'] = group.shape[0] dct_cnt = pd.Series(dct_cnt) return dct_cnt 1cnt_bidder = bidders.apply(feature_count) 123display(cnt_bidder.describe())# cnt_bidder.to_csv('cnt_bidder.csv')# print cnt_bidder[cnt_bidder['merch_c']==2] - auc_mean auction_c bids_c countries_c devices_c ip_c merch_c url_c count 6609.000000 6609.000000 6609.000000 6609.000000 6609.000000 6609.000000 6609.000000 6609.000000 mean 6.593493 57.850810 1158.470117 12.733848 73.492359 544.507187 1.000151 290.964140 std 30.009242 131.814053 9596.595169 22.556570 172.171106 3370.730666 0.012301 2225.912425 min 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 25% 1.000000 2.000000 3.000000 1.000000 2.000000 2.000000 1.000000 1.000000 50% 1.677419 10.000000 18.000000 3.000000 8.000000 12.000000 1.000000 5.000000 75% 4.142857 47.000000 187.000000 12.000000 57.000000 111.000000 1.000000 36.000000 max 1327.366667 1726.000000 515033.000000 178.000000 2618.000000 111918.000000 2.000000 81376.000000 特征相关性在对竞标行为数据按照用户分组后，对数据集中的每一个产品特征构建一个散布矩阵（scatter matrix），来看看各特征之间的相关性 12# 对于数据中的每一对特征构造一个散布矩阵pd.scatter_matrix(cnt_bidder, alpha = 0.3, figsize = (16,10), diagonal = 'kde'); 在针对竞标行为数据按照竞标用户进行分组基本统计后由上表可以看出，此时并未考虑时间戳的情形下，有以下基本结论： 由各项统计的最大值与中位值，75%值的比较可以看到除了商品类别一项，其他的几项多少都存在一些异常数值，或许可以作为异常行为进行观察 各特征的倾斜度很大，考虑对特征进行取对数的操作，并再次输出散布矩阵看看相关性。 商品类别计数这一特征的方差很小，而且从中位数乃至75%的统计来看，大多数用户仅对同一类别商品进行拍卖，而且因为前面数据探索部分发现商品类别本身适合作为类别数据，所以考虑分多个类别进行单独统计，而在计数特征中舍弃该特征。 1cnt_bidder.drop('merch_c', axis=1, inplace=True) 1cnt_bidder = np.log(cnt_bidder) 1pd.scatter_matrix(cnt_bidder, alpha = 0.3, figsize = (16,10), diagonal = 'kde'); 由上面的散布矩阵可以看到，个行为特征之间并没有表现出很强的相关性，虽然其中的ip计数和竞标计数，设备计数在进行对数操作处理之后表现出轻微的正相关性，但是由于是在做了对数操作之后才体现，而且从图中可以看到并非很强的相关性，所以保留这三个特征。 针对前述的异常行为，先从原train数据集中的机器人、人类中分别挑选几个样本进行追踪观察他们在按照bidders分组后的统计结果，对比看看 1cnt_bidder.to_csv('cnt_bidder.csv') 12345678# trace samples,first 2 bots, last 2 humenindices = ['9434778d2268f1fa2a8ede48c0cd05c097zey','aabc211b4cf4d29e4ac7e7e361371622pockb', 'd878560888b11447e73324a6e263fbd5iydo1','91a3c57b13234af24875c56fb7e2b2f4rb56a']# build a DataFrame for the choosed indicessamples = pd.DataFrame(cnt_bidder.loc[indices], columns = cnt_bidder.keys()).reset_index(drop = True)print \"Chosen samples of training dataset:(first 2 bots, last 2 humen)\"display(samples) Chosen samples of training dataset:(first 2 bots, last 2 humen) auc_mean auction_c bids_c countries_c devices_c ip_c url_c 0 1 2 3 3.190981 5.594711 8.785692 4.174387 6.011267 8.147578 7.557995 2.780432 4.844187 7.624619 2.639057 3.178054 5.880533 1.609438 0.287682 1.098612 1.386294 1.098612 1.386294 1.386294 0.000000 0.287682 2.890372 3.178054 1.791759 2.639057 2.995732 0.000000 使用seaborn来对上面四个例子的热力图进行可视化，看看percentile的情况 123456789101112import matplotlib.pyplot as pltimport seaborn as sns# look at percentile rankspcts = 100. * cnt_bidder.rank(axis=0, pct=True).loc[indices].round(decimals=3)print pcts# visualize percentiles with heatmapsns.heatmap(pcts, yticklabels=['robot 1', 'robot 2', 'human 1', 'human 2'], annot=True, linewidth=.1, vmax=99, fmt='.1f', cmap='YlGnBu')plt.title('Percentile ranks of\\nsamples\\' feature statistics')plt.xticks(rotation=45, ha='center'); auc_mean auction_c bids_c \\ bidder_id 9434778d2268f1fa2a8ede48c0cd05c097zey 94.9 94.6 97.0 aabc211b4cf4d29e4ac7e7e361371622pockb 92.4 87.2 92.3 d878560888b11447e73324a6e263fbd5iydo1 39.8 30.4 30.2 91a3c57b13234af24875c56fb7e2b2f4rb56a 39.8 60.2 53.0 countries_c devices_c ip_c url_c bidder_id 9434778d2268f1fa2a8ede48c0cd05c097zey 95.4 95.6 96.7 97.4 aabc211b4cf4d29e4ac7e7e361371622pockb 77.3 63.8 84.8 50.3 d878560888b11447e73324a6e263fbd5iydo1 48.8 38.7 34.2 13.4 91a3c57b13234af24875c56fb7e2b2f4rb56a 63.7 56.8 56.2 13.4 由上面的热力图对比可以看到，机器人的各项统计指标除去商品类别上的统计以外，均比人类用户要高，所以考虑据此设计基于基本统计指标规则的基准模型，其中最显著的特征差异应该是在auc_mean一项即用户在各个拍卖场的平均竞标次数，不妨先按照异常值处理的方法来找出上述基础统计中的异常情况 设计朴素分类器由于最终目的是从竞标者中寻找到机器人用户，而根据常识，机器人用户的各项竞标行为的操作应该比人类要频繁许多，所以可以从异常值检验的角度来设计朴素分类器，根据之前针对不同用户统计的基本特征计数情况，可以先针对每一个特征找出其中的疑似异常用户列表，最后整合各个特征生成的用户列表，认为超过多个特征异常的用户为机器人用户。 123456789101112# find the outliers for each featurelst_outlier = []for feature in cnt_bidder.keys(): # percentile 25th Q1 = np.percentile(cnt_bidder[feature], 25) # percentile 75th Q3 = np.percentile(cnt_bidder[feature], 75) step = 1.5 * (Q3 - Q1) # show outliers # print \"Data points considered outliers for the feature '&#123;&#125;':\".format(feature) display(cnt_bidder[~((cnt_bidder[feature] &gt;= Q1 - step) &amp; (cnt_bidder[feature] &lt;= Q3 + step))]) lst_outlier += cnt_bidder[~((cnt_bidder[feature] &gt;= Q1 - step) &amp; (cnt_bidder[feature] &lt;= Q3 + step))].index.values.tolist() 再找到各种特征的所有可能作为‘异常值’的用户id之后，可以对其做一个基本统计，进一步找出其中超过某几个特征值均异常的用户，经过测试，考虑到原始train集合里bots用户不到5%，所以最终确定以不低于1个特征值均异常的用户作为异常用户的一个假设，由此与train集合里的用户进行交叉，可以得到一个用户子集，可以作为朴素分类器的一个操作方法。 12345# print len(lst_outlier)from collections import Counterfreq_outlier = dict(Counter(lst_outlier))perhaps_outlier = [i for i in freq_outlier if freq_outlier[i] &gt;= 1]print len(perhaps_outlier) 214 123# basic_pred = test[test['bidder_id'].isin(perhaps_outlier)]['bidder_id'].tolist()train_pred = train[train['bidder_id'].isin(perhaps_outlier)]['bidder_id'].tolist()print len(train_pred) 76 设计评价指标根据前面数据探索知本实验中的数据集的正负例比例约为19:1，有些失衡，所以考虑使用auc这种不受正负例比例影响的评价指标作为衡量标准，现针对所涉及的朴素分类器在原始训练集上的表现得到一个基准得分 1234567from sklearn.metrics import roc_auc_scorey_true = train['outcome']naive_pred = pd.DataFrame(columns=['bidder_id', 'prediction'])naive_pred['bidder_id'] = train['bidder_id']naive_pred['prediction'] = np.where(naive_pred['bidder_id'].isin(train_pred), 1.0, 0.0)basic_pred = naive_pred['prediction']print roc_auc_score(y_true, basic_pred) 0.54661464952 在经过上述对基本计数特征的统计之后，目前尚未针对非类别特征：时间戳进行处理，而在之前的数据探索过程中，针对商品类别和国家这两个类别属性，可以将原始的单一特征转换为多个特征分别统计，此外，在上述分析过程中，我们发现针对用户分组可以进一步对于拍卖场进行分组统计。 对时间戳进行处理 针对商品类别、国家转换为多个类别分别进行统计 按照用户-拍卖场进行分组进一步统计 对时间戳进行处理主要是分析各个竞标行为的时间间隔，即统计竞标行为表中在同一拍卖场的各个用户之间的竞标行为间隔 然后针对每个用户对其他用户的时间间隔计算 时间间隔均值 时间间隔最大值 时间间隔最小值 1234567891011121314151617181920212223from collections import defaultdictdef generate_timediff(): bids_grouped = bids.groupby('auction') bds = defaultdict(list) last_row = None for bids_auc in bids_grouped: for i, row in bids_auc[1].iterrows(): if last_row is None: last_row = row continue time_difference = row['time'] - last_row['time'] bds[row['bidder_id']].append(time_difference) last_row = row df = [] for key in bds.keys(): df.append(&#123;'bidder_id': key, 'mean': np.mean(bds[key]), 'min': np.min(bds[key]), 'max': np.max(bds[key])&#125;) pd.DataFrame(df).to_csv('tdiff.csv', index=False) 1generate_timediff() 后续内容请移步使用机器学习识别出拍卖场中作弊的机器人用户(二)","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"平坦世界","date":"2017-05-29T08:30:23.000Z","path":"2017/05/29/世界是平的/","text":"这次给大家推荐一本书——“The world is flat”（《世界是平的》），刚刚看完了一遍，感觉这本书的视角很广阔，让我看到自己受限于地理或者意识因素所看不到的一些其实就实实在在发生在身边的变化，私以为，作为即将毕业的大学生而言，是很有必要一读的。 【这本书面世其实快10年了，但是书中所述的一些变化我们或经历过，或忽略掉了，所以即使放在现在读，也是很有前瞻性的。特别由于作者本身是记者，写作风格还是颇为轻松地，所以，读起来感觉像是跟着作者开着上帝视角实现了所谓“世界那么大，我想去看看”的成就一般。】 此书整体大概是由世界变平坦的种种例证、现象到作者所认为使世界变平坦的一些因素再到各个“成员”大至国家公司、小至每一个个体在这平坦世界里的角色和相互影响，最后是平坦世界所带来的或即将造成的可能负面影响和威胁来逐层叙述的。我想先就宏观角度简单记叙下书中的一些观点，然后谈一下书中所说而我也感觉到的身边的一些变化，最后就自己而言随意谈一下想法。 就宏观而言，书中让我印象最深的是全球化的重中体现，首先是全球合作的一些案例，这个曾经在《互联网时代》里也看到过，比如现在的波音飞机的组装以及各零件的制造是分散到世界各个国家各个地区去做的，然后又井然有序的由各个地方汇聚到一个点组装成型，书中还举得一个例子是沃尔玛的生产线，和前者类似，也是一种像分布式的结构，这种做法极大地提高了效率并降低了成本，而其中最让我目眩神迷的是这么个巨大的整体居然能够这么一致的展开协同工作；另外一点是关于离岸外包，这里就不得不提到一个国家是印度，或者说印度的城市班加罗尔，看了这本书我感觉像亲身去看一下，所谓外包，就是将本国家的一些相对而言中低端而要耗费大量人力的基础工作交给其他国家去做，这里你也许会想到所谓的”Made in China”，诚然，中国也是外包的一个很好的例子，换做以前我会觉得嗤之以鼻，感觉我们国家就老是像给美国之类的国家打工一样，其实，分析一下，就目前而言，这个是双赢的，美国那边自不用说，花一分美国工人的钱在中国或者印度雇佣到6~8个同样水平的技术工人，极大地降低了成本，而且这里面不得不说有个很有意思的东西是——时差，因为有些工作在美国的白天又美国人做，晚上就交给印度人（印度正在白天）做，极大地提升了效率，这个我在电影《贫民窟的百万富翁》里看到过的一个场景就是印度人通过远程监控摄像头替美国人监控停车场让我印象深刻，那么对于印度和中国这些国家呢，好处在哪？其一就是增多了工作岗位，就印度而言的话，由于现在印度培养了大量的理工学生，而毕业后以前只能去国外谋出路，否则在国内一般只能成为出租车司机，而经过外包，他们可以在本土就进行软件开发的工作，虽然工资不如美国人，但是相对于自己本身而言，他们的生活已经得到不小的提升了，另外，通过这种形式，印度和中国还可以学习一些国外的技术，甚至是抢占国外的市场，书中后面有个例子是中国生产的埃及传统的灯几乎快要垄断埃及的市场，因为在技术上有所突破，而且价格也低廉。 当然，以上只是从书中看到的知识，也许我以后还需要实地考察一番看看实际情况是否真如作者所述，不过，对于我们国家，我还是希望抱有乐观的态度去看他的发展，相信他会越变越好。 就身边的所见所闻而言，我印象比较深的是一个 距离 的问题，和书中作者一次下飞机乘出租车的经历一般，在乘车那段时间里，司机一直在通过蓝牙耳机聊天，车上开着导航，播放着电影，而作者在自己的笔记本上整理文章以及收发e-mail，用作者的话说，在那一个小时里他们同时做了很多事，却几乎没有交流，作者甚至猜想，也许司机正和远在另一个国度的父母通话呢。这大概就是目前发生在我们身边的一个尴尬境地了，一方面，技术的发展拉近了我们的距离，而另一面却又使我们的距离变得遥远，它拉近了我们和处在不同空间的亲人之间的距离，却又在此时此地的就在你眼前的我面前树起了一道屏障。我想起了高中语文老师发的一篇文章里对动车、高铁上的年轻人乘客们的描述，确确实实的感受到这一真切的现象，大家一上车就戴上耳机，或插入ipad，或插入iphone，而彼此之间却没有交流，这和绿皮火车上的情景完全不同，也是值得我们反思的一件事。 最后我想简单谈一下自己的一些想法。首先关于教育和竞争方面，这里还得扯下印度，书中说道，班加罗尔的接线员晚上为美国（白天）的乘客们进行咨询和失物找回工作，晚上还会自己学习一些知识，攻读一些学位什么的，顿时感到一种压迫感。从前经常听到政治老师说美国家长告诫自己的孩子说快吃饭，不然中国和印度的孩子就把你的饭给吃了或者快努力学习，中国或印度的孩子快把你的饭碗抢走了之类的。而现在是大家都站在几乎同一个平台上竞争，前面我们需要追赶美国，而同时，印度的青年们却也在旁虎视眈眈。而身处计算机专业这一日新月异，竞争更加残酷的环境下，我一面感到威胁，一面感到兴奋，兴奋的是，试想一下你即将站在一个大舞台上，和来自不同城市乃至不同国家的人角逐，而威胁在于，你们本身的水平是不一样的，就我而言，在我将要踏上这个舞台的那一天，我不仅会和同班同学竞争，稍远一点还有来自全国各大高校的强者，再远一点还有世界其他角落的高手出没，而要想不被碾成炮灰，或者说不被这一变平的趋势所冲倒的话，我就得不断地去吸收和学习新的技术、新的知识，还要进一步强化自己的全局意识，做到让自己所做的事无可替代，也就不会被”外包”掉。另外一点就是之前实习时听到负责人说现在的联系方式太多了，又是电话、又是微信、QQ的，反而造成了联系上的障碍，以前我们只有电话时一般就通过电话联系，而现在常常是不同人有不同的习惯，你得把这些一股脑全开着，否则搞不好会错过重要讯息，我常常在想，世界是多元化的，但本意是方便我们交流的工具最后却慢慢成了束缚我们的枷锁，从这个角度来看，人类到底是进步了，还是退化了？","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"Economy","slug":"Economy","permalink":"http://yoursite.com/tags/Economy/"}]},{"title":"面壁者，我是你的破壁人","date":"2017-05-29T08:28:53.000Z","path":"2017/05/29/三体/","text":"对一个人的评判若不结合他所处的时代都是不公允的。 终于看完了《三体》全集，这部科幻小说给人的感觉真像作者自己命名的“地球往事”，像一本历史书，以宇宙为坐标，以光年为刻度，读罢，借用一句话“科幻小说虽然尽是对于未来的想象，但我们探索的一直是人的内心”。 情节上我就不剧透太多了，我想谈谈《三体》里的几个角色。 首先是罗辑，面壁者、执剑人、守墓人是他不同时期的身份，他是一个充满传奇色彩的人，用他自己的话是一个及时行乐的人，对其他的人、其他的事都不怎么上心的人。但是我看到的是他受人胁迫而终于接受面壁者身份时的无奈和挣扎；在冬眠复苏时被人当做笑话时的淡然、洒脱，再一次被推上历史转折点时的坚韧以及作为执剑人半个多世纪面壁时的狠厉。我得承认全书给我印象最深的是《黑暗森林》最高潮的部分，当罗辑站在自己挖掘的坟墓边对着虚空中的智子亮出最后底牌的那一刻；我真的有一种作为人类终于得救了、终于又有了希望的感觉。命运喜欢捉弄罗辑，在他吊儿郎当、无所事事时对他施以面壁者的诅咒，在他拯救了全人类时却让人类对他报以敌视；但罗辑不在乎这些，他只在乎自己所爱的那个女孩的幸福。 然后是《死神永生》里的女主角程心。之前有看一个《x分钟读完三体》的短片，当时发现弹幕上对于程心大多数是一片骂声，随处可见诸如“程圣母毁灭世界”的字眼。不得不说在看《死神永生》的中后段我也一直对于程心是恨得咬牙切齿的，实话说，我讨厌她的不作为和自以为是爱与善的化身所做的所有决定，讨厌她毁灭了地球、害人类灭亡，但后来一想，我其实是开了上帝视角在看这个角色，其实回过头来看，程心是当前普通人类的典型，甚至，是大众中向善的群体的典型，她一直秉持着自己的责任，逆来顺受；她是勇敢的，敢于牺牲自己，但是，她的能力是不够的。整部小说看下来程心大部分时间处于冬眠状态，而经常是其他人帮她做足了准备，然后突然一下把她推到全人类命运决策的位置，可是却没有想过她准备好了没有，结果她只能依据自己当前的认识做出最佳选择，然后成了公众的替罪羊，其实换位思考，我们估计在那个节点做的更差；不信你把程心放到不同的时间段看公众对于她作为执剑人的那几分钟的态度的不停反复就可以看出来了，大众只是需要一个为自己顶包的人，然后好像自己就可以置身事外了。 如果说程心更多的依靠感性来做判断，那么就不得不提到维德了，这个极端理性到冷酷的男人。最开始他对于程心的捉弄确实让我很不爽，从发射云天明的大脑一直到后面他不择手段的想要暗杀程心竞争执剑人。但读到后面你会发现维德其实是个表里如一的人，对于自己认定的事他能坚持到底，不择手段的坚持到底。 所以不存在这么一个人，把他放到任何一个时空他的判断都是对的，或者说大众对于个体的判断其实也不能代表什么，一个人只需坚持自己所相信的，在乎自己所爱的，到最后一刻，就够了。 各位面壁者们，准备好做自己的破壁人了么？","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"Novel","slug":"Novel","permalink":"http://yoursite.com/tags/Novel/"}]},{"title":"平凡的世界，不平凡的凡人","date":"2017-05-29T08:27:42.000Z","path":"2017/05/29/平凡的世界/","text":"一直想读这套书，在临近毕业时还专门从朋友那收藏了一套，借着在MEB的机会终于是忙里偷闲的看完了，虽意犹未尽，却也感到一种平和和满足。 如果要用一句话概括，《平凡的世界》写的是爱情和面包。 书中描述的爱情种类比较多，我就简单挑给我印象最深的三个来说说吧。首先是润叶和少安的爱情，那句诗里写的“郎骑竹马来，绕床弄青梅”大抵描述的就是这么一种爱情。少安和润叶是两小无猜的一对童年玩伴，随着两家各自发展出的家庭背景的差异却也慢慢在双方之间产生了巨大的鸿沟，一个留校任教成了城里人，另一个却回家务农，给家里分担压力。诚然，你可以说是少安主观上的懦弱导致了最终两人的分离和三个家庭的阴影和痛苦，但是个人认为，在那么一种背景下，就一个男人而言是不得不考虑这些东西的。书中虽说如果两人身份对调也许就没什么问题，但我觉得即使两人身份对调而彼此相爱也还是要考虑双方的家庭背景，这里我倒不是在推崇门当户对的老观念，只是客观的分析一下，毕竟，我一直认为，爱情是建立在一定的物质基础之上，不是简单的彼此相爱，毕竟，爱情可能产生在一瞬间，但是维系这份感情却需要长期的努力，也就需要一定的物质作为基础，而若双方各自条件不对等，时间长了或多或少会产生并积累一些问题。由此看来，古人所推崇的门当户对确有一定道理。 当然，我不否认灰姑娘和王子的童话故事，但是即使在灰姑娘的故事里，灰姑娘也是有仙子为她做后盾的而且她本身有相对应的素质作为支撑。 扯远了，其实书中倒也有这么一对，就是少平和晓霞。我对这一对的定义是充满理想和浪漫主义色彩的情侣，两人由最初的共同爱好或说理想发展出深厚的友谊，继而在不同的人生道路上产生的共鸣引导出爱情，说实话，看到这里我已经觉得有点过于理想化了，到后来晓霞拒绝高富帅依然坚定地爱着少平时，我也深深被打动，从心理上我是希望看到这一对能有美好的结局的，我原希望会是少平从煤炭场最后走出来完成逆袭然后迎娶白富美的励志故事（原谅程序员的屌丝气息），不过感觉依作者的套路势必会再给这对有情人制作一些波折，可是，万万没想到，情节发展居然是晓霞的牺牲，当时感觉很难接受这一事实，毕竟这朴实的书中的一朵充满浪漫主义色彩的绚丽花朵竟以这种方式枯萎。 最后一对就是润叶和向前，很难用爱情定义这一对，因为故事前段一直是向前的单恋，即使两人的结合也是润叶的自我牺牲和自我放逐，结果是长期的互相折磨到最后，然后转折点在向前的事故所带来的残疾，润叶因为内疚而突然回头，两人复合（个人觉得此处作者不够深入），然后两人开始真正的婚姻生活，不过也很难说是爱情，连文中也提及此时的润叶更像是一个虔诚的修女，给予向前的除了妻子的责任另外确是一种变相的母性关怀，姑且称之为——怜悯或者同情吧，其实我是很喜欢润叶这么个姑娘的，敢爱敢恨，在和少安的爱情中表现出勇敢，却又太懂事，这样矛盾的性格造成她最终的屈服，与其说她是嫁给了一个自己不爱的男人，倒不如说她是嫁给了生活。但我又不能怪罪向前，爱情这东西说不清道不明的，很多时候其实就是，我喜欢你然而我并不能理解你为何不喜欢我，其结果至少有一人受伤。 以上是对书中描述的几种爱情的粗浅理解，当然，作为一个纯理论家，请相信，我说的每一句话都是谎话。 还是来谈谈另一条线——面包吧。 全书主要描述了双水村这么个平凡的小天地里一个又一个像你像他像那路边野花的平凡人的平凡而又不平凡的生活。总体来说，大的背景是变化莫测的，从文革末的动乱年代到改革开放的小康生活，立足在这么一个大环境下，每个人的命运都不能孤立的来看，刚开始看这本书的时候我常常随着故事的情节发展给故事中的人下定义是好是坏是对是错，后来随着作者时不时的客观分析才发现自己的浅薄，其实就像我们所生活中的生活，它不像我们做的卷子上的题目，没有什么绝对的对错，每个人只是在给他演出的那么一段镜头里或优雅谢幕或落荒而逃，导演却是生活。也许我仍不能从简单的评判一个人是好人坏人，他做的一件事是对的还是错的这么一种观念里走出来，但是从这本书里我看到了生活这辆无缰马车上的形形色色人物的身不由己，同时，我也看到他们的奋斗，一次一次被生活喊cut却又一次一次带着微笑或者含着泪水伤痕累累的跑龙套，在这场戏里，没有主角，或者说每个人都是主角，轮番上场。 每一个人物都有其鲜明特色又有其局限性，之前有一朋友说他觉得书中把任务刻画得太脸谱化了，我觉得是有这么一点，但是大体而言路遥对人物的刻画还是很鲜明，很丰满而有血有肉的。我欣赏少安在物质生活追求上的不屈不挠精神，却又为他有时的偏执扼腕，对于他在爱情上的胆怯以及一意孤行而叹息；我喜欢少平这么一个脚踏实地的为生活战斗，为自己为他人着想的‘精神贵族’。他太可爱了，看这本书真的就是看着这个少年的成长，看着他咀嚼生活，消化生活，不过我希望他能更勇敢，有时能多为自己考虑一点，勇敢的追逐自己的幸福；至于晓霞，自不用说，感觉书中几乎刻画成女神一般的存在了，可惜也由于她的冒险精神和舍己为人的精神而香消玉殒；润叶，真是一个让人忍不住心疼却又忍不住尊敬的女性，她是那么为大家着想，甚至于可以牺牲自己；还有孙玉厚，这个伟大的老父亲，虽然书中没太多专门的笔墨，但是仍可以看见少安、少平甚至兰花从他身上传承的坚韧的基因，也许从早期发狠供玉亭念书也能窥见一二，另外，这位老人在儿子追求自己目标时默默支持，在孩子们遭受不幸时却总是挺身而出，实在令人感动；其他还有用情专一的向前，逛鬼王满银，懂事的兰香等等，像你我身边的每一个人一样在这个平凡的世界里献上了这精彩的演出，然后帷幕缓缓落下，他们又消失在人群中，从此你我看到的身边的每一个人都似曾相识。 以上大概就是这粗略的第一次阅读《平凡的世界》的一点不吐不快的想法，虽然书有一点点小瑕疵比如各个主角的结局实在难以让我这么一个习惯了happy ending的人满意，比如中间外星人的情节有点乱入的感觉，不过还是强力推荐这本书，怎么说呢，在看这本书的过程中，我常常有一种踏实感，活着的踏实感，在我合上书页，回想起书中一些情节时情不自禁的感受到生活，从某种程度上，它减轻了我若有若无的虚无感。","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"Novel","slug":"Novel","permalink":"http://yoursite.com/tags/Novel/"}]},{"title":"韶华倾负，难舍皮囊","date":"2017-05-29T08:22:21.000Z","path":"2017/05/29/韶华倾负，难舍皮囊/","text":"利用在动车上的四个小时看完了《皮囊》（除去中间15分钟和邻座美女搭讪的时间），感觉是很不错的一本书，不同的章节写的不同的故事，如果你有时间不妨通篇阅读一下，如果没有时间不妨听我说一说。 个人觉得，前五章的主题关于生离，关于死别。 第一章和书同名是为《皮囊》，讲述的是99岁的阿太。 我们的生命本来多轻盈，都是被这肉体和各种欲望的污浊给拖住。阿太，我记住了。“肉体是拿来用的，不是拿来伺候的。” 没文化的神婆阿太穷极一生都在利用自己的那副皮囊，甚至要求周围的人也学会去利用好这幅皮囊，所以她把年幼的孩子扔进海里让他学游泳，所以她即使白发人送黑发人也保持一副让人不解的嘲弄表情，或许在她看来，失掉这皮囊，灵魂最终的归宿反而是自由吧。 但这并不意味着阿太是那种不在乎生死的人，相反，我觉得她是站在更高的角度看待生死这个问题的人，而这也许隐隐约约影响着身边的人。所以作者后来写下这么一句话： 从小我就喜欢闻泥土的味道，也因此其实从小我不怕死，一直觉得死是回家，是入土。我反而觉得生才是问题，人学会站立，是任性地想脱离这土地，因此不断向上攀爬，不断抓取任何理由——欲望、理想、追求。然而，我们终究需要脚踏着黄土。在我看来，生是更激烈的索取，或许太激烈的生活本身就是一种任性。 然后是第二章——《母亲的房子》写的是母亲，房子还有爱情。 每个人都会有自己的执着，而那种执着最后就物化成某种具体的事物，然后进一步，有的化成妥协后的一道疤痕，有的成为穷极一生的执念。而文中的母亲显然是后者，而她所执着的是那所房子，是年轻时的父亲曾允诺的房子，以一种执着的近乎任性的方式。从最开始的一修再修，这所母亲执意要修建的房子联系着一家人的生活和命运，从拮据的生活到后来周围人的不解最后甚至家人的埋怨，母亲一度支撑不下去却又不甘心，最难过的日子里甚至一家人想要一起求死，直到后来身为“不合格的一家之主”的我终于理解了母亲： 事实上，知道母亲坚持要建好这房子的那一刻。我才明白过来，前两次建房，为的不是她或者我的脸面，而是父亲的脸面——她想让父亲发起的这个家庭看上去是那么健全和完整。 这是母亲从老没表达过，也不可能说出口的爱情。 另外，关于母亲的故事在《我的神明朋友》里会继续讲述，你会看到更多母亲身上的坚韧。 然后《残疾》讲述的是父亲中风后的家里的种种艰辛和一家人和生活的抗争。 不同于以前语文课本里父亲那种高大、沉默的形象，《皮囊》里的父亲首先是一个中风而偏瘫了的父亲，而后是一个曾经混黑社会呼风唤雨的混混头子，中风前后的心理落差造成了父亲接下来的一系列变化，他从假装坚强，希望靠对时间的严苛要求和每天的活动恢复到以前的健壮身体，于是一家人默契的相互演着戏，却也过了一段幸福的时间，直到被一场意料之中的台风摧毁。 虽然知道根本不是台风的错。那结局是注定的，生活中很多事情，该来的会来，不以这个形式，就会以那样的形式。但把事情简单归咎于我们无能为力的某个点，会让我们的内心可以稍微自我安慰一下，所以，我至今仍愿意诅咒那次台风。 那场台风刮倒的不仅仅是父亲，更打碎了父亲伪装的坚强和一家人匆忙写就的生活的剧本。自此，父亲进入了第二个状态，先是放弃了坚强，呈现出一副自暴自弃的样子期盼着死亡。而后又进入下一阶段，甚至于舍弃了父亲的身份，像个不懂事的孩子一样撒娇、任性，却不再期盼死亡。 但是父亲的故事并没有完，在《重症病房里的圣诞节》，作者以在重症病房里的看护家属的视角半写实半轻描淡写地描述了退去浮华身份后的种种生离死别。 疾病在不同的地方找到了他们，即使他们当时身处不同的生活，但疾病一眼看出他们共同的地方，统一把他们赶到这么一个地方圈养。 在这一章里，作者刻画了几个不同的人物，有乐观的病人，有刻意显得刻薄的医护人员，有早熟的同龄孩子。在医院这么一个特殊的小世界里，赤裸的生长着。有几个事件的刻画让我印象深刻，比如‘我’对于医院里电梯的描写，对于走过一间间病房核查之前的病人是否还在；比如和同龄的孩子交流，发现相同点，大家拥有相同的眼睛，那是经历过生命消逝后才看得到的眼睛，所以会被一眼看透，而无法交到同龄的朋友，因为他们只有一次也只能有一次痛彻心扉的谈话；比如乐观的病友最后还是被夺走生命后‘我’心态上的变化，印象最深的是那个为父亲在圣诞节违禁燃放烟花的年轻人，只因他父亲要进行手术了，然后他被三个保安团团围住…… 比如，在帮父亲换输液瓶时，会发觉他手上密密麻麻的针孔，找不到哪一寸可以用来插针；比如医生会时常拿着两种药让我选择，这个是进口的贵点的，这个是国产的便宜的，你要哪种？我问了问进口的价钱，想了很久。“国产的会有副作用吗？”“会，吃完后会有疼痛，进口的就不会。”我算了算剩下的钱和可能要住院的时间，“还是国产的吧。” 然后看着父亲疼痛了一个晚上，怎么都睡不着。 在这里，你一不小心留出空当，就会被悲伤占领——这是疾病最廉价、最恼人的雇佣兵。 这种笔触没在重症病房待过的人是写不出来的，那种切实的压抑感和空闲下来被对未来的恐惧和迷茫逼到绝路的走投无路感。 接下来的几章个人觉得是关于青春和梦想的。 首先《张美丽》这一章从这么一个略显俗气的名字开始，从迷信的桃色传说开始记叙了传说中自杀的殉情张美丽，而后记叙了现实中被夹在理想和世俗之间压垮的张美丽，同样是一副让荷尔蒙萌动的青春期少年燥起来的皮囊，演绎了不同的故事。这故事一面让我想起了《搜索》这部电影里舆论的可怕，人们对于未知的事物的不理解演化成的嫉妒和驱逐和以讹传讹造成的悲剧上演。另一面看到的是被遗忘的先驱们，为了自己的理想而头破血流，然后若干年后在残次不齐的字里行间被遗忘或变成谈资。只是人们一同忘掉的，是现在所呈现在眼前的似曾相识。 接下来《阿小和阿小》、《天才文展》记叙的算是童年和成长吧。 关于城市，那是不在城市长大的孩子们小时候的天堂，是他们在电视屏幕上看到的模样，而两个阿小，一个土生土长在小城镇；另一个为即将开始的大城市生活过度而寄居在小城镇。同样的名字，也沉浸在同样的幻想里，只是他们误读了城市，他们以为的城市就真的是电视屏幕上的那样，数不过来的高楼，四六分的香港发饰，衬衫，洁白的牙齿……于是，他们开始注重外表上的模仿和行为上的接近，在我看来，这正是一种青春期的迷茫，对于寻找自我定位时的种种探索，追求那些很酷很与众不同的感觉。但是结局却不尽如意： 大部分人都困倦到睡着了——他们都是一早七点准时在家门口等着这车到市区，他们出发前各自化妆、精心穿着，等着到这城市的各个角落，扮演起维修工、洗碗工、电器行销售、美发店小弟……时间一到，又仓皇地一路小跑赶这趟车，搭一两个小时回所谓的家，准备第二天的演出。 他们都是这城市的组成部分。而这城市，曾经是我们在小镇以为的，最美的天堂。他们是我们曾经认为的，活在天堂里的人。 而《天才文展》里的文展则代表着一种有远大抱负却囿于家庭环境的早熟的年轻人，也许我们都碰到过像这样的人，以一种居高临下的姿态看着你，对于自我有着严苛的要求，希望通过一项项计划通来向别人证明自己，来狠狠地扇曾经看不起自己的人一耳光。 我才明白，那封信里，我向文展说的“小时候的玩伴真该一起聚聚了”，真是个天真的提议。每个人都已经过上不同的生活，不同的生活让许多人在这个时空里没法相处在共同的状态中，除非等彼此都老了，年迈再次抹去其他，构成我们每个人最重要的标志，或许那时候的聚会才能成真。 然而结局却是悲剧的，‘我’最终过上了文展所希冀的生活而遭到文展的嫉妒和排斥，令人吃惊的是‘我’却也产生了一些负罪感。就像电视剧里经常碰到的桥段，“如果不是因为当初XXX，你现在所有的一切本该是我的。”其实这何尝不是自我欺骗呢，就算自我假设的前提条件不成立，结局却不见得会不一样，失败者把失败原因归结到自己以外的事物上注定还是会重蹈覆辙。 然后是《厚朴》描述的这么一个可爱的男生，英文名又是hope，伴着激情和叛逆的形象出场，不得不说让我想起身边一些充满朝气和正能量的人，平时还是很羡慕他们的，不过厚朴是不同的，开始我也很希望看看他所谓的“务虚”能走出一片天地来，俗话说就是理想主义者，因为我感觉自己更类似于作者。不过可惜的是最后结局让人痛心，像作者所形容的 不清楚真实的标准时，越用力就越让人觉得可笑。 不合时宜的东西，如果自己虚弱，终究会成为人们嘲笑的对象，但有力量了，或坚持久了，或许反而能成为众人追捧的魅力和个性——让我修正自己想法，产生这个判断的，是厚朴。 他以为自己做着摧毁一切规矩的事情，但其实一直活在规矩里。我以为自己战战兢兢地以活在规矩里为生活方式，但其实却对规矩有着将其彻底摧毁的欲望。 所以厚朴所表现出来的其实是一种沉浸在自己构造的幻想世界里而最终难以回到现实，他不愿接受自己失败了的现实，只是表现的叛逆。但话说回来我是希望能看到一种理想主义者最终实现理想的故事，我相信如果有那么一定会是精彩的。 说到这里我甚至有一种奇怪的感觉，是否厚朴其实就是作者的另一面，只是最终发现自己其实是在规则里画着圈而随着屈服而死掉的那部分。 最后几章主要简单谈了些关于城市、理想之类的话题，印象较之之前的几章倒是没那么深了，截取一段如下 只是我觉得城市不好。特别是中国的城市不好。厦门和中国大部分城市的建设都有个基础——人家国外的城市是怎么样的，以及人们该怎么被组织的，然后再依据这样的标准建设。中国近代的城市不是长出来的，不是培植出来的，不是催生出来的，而是一种安排。因为初期必然要混乱，所以中国的城市也表现出强大的秩序意识，人要干吗，路要怎么样。生长在这样环境里的人，除了维护秩序或者反抗秩序，似乎也难接受第二层次的思维了。 回头来看，几篇文章倒确实很契合《皮囊》这么一个主题，就像开头所述的，灵魂离不开皮囊，无论你如何讨厌自己的这幅皮囊，你的灵魂也得附庸其下。皮囊往小的说就是阿太所谓的驱壳和父亲半瘫的残疾，往大了说是一个人所处的位置，他背后的故事，就像母亲的那所房子，阿小的香港梦，厚朴的架子鼓。过去我常常看不懂电视剧里有些人的无可奈何，一厢情愿的认为这么容易的事换做是我直接三下五除二就解决了，实际情况却是不要以好坏善恶对错来划分人群，你看到的只是他当前展现给你的，你看不到的是他这一举动后面的挣扎，人是不能孤立和剥离来看的，当他站在你面前时，你得看到他的背景。 我常对朋友说，理解是对他人最大的善举。当你坐在一个人面前，听他开口说话，看得到各种复杂、精密的境况和命运，如何最终雕刻出这样的性格、思想、做法、长相，这才是理解。而有了这样的眼睛，你才算真正“看见”那个人，也才会发觉，这世界最美的风景，是一个个活出各自模样和体系的人。 如果每个人都是一段程序的话，那么我们在和不同的人交往的过程中就有意无意的修改了他们的代码，从此也许他们身上就带着你的痕迹；而身边亲近的人，更像是写进你基因里的代码段，在某个特定的时间里被激活，让你似曾相识，让你视线模糊。 无论如何，请带着这副皮囊好好生活。","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"Novel","slug":"Novel","permalink":"http://yoursite.com/tags/Novel/"}]}]