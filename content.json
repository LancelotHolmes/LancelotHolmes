[{"title":"口述模型整理","date":"2017-09-03T06:54:04.000Z","path":"2017/09/03/model-pre/","text":"尝试整理描述一些模型 算法要从以下几个方面来掌握 产生背景 适用场合（数据规模，特征维度，是否有 Online 算法，离散/连续特征处理等角度）； 原理推导（最大间隔，软间隔，对偶）； 求解方法（随机梯度下降、拟牛顿法等优化算法）； 优缺点，相关改进； 和其他基本方法的对比； 不能停留在能看懂的程度，还要对知识进行结构化整理，比如撰写自己的 cheet sheet，面试是在有限时间内向面试官输出自己知识的过程，如果仅仅是在面试现场才开始调动知识、组织表达，总还是不如系统的梳理准备；从面试官的角度多问自己一些问题，通过查找资料总结出全面的解答，比如如何预防或克服过拟合。 Logistic Regression原理 用线性回归模型的预测结果去逼近真实标记的对数几率 对于二分类任务：其输出标记$y \\in \\lbrace 0,1 \\rbrace$,而线性回归模型产生的预测值$f(x)=w\\cdot x+b$是实值 而根据广义线性模型的思想，可以通过寻找一个联系函数来将分类任务的真实标记与线性回归模型的预测值联系起来 最直接的想法是使用单位阶跃函数，即![step curve][3] 但是单位阶跃函数不连续，所以我们希望找到一个替代函数来在一定程度上近似单位阶跃函数，并希望该函数单调可微，于是就引入了对数几率函数$$y=\\frac{1}{1+e^{-z}}$$ 将对数几率函数作为$g^-(\\cdot)$带入广义线性模型则有$$y=\\frac{1}{1+e^{-(w \\cdot x+b)}}$$ 其变形形式为$$ln\\frac{y}{1-y}=w\\cdot x+b$$,若将$y$视作样本$x$作为正例的可能性，则$1-y$是其作为反例的可能性，两者的比值$\\frac{y}{1-y}$称为几率，反映了$x$作为正例的相对可能性 关于sigmoid的由来也可已通过推导的方式得到 其一是假设输入样本的分布服从伯努利分布，而伯努利分布属于指数族分布，所以我们可以通过将用来进行模型参数估计的似然函数调整为服从指数族分布的表现形式就可以推导得到sigmoid函数 其二则是根据最大熵模型导出，通过对模型的复杂度作出限制，根据模型未知时熵最大的模型最佳，来转化为最大熵的约束条件优化问题，然后通过拉格朗日对偶性求解得到 优缺点 优点 判别模型：LR模型直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题 预测概率：LR模型不是仅预测出类别，而是可得到近似概率预测，这对于许多需利用概率辅助决策的任务很有用 凸函数：对数几率函数是任意阶可导的凸函数，有很好的数学性质，现有很多数值优化算法均可直接用于求取最优解 效率：实现简单，分类时计算量非常小，速度快，存储资源低 缺点 容易欠拟合，一般准确度不太高 原始的LR仅能处理二分类问题，且必须线性可分(衍生出的softmax可以用于多分类) 模型训练 参数估计：应用极大似然估计法估计模型参数，从而将问题转化为以对数似然函数为目标函数的最优化问题，然后采用梯度下降法或者拟牛顿法进行求解 损失函数：$$-\\frac{1}{N}\\sum_{i=1}^N[y_iln\\pi(x_i)+(1-y_i)ln(1-\\pi(x_i))]$$,最大对数似然函数与最小对数损失函数是等价的，对数似然函数表示为$$lnL(w)=\\sum_{i=1}^N[y_iln\\pi(x_i)+(1-y_i)ln(1-\\pi(x_i))]$$ LR模型在工业界的应用常见应用场景 预估问题场景（如推荐、广告系统中的点击率预估，转化率预估等） 分类场景（如用户画像中的标签预测，判断内容是否具有商业价值，判断点击作弊等 LR适用上述场景的原因LR模型自身的特点具备了应用广泛性 模型易用：LR模型建模思路清晰，容易理解与掌握； 概率结果：输出结果可以用概率解释（二项分布），天然的可用于结果预估问题上； 强解释性：特征（向量）和标签之间通过线性累加与Sigmoid函数建立关联，参数的取值直接反应特征的强弱，具有强解释性； 简单易用：有大量的机器学习开源工具包含LR模型，如sklearn、spark-mllib等，使用起来比较方便，能快速的搭建起一个learning task pipeline； 其他考点横向对比与联系 LR与线性回归的联系与区别逻辑回归和线性回归首先都可看做广义的线性回归，其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。 LR与最大熵模型逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应类别为二类时的特殊情况，也就是当逻辑回归类别扩展到多类别时，就是最大熵模型。 指数簇分布的最大熵等价于其指数形式的最大似然。 二项式分布的最大熵解等价于二项式指数形式(sigmoid)的最大似然； 多项式分布的最大熵等价于多项式分布指数形式(softmax)的最大似然。 LR与感知机 相同点 都是线性分类模型，原始模型都只能处理二分类问题 模型的形式相似(都是在线性模型外通过联系函数映射到另外的空间，只不过感知机是符号函数，LR是sigmoid(或者说Logistic)函数) 训练方法都是使用梯度下降法 不同点 LR输出为仅是概率的值，而感知机输出为1/-1 多分类-softmax如果y不是在[0,1]中取值，而是在K个类别中取值，这时问题就变为一个多分类问题。有两种方式可以出处理该类问题：一种是我们对每个类别训练一个二元分类器（One-vs-all），当K个类别不是互斥的时候，比如用户会购买哪种品类，这种方法是合适的。如果K个类别是互斥的，即y=i的时候意味着y不能取其他的值，比如用户的年龄段，这种情况下 Softmax 回归更合适一些。Softmax 回归是直接对逻辑回归在多分类的推广，相应的模型也可以叫做多元逻辑回归（Multinomial Logistic Regression）。模型通过 softmax函数来对概率建模，具体形式如下：$$P(y=i|x, \\theta) = \\frac{e^{\\theta_i^T x}}{\\sum_j^K{e^{\\theta_j^T x}}}$$其中决策函数为$$y^* = argmax_i P(y=i|x,\\theta)$$损失函数为$$J(\\theta)=-\\frac{1}{N} \\sum_i^N \\sum_j^K {1[y_i=j] \\log{\\frac{e^{\\theta_i^T x}}{\\sum {e^{\\theta_k^T x}}}}}$$ 正则化核函数SVM原理概述 从感知机说开：首先SVM也是一种针对分类特别是二分类任务的模型，而关于SVM模型的原理可以先从感知机说起；感知机在处理一项二分类任务时，从几何的角度理解可以认为是试图在输入空间内找到一个可以将样本点按照类别分开的超平面，不过感知机是通过最小化误分类的样本点来完成这样操作，直观上理解就是将划分超平面往被错分的样本点方向移动直到越过误分类点完成正确分类，所以这种方式寻找到的分类超平面往往存在无穷多个(对于线性可分问题)；而支持向量机则不限于此，其目的不仅是找到一个可以将正反样例正确分类的超平面，它还试图将正反样例尽可能地分开，也就是说让样例点到分割超平面的距离尽可能大；所以相对而言，(线性可分)支持向量机可以理解为在感知机的基础上加了一层约束，及间隔最大化约束，使得训练得到的分离超平面是唯一的；可以看看感知机和SVM的模型表达形式是一致的，只是SVM多了更强的约束条件$$f(x)=sign(w^*\\cdot x+b^*)$$ 那么如何使得样例点到分割超平面的距离最大化呢？首先我们需要找到距离分离超平面距离最近的点(即支持向量)，然后如果我们能最大化这些点到分离超平面的距离也就间接地最大化了所有数据集到分离超平面的距离；其次这个距离该如何衡量呢？定义函数间隔来衡量这一距离(通过点到平面的距离可以推导得出几何间隔)，$\\widehat{\\gamma}=min_{i=1,…,N}\\widehat{\\gamma_i}$，但是选择分离超平面时，仅有函数间隔是不够的，因为$w,b$可以成比例变化使得函数间隔为无穷大而不具备可比性，所以需要对$w$加约束，如规范化，令$||w||=1$，于是得到几何间隔$\\gamma_i=y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})$，因此问题就转化成了最大化样例点到分离超平面的几何间隔的约束最优化问题，也就是SVM的原始问题，可通过凸优化直接求解$$max_{w,b} \\frac{\\widehat{\\gamma}}{||w||}$$$$ s.t. y_i(w\\cdot x_i+b) \\geq \\gamma i=1,2,…,N$$,N$$，其中$\\widehat{\\gamma}$即表示超平面关于数据集的函数间隔，是超平面关于数据集中的样本点的函数间隔$\\widehat{\\gamma_i}$的最小值因为函数间隔并不会影响最优化问题的解，所以原问题等价于$$min_{w,b} \\frac{1}{2}{||w||^2}$$$$ s.t. y_i(w\\cdot x_i+b) - 1\\geq 0 i=1,2,…,N$$ 对偶算法，但是上述使用间隔最大化方法求解分离超平面的方法计算效率并不高，所以人们通过引入拉格朗日乘子根据拉格朗日对偶性将问题转化为广义的拉格朗日极大极小问题，然后使用SMO等算法求解出模型的参数值 转化为对偶问题的好处 对偶问题往往更易求解：以前新来的要分类的样本首先根据$w$和$b$做一次线性运算，然后看求的结果是大于0还是小于0,来判断正例还是负例。现在有了$\\alpha_i$，我们不需要求出$w$，只需将新来的样本和训练数据中的所有样本做内积和即可。那有人会说，与前面所有的样本都做运算是不是太耗时了？其实不然，我们从KKT条件中得到，只有支持向量的$\\alpha_i&gt;0$，其他情况$\\alpha_i=0$。因此，我们只需求新来的样本和支持向量的内积，然后运算即可。 自然地引入了核函数 进一步的为了使模型能够解决非线性分类问题，人们引入了核函数，那么核函数是什么，为什么能解决非线性问题？首先我们的目的是使用一个线性分类方法来求解非线性分类问题，而用线性分类方法求解非线性分类问题分为两步： 首先使用一个变换将原空间的数据映射到新空间； 在新空间里用线性分类学习方法从训练数据中学习分类模型；核技巧就属于这样的方法 核技巧 核技巧应用到支持向量机，其基本想法是通过一个非线性变换将输入空间（欧式空间或离散集合）对应于一个特征空间（希尔伯特空间），使得在输入空间中的超曲面模型对应于特征空间中的超平面模型(支持向量机)；这样分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成了 核技巧：在核函数给定的条件下，可利用解线性分类问题的方法求解非线性分类问题的支持向量机；学习是隐式地在特征空间进行的，不需要显式地定义特征空间和映射函数 实际应用中，常依赖于领域知识直接选择核函数，其有效性通过实验验证 个人理解是对输入数据做了一个映射操作(输入空间–&gt;特征空间)，使得原本线性不可分的样例点现在线性可分了，这样就可以直接使用SVM求解线性可分问题的方法来解决原本线性不可分的问题 扩展拉格朗日对偶性的KKT条件https://zhuanlan.zhihu.com/p/26514613 优缺点 优点可实现非线性分类、可用于分类与回归，低泛化误差，易解释 缺点对核函数及参数敏感 训练方法SMO算法(如何理解以及为什么高效？) SMO算法是支持向量机学习的一种快速算法，其特点是不断的将原二次规划问题分解为只有两个变量的二次规划问题，并对子问题进行解析求解，直到所有变量满足KKT条件为止，这样通过启发式的方法得到原二次规划问题的最优解，因为子问题有解析解，所以每次计算子问题都很快，虽然计算子问题次数很多，但在总体上还是高效的 损失函数等价于合页损失函数 概述软间隔/线性支持向量机的原始问题可以等价于添加了正则化项的合页损失函数，即最小化以下目标函数$$min_{w,b} \\sum_{i=1}^{N}[1-y_i(w\\cdot x_i+b)]_++\\lambda ||w||^2$$ 第一项为合页损失函数$L(y(w\\cdot x+b))=[1-y_i(w\\cdot x_i+b)]_+$,一般对于函数$[z]_+$有：$$[z]_+=z if z&gt;0$$$$[z]_+=0 if z\\leq 0$$所以原式表明当样本点$x_i,y_i$被正确分类且函数间隔(确信度)$y_i(w\\cdot x_i+b)$大于1时损失为0，否则损失是$1-y_i(w\\cdot x_i+b)$ 第二项为正则化项，是系数为$\\lambda$的$w$的$L_2$范数 对比线性支持向量机的目标函数可以发现合页损失函数的第一项等价为惩罚项(正则项)，而第二项则等价于“分离超平面的参数项” 线性支持向量机的原始最优化问题与上述目标函数的最优化问题是等价的 为什么使用合页损失函数？ 由于0-1损失函数不是连续可导的，直接优化由其构成的目标损失函数比较困难，所以对于svm而言，可以认为是在优化由0-1损失函数的上界(合页损失函数)构成的目标函数，又称为代理损失函数 合页损失函数对学习有更高的要求 常用替代损失函数，通常具有较好的数学性质，比如凸的连续函数且是0/1损失函数的上界 常见工业应用场景将支持向量机改进的聚类算法被称为支持向量聚类，当数据未被标记或者仅一些数据被标记时，支持向量聚类经常在工业应用中用作分类步骤的预处理。 GBDT原理及推导从三个方面来分解讲解，其一是boosting，其二是梯度上升，其三则是决策树关于这三点，个人感觉从加法模型的前向分步算法来理解比较好，首先前向分步算法提供了一种学习加法模型的普遍性方法，不同形式的基函数、不同形式的损失函数都可以用这种普遍性方法去求出加法模型的最优化参数；而对于boosting而言，基本思想是训练一系列的弱学习器，每一个学习器修正前一个基学习器的错误，最终得到的一个整体的学习器就是我们的集成模型，而我们在每一轮训练一个基学习器并将其累加到当前的集成学习器的过程刚好符合前向分步算法的思想；如果将其形式化为加法模型地表示形式就是一系列基学习器加权和的形式，而我们的目的是要求得最小化训练损失函数的基学习器以及该学习器的权重值； 这个时候就要用到梯度上升的思想了，我们可以将训练损失函数对当前集成学习函数进行泰勒一阶展开，基学习器的权重值为正，因此当我们假设基学习器的值为损失函数关于当前集成模型的响应(负梯度值)时后面一项为负，即可实现损失函数的降低，而另基学习器为当前集成模型的响应(负梯度值)也恰好给我们学习怎样的基学习器提供了思路，所以基学习器的学习也就转化成一个拟合响应值的回归问题(这也就是为什么使用回归树而不是分类树的原因)，而一旦确定了基学习器，对于其权重的确定就是一个线性搜索的过程(而这一过程又体现了机器学习中先局部后整体的思想，即先做局部优化再做整体优化，可类比决策树的生成和剪枝) 对于GBDT而言，具体的实现boosting这一思想是通过梯度上升的方法，这里可以稍微和AdaBoost进行一个对比，最开始完成boosting思想的实现的学习器是AdaBoost,而这个分类器的boosting思想是通过调整每一轮用于训练基学习器的训练集样本权重分布，使得上一轮基学习器分类错误的样本在这一轮赋予更高权重使其受到更多关注来实现的；而GBDT则是通过梯度上升的方式来实现这一思想，使得损失函数泛化成更一般的损失函数，具体的做法是每一轮根据预设的损失函数关于当前集成模型的响应来训练基学习器，这里也就用到了梯度上升的思想，因为对于损失函数而言，在梯度方向的变化是最快的； 然后就是GBDT里面使用的基学习器一般是决策树准确的说是回归树，因为在使用GBDT做分类时，我们每一步拟合响应用来与损失函数作比较的是一个可加的实数值，这样可以将当前学习到的基学习器的输出结果累加到当前的集成模型上，这样一步一步训练降低误差并逼近真实的样本标记 优缺点 优点：它的非线性变换比较多，表达能力强，而且不需要做复杂的特征工程和特征变换。 缺点:Boost是一个串行过程，不好并行化，而且计算复杂度高，同时不太适合高维稀疏特征。 模型训练 每一轮根据响应来训练一棵回归树，所以GBDT的训练实际上是多棵决策树的训练，每一轮先计算设定的损失函数关于当前的集成模型的响应，然后通过拟合响应来训练回归树，这里树的生成过程中，原始的CART回归树是逐个遍历某特征的所有特征值作为切分点选择平方误差最小的切分点作为切分 得到该轮的回归树后通过线性搜索的方式来进一步最小化集成模型与回归树的加权和与真实标记的误差来确定回归树的权重(步长)，这样就确定了该轮学得的集成模型 终止条件是基学习器数目达到预设的值 工业应用 CTR预估(广告点击率预估)：CTR预估中应用得最多的模型是LR，LR属于线性模型，容易并行化，但是线性模型的学习能力有限，需要大量的特征工程去预先分析出有效的特征、特征组合从而间接的增强LR的非线性学习能力(是不是想到了神经网络的输出层与隐层的关系)，Facebook于2014年的文章介绍通过使用GBDT来解决LR特征组合的问题，而随后Kaggle竞赛也有时间类似思路使用GBDT+FM，主要原因是GBDT可以发现多种有区分性的特征及特征组合，决策树的路径可以直接作为LR的输入特征使用，省去了人工寻找特征、特征组合的步骤 决策树决策树的损失函数对数似然损失 参考 百度文库","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"校招","slug":"校招","permalink":"http://yoursite.com/tags/校招/"},{"name":"面试","slug":"面试","permalink":"http://yoursite.com/tags/面试/"}]},{"title":"聚类小记","date":"2017-08-30T08:29:06.000Z","path":"2017/08/30/cluster/","text":"聚类任务无监督学习常见的无监督学习包括 聚类 密度估计 异常检测 NOTE:异常检测常借助聚类或距离计算进行，如将远离所有聚簇中心的样本作为异常点，或将密度极低处的样本作为异常点 聚类定义 聚类试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”(cluster).通过这样的划分，每个簇可能对应于一些潜在的概念(类别)，这些概念对聚类算法而言事先是未知的，聚类过程仅能自动形成簇结构，簇所对应的概念语义需由使用者来把握和命名 聚类的用途聚类既能作为一个单独的过程，用于寻找数据内在的分布结构，也可作为分类等其他学习任务的前驱过程 聚类算法涉及的两个基本问题性能度量——“有效性指标” 对聚类结果需要通过某种性能度量来评估其好坏 若明确了最终将要使用的性能度量，则可直接将其作为聚类过程的优化目标，从而更好地得到符合要求的聚类结果 “物以类聚”：同一簇的样本尽可能彼此相似，不同簇的样本尽可能不同，即聚类结果的“簇类相似度高且簇间相似度低” 外部指标将聚类结果与某个“参考模型”进行比较 Jaccard系数(JC)FM系数(FMI)Rand系数(RI)内部指标直接考察聚类结果而不利用任何参考模型 DB指数(DBI)Dunn指数(DI)Note:聚类的性能度量还有F值，互信息，平均廓宽等 距离计算 有序属性——闵可夫斯基距离(此外还有内积距离、余弦距离等) 欧氏距离 曼哈顿距离 无序属性——VDM 两者可结合处理混合属性 加权距离针对样本空间中不同属性的重要性不同的情形 非度量距离距离度量不满足直递性，模式识别、图像检索等涉及复杂语义的应用中常会涉及 Notes: 属性即特征 上述距离计算都是事先定义好的，现实任务中有时需要基于数据样本来确定合适的距离计算式，此时可通过“距离度量学习”来实现 聚类算法类型原型聚类——基于原型的聚类假设聚类结构能够通过一组原型刻画，一般算法先对原型进行初始化，然后对原型进行迭代更新求解，采用不同的原型表示，不同的求解方式将产生不同的算法 基于原型向量K均值 最小均值误差 贪心策略 迭代优化 K均值的变体 k-medoids算法强制原型向量必为训练样本 k-modes算法可处理离散属性 Fuzzy C-means(FCM)则是软聚类算法，允许每个样本以不同程度同时属于多个原型 采用某种Bregman距离可增强k均值算法对更多类型簇结构的适用性(原本k均值算法仅在凸形簇结构上效果较好) 引入核技巧可得到核k均值算法，与谱聚类有密切联系，谱聚类可视为在拉普拉斯特征映射降维后执行k均值聚类 LVQ——学习向量量化与k均值算法类似，“学习向量量化”（Learning Vector Quantization，LVQ）也是试图找到一组原型向量来刻画聚类结构，区别在于LVQ假设数据样本带有类别标记，学习过程利用样本的这些监督信息来辅助聚类。 基于概率模型高斯混合聚类 使用高斯分布刻画原型，簇划分则由原型对应后验概率确定 常采用EM算法迭代优化求解 Notes: k均值算法可视为高斯混合聚类在混合成分方差相等、且每个样本仅指派给一个混合成分时的特例 密度聚类密度聚类亦称“基于密度的聚类”（density-based clustering），此类算法假设聚类结构能通过样本分布的紧密程度确定。通常情况下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇以获得最终的聚类结果。 DBSCANDBSCAN是一种著名的密度聚类算法，它基于一组“领域”（neighborhood）参数来刻画样本分布的紧密程度 其他密度聚类算法 OPTICS DENCLUE 层次聚类层次聚类（hierarchical clustering）试图在不同层次对数据集进行划分，从而形成树形的聚类结构。数据集的划分可采用“自底向上”的聚合策略，也可采用“自顶向下”的分拆策略。 AGNESAGNES是一种采用自底向上聚合策略的层次聚类算法。它先将数据集中的每个样本看作一个初始聚类簇，然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并，该过程不断重复，直至达到预设的聚类簇个数，这里的关键是如何计算聚类簇之间的距离。 自顶向下 DIANA AGNES与DIANA都不能对已合并或已分拆的聚类簇进行回溯调整，BIRCH则对此进行了调整","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"聚类","slug":"聚类","permalink":"http://yoursite.com/tags/聚类/"}]},{"title":"深度学习小记","date":"2017-08-17T11:21:35.000Z","path":"2017/08/17/deep-learning/","text":"基于李宏毅老师的讲义对深度学习做一个简单入门整理 导论从机器学习讲起 机器学习可视为寻找一个表征输入输出之间的函数关系 深度学习 基础 权重&amp;偏置 神经元 不同连接方式导致不同的网络结构 全连接前向反馈神经网络 训练 寻找最小化整体损失的函数–&gt;寻找最小化整体损失的函数的参数 如何选择参数：枚举不可取，一般使用梯度下降法(先设置一个初始值，随机挑选或者RBM预训练，使用梯度下降法迭代更新参数直到收敛) BP 何为deep?为什么是deep?任何连续函数可以被进一层隐层的神经网络实现，只要有足够多的神经元 那为何偏爱更深层的神经网络而不是更胖的神经网络呢？ 相同参数数目的前提下，越深/窄的神经网络性能优于越胖/短的神经网络 类比逻辑电路，使用多层神经元代表一些函数更简单 模块化 前一层的神经元被作为一整个模块共享给下一层神经元，使得每个神经元都有充足的训练样本(相当于每个基分类器都有充足的训练样本)，这样可以用不多的数据来训练模型 模块化是从数据中自动学习的 深度学习的”hello world” 程序Keras vs. TensorFlow/theano Keras相当于一个封装了TensorFlow/theano的接口 Keras易于学习和使用，也具有一定的灵活性 TensorFlow/theano更灵活，但学习的成本较高 识别手写数字使用Keras的步骤 定义一个模型集合 评价模型的优劣 挑选最佳模型 训练深度神经网络的一些技巧常用训练技巧(调参/优化) 选择合适的损失函数 多分类使用softmax时用交叉熵作为损失函数而不是均方误差 mini-batch batch_size&amp;nb_epoch 每一轮打乱训练样本 新的激活函数 对于Sigmoid激活函数，神经网络层数过多时会出现梯度消失现象(反向传播梯度到前面的神经元时梯度缩小到很小的值导致学习较慢几乎是随机更新参数) 以前人们会使用RBM与训练来尝试解决问题，现在人们使用新的激活函数ReLU(Rectified Linear Unit) 计算速度快 解决了梯度消失的问题 Leaky ReLU vs. Parametric ReLU Maxout,ReLU是Maxout的一种特例 适应性学习率 设置特殊变量每一轮适当的减小学习率 对不同的参数设置不同的学习率–Adagrad,由参数的导数决定，导数小则学习率大 Momentum(动量)难以确定最佳的网络参数，引入物理学中动量的思想来尝试跳出局部最小点–Adam 过拟合最佳方法–&gt;获取更多训练数据或者创造更多的训练数据 Early Stopping 正则化 权重衰减，类比人类大脑会随着年龄增长去掉神经元之间的一些无用的连接 Dropout 每次更新参数之前，每个神经元有p%的概率被丢弃，从而变化了网络结构 可视为一种集成方式 网络结构 神经网络的变体CNN(Convolutional Neural Network)广泛应用于图像处理当使用全连接网络处理图像时，第一层会非常大，能否根据图像识别的特性来简化全连接网络呢？ 结构概述 卷积 一些模式相对整个图像来说非常小，一个神经元不必看到整张图片来发现这一模式，可以使用少量参数，连接到一小块区域 同一模式可能出现在不同区域，然而对应的神经元所进行的工作基本相同，所以可以让他们共享参数 其过程有点类似根据不同的过滤器filter来得到不同的模式，同样一张图像根据模式被“切片”成一层一层的薄片 池化 降低分辨率不会改变物体，所以可以尝试降低分辨率来使图片变小从而减少网络的参数 每个filter是一个通道(channel)来进行max pooling得到比原始图像小的图像 Flatten 将上述操作得到的二维切片展开成一维然后输入到全连接网络 应用 模型训练仍然是使用梯度下降的方法 围棋就可以用CNN来训练，因为棋局具备一些局部的模式，而这些局部的模式在棋盘上会在不同地方多次出现，但是Alpha Go里并没有使用最大池化层 RNN(Recurrent Neural Network)Neural Network with Memory 特点 隐层的输出被存储在内存中再次作为输入 LSTM(Long Short-term Memory) 4 inputs,1 output input gate,output gate,forget gate 学习方法：BPTT(Backpropagation through time),实际应用中RNN的学习并不容易 可以处理梯度消失现象，但不能应对梯度爆炸问题 GRU(Gated Recurrent Unit)比LSTM简单 其他的RNN Clockwise RNN SCRN(Structurally Constrained Recurrent Network) Vanilla RNN 应用案例 插值填充(Slot Filling)–相同长度的输入输出序列 电影的评价分类–输入是一个向量的序列(比如一条一条带有感情色彩的评论)，输出则是一个向量(对于电影的评价态度分级) 语音识别–输入输出都是句子，但是输出较短–&gt;CTC(Connnectionist Temporal Classification) 机器翻译–输入输出都是句子序列，而且长度没有限制 输入图像输出一句描述–One to Many 下一个浪潮 监督学习 超深度学习网络不同深度的神经网络的集成 Residual Networks FractalNet highway network Attention模型 阅读理解 视觉问答 语音问答(托福听力阅读测试) 强化学习 执行使奖励最大化的行为 Alpha GO是监督学习+强化学习的产物 当前的一些困难 短期奖励与长远奖励 agent的行为会影响他接下来接收的数据–&gt;exploration 深度强化学习 应用 交互式检索 AI玩电子游戏 自动驾驶 Google通过DeepMind-Powered AI削减了巨大的电力支出 非监督学习 图像–感知世界 生成新的图像 deep dream auto-encoder GAN(Generative Adversarial Network) 文本–理解字词的含义 词向量、类比、相似度 语音–非监督式地学习自然语言 WaveNet AI训练师–口袋妖怪训练师","tags":[{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"}]},{"title":"Spark小记","date":"2017-08-16T10:59:23.000Z","path":"2017/08/16/spark/","text":"概述 Spark是一种开源的分布式计算框架，设计初衷是为了克服MapReduce集群计算模式的一些缺陷(线性数据流结构)，相较之下，它支持循环算法、交互式数据分析、流式处理、批应用等，此外它还降低了保持工具分离的门槛 特点 速度快：in-memory cluster computing,将马上要处理的数据存储在内存中，减少了对磁盘的读写操作 支持多种语言，含有内置的针对Java,Scala,Python等语言的接口 可用于更高级的数据分析：不仅仅支持MapReduce,还支持SQL查询、流式数据、机器学习、图算法等 Spark与HadoopSpark一般有三种方式来与Hadoop相结合，如图 单机(Standalone):Spark位于HDFS之上，分配空间给HDFS,这种方式下Spark和MapReduce在集群上并行执行各自的任务 Hadoop Yarn:Spark位于Yarn之上，无需任何预安装或者根访问权限，帮助Spark融入Hadoop生态 Spark in MapReduce(SIMR):用户可以直接运行Spark并使用它的shell而无需任何管理权限 架构 CoreSpark框架的基础，是其他所有功能赖以执行的底层引擎，提供了以下功能 分布任务 任务调度 基本的I/O功能 内存中的计算和建立外部存储系统上的数据集的索引 SQL 引入称之为DataFrame的数据抽象，支持结构型/半结构型数据的处理 DSL(domain specific languages)支持多语言编写(Java,Scala,Python)以及SQL语言的处理 Streaming 利用了Spark核心的快速调度能力来执行流式分析 通过mini-batches的方式处理数据并对数据进行RDD(Resilient Distributed Datasets)转换 实现了Lambda结构 MLlib基于Spark的分布式机器学习框架，比Mahout快，比Vowpal Wabbit伸缩性强 GraphX基于Spark的图处理框架 RDD(Resilient Distributed Datasets)RDD是Spark中的基本数据结构，它是一种不可更改的分布式对象的集合，RDD中的每一个数据集都会被逻辑分割从而在集群的不同节点上进行运算；RDD能够包含任意的Python、Java、Scala对象，包括用户自定义类型；它是一种只读的记录切分的集合，具有容错能力，能被并行处理 创建RDD的方法 在驱动程序中并行化现有的一个集合 对外部存储系统上的数据集建立索引，可以是文件共享系统、HDFS、HBase或者任何提供Hadoop输入格式的数据源 MapReduce vs. RDD Spark通过RDD的概念来实现更快、更高效的MapReduce操作 关键字：disk I/O vs. in-memory computing 数据共享 MapReduce MapReduce上的数据共享较慢。在大多数框架下，各个运算之间重复使用数据的方式通常只有将数据写入一个外部的稳定存储系统中，尽管该框架提供了多种访问集群运算资源的抽象方式，用户们的需求仍然得不到满足。 循环和交互式应用都需要并行任务之间快速的数据共享，而MapReduce中的数据共享较慢，主要是由于复制、序列化、磁盘读写等操作；考虑存储系统，大多数Hadoop应用90%的时间消耗在对HDFS的读写操作上 RDD支持内存中执行运算，将内存的状态存储为一个对象并在各个任务之间共享，这种方式使得比基于网络、磁盘的方式快10~100倍 循环操作 MapReduce在多个阶段的应用中的多个运算中重复使用中间结果 RDD将中间结果存储在分布式内存中，而不是存储在磁盘上，从而使系统更快(当然若分布式内存不足时，会被存储到磁盘上) 交互式操作 MapReduce用户在数据的同一子集上执行ad-hoc查询，每个查询都会对固定的存储进行磁盘读写操作，而这一操作可能会主导整个应用的执行时间。 RDD将经常被查询的数据子集存储在分布式内存上提高访问速度 Notes:默认情况下，每次对RDD的转换都会重新进行一次计算，可以维持一个永久的RDD在内存中，这样Spark就可以为集群上的元素提供更快的访问速度。 共享变量Broadcast variable Broadcast variable允许编程人员在每个机器上维护一个只读的变量，而不必随任务传递它，可以用来以一种高效的方式给每个节点一个大型数据集的副本 Spark还尝试使用高效的broadcast算法来分布broadcast变量从而减少通信消耗 AccumulatorsAccumulators变量只被用于协助性的操作，因此支持高效的并行","tags":[{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"Big Data","slug":"Big-Data","permalink":"http://yoursite.com/tags/Big-Data/"}]},{"title":"推荐系统实践小记","date":"2017-08-15T13:25:09.000Z","path":"2017/08/15/recommend-in-action/","text":"基于项亮《推荐系统实践》一书的笔记 概述分类信息vs.搜索vs.推荐系统以购物为例，若可供选择的商品很少，就像你信步走进一家小便利店，也许你只需要随意浏览就能找到你所需要的商品，但如果数据量较大时。 类型 类比 分类信息 沃尔玛的分类指示牌 搜索引擎 淘宝、当当等电商的搜索引擎 推荐系统 淘宝、当当等的诸如“猜你喜欢、喜欢XX的用户也喜欢XX” 推荐系统与分类信息、搜索引擎的主要区别在于用户是否有明确的需求或者说用户的需求是否能够用言语描述 任务 推荐系统的任务就是联系用户和信息，一方面帮助用户发现对自己有价值的信息，另一方面让信息能够展现在对它感兴趣的用户面前，从而实现信息消费者和信息生产者的双赢 长尾推荐系统通过发掘用户的行为，找到用户的个性化需求，从而将长尾商品准确地推荐给需要它的用户，帮助用户发现那些他们感兴趣但很难发现的商品。 本质推荐算法的本质是通过一定的方式将用户和物品联系起来，而不同的推荐系统利用了不同的方式 应用评测","tags":[{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"推荐系统实践","slug":"推荐系统实践","permalink":"http://yoursite.com/tags/推荐系统实践/"},{"name":"推荐系统","slug":"推荐系统","permalink":"http://yoursite.com/tags/推荐系统/"}]},{"title":"hadoop小记","date":"2017-08-14T13:00:37.000Z","path":"2017/08/14/hadoop/","text":"Hadoop是一种开源框架，通过这一框架可以在由不同的主机的集群所构成的分布式环境上使用简单的编程模型来存储和处理大数据。 大数据背景：大数据爆发 何谓大数据？大量数据集的集合，难以被传统的计算技术处理，现在已经不单单指数据，而是一个完整的课题，包括相关的各种工具、技术和框架 常见大数据 黑盒数据：记录了飞机的信息 社交网络数据 股票交易数据 能源网数据 交通数据 搜索引擎数据 数据类型 结构型数据：关系型数据 半结构型数据：XML数据 非结构型数据：文档、日志、文本等 技术类型 操纵大数据 MongoDB:即时交互 NoSQL+云计算：高效、降低计算成本、技术成本 分析大数据 MPP(Massively Parallel Processing)数据库系统+MapReduce 面临挑战 获取数据 策展(Curation) 存储 查找 共享 传输 分析 表示 传统方法 概述：使用关系型数据库诸如Oracle,SQL Server,DB2等来与数据库进行交互和管理，针对目标数据进行操作并展示给用户来做进一步分析 局限性：数据量少时可以很好的执行，数据量大时会受到执行时间、处理器性能的制约 Google的解决方案 MapReduceGoogle提出了MapReduce算法通过将原始任务分解为一个一个的小部分然后分配给多个互联的计算机来并行处理，最终收集各个计算机的结果来组成最终所需的结果 HadoopHadoop通过MapReduce算法来在多个不同的CPU上处理数据 Hadoop架构 Hadoop Common: 其他模块的基础，提供了文件系统和操作系统的抽象，包含了必要的Java库和脚本来启动Hadoop Hadoop YARN: 用来进行任务调度和集群资源管理 Hadoop Distributed File System (HDFS™): 分布式文件系统，提供了对应用数据的高吞吐量访问权限 Hadoop MapReduce: 基于YARN的系统，用来并行的处理大型数据集 Notes:自2012年起，Hadoop不仅仅指上面提到的基本模块，还有一系列基于这些模块或相当的比如Apache Pig, Apache Hive, Apache HBase, Apache Spark等等 MapReduceHadoop MapReduce是一种软件框架能够很容易的编写应用来在大型集群上并行地处理大数据，具有可靠性、容错性 基本任务 Map将原始的数据进行转换，通常将独立元素重构成元组 Reduce将Map操作生成的数据元素转换成更小的元素集合 一般而言输入输出都保存在文件系统，这一框架还需要进行任务调度、任务监控以及重新执行失败的任务；MapReduce含有一个JobTracker，针对每一个集群节点都有一个TaskTracker JobTracker负责资源管理、追踪资源的消耗和权限并调度slaves之间的任务，监控slaves并重新执行失败的任务 TaskTracker执行master所传达的任务，并间隔性的向master反馈任务状态信息 Notes:最初Hadoop的MapReduce服务仅有一个JobTracker，若是该JobTracker出了故障，整个执行任务都会停止 HDFS(Hadoop Distributed File System) Hadoop最常用的一种文件系统 基于GFS(Google File System)，提供分布式文件系统，保证程序可靠、能容错的运行在有成千上百的计算机构成的大型集群上 采用master/slave架构，其中的仅含有一个master(NameNode)来管理文件系统中的元数据，此外含有一个或多个slave(DataNodes)来存储真实数据 HDFS命名空间中的文件被切分为多个区块(block)，而这些区块保存在一系列的DataNodes中 NameNode负责将区块映射到各个DataNodes DataNodes负责对文件系统的读写操作，以及接收NameNode的指令来对区块进行创建、删除、复制等操作 Hadoop的优点 Hadoop框架为用户快速编写和测试分布式系统提供了便利，它高效、自动地将数据和任务分配到底层的机器或者并行的处理器上 Hadoop自己提供了错误检测和处理的库，不依赖于硬件来提供容错性 可以动态的增删服务器而不会中断Hadoop的运行 基于Java，兼容几乎所有平台 开源 HDFS概述HDFS的特点 高容错性，兼容低成本硬件 存储大量数据并提供便捷的访问 文件可存储在不同类别的机器上，并以保留副本的形式来使系统具备错误修复的能力 使应用能够被并行处理 便于分布式存储和执行 Hadoop提供了与HDFS交互的命令接口 内置的namenode\\datanode服务器帮助用户更方便的检查集群的状态 提供对文件系统数据的流式访问 提供权限管理功能 HDFS的架构 Namenodenamenode是含有操作系统以及namenode软件的商业性硬件，作为系统的master服务器，主要执行以下任务 管理文件系统命名空间 管制客户端对文件的权限 对文件系统执行重命名、关闭、打开等操作 Datanode对于集群的每个节点都含有一个datanode(由GNU/Linux操作系统+datanode软件构成的商业性硬件设施)，负责管理他们系统的数据存储 针对每一个用户的请求对文件系统执行读写操作 根据namenode的指令执行终止创建、删除、复制等操作 Block一般用户的数据以文件的形式存储在HDFS中，文件系统中的文件会被切分为一个或多个分段来存储在独立的数据节点上，这些数据分段就被称为区块(block)，也就是HDFS中数据的最小读写单位，默认的区块大小为64MB，可以通过HDFS的配置进行修改。 HDFS的目标 错误检测和恢复 针对大型数据集，HDFS应当具备上百个节点来管理具有大型数据集的应用 数据分配的硬件，当计算任务发生在数据附近时，请求会被快速执行，特别涉及大量的数据集，可以有效减少网络流量而增加吞吐量 MapReduce概述分治的思想最核心的两项操作分别是Map()和Reduce()，Map()通常负责将输入数据映射成字典型数据(键值对)，而Reduce()操作则将Map()转换后的数据组合成一组组的键值对数据并进一步的采取特定的操作；MapReduce的好处在于非常便于将数据分布式的在多个计算机节点上执行； 核心思想通俗来讲MapReduce就是讲计算机“送往”数据所在的位置MapReduce的执行主要有三个步骤 Map {k1:v1} –&gt; [{k2:v2}]处理输入数据，将原始的HDFS文件系统中的数据逐行处理为多个数据块(several chunks of data) Shuffle根据键值将经Map处理后的数据分配到各个Reduce处理设备上 Reduce {k2:[..v2..]} –&gt; [{k3:v3}]各个处理设备并行处理所分配的数据，并产生对应的结果 更细化的操作可以参考Wikipedia StreamingHadoop Streaming使得用户可以通过将任意可执行的文件或脚本作为mapper/reducer来创建或者执行Map/Reduce任务 参考资料 Hadoop Tutorial Wikipedia","tags":[{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"big data","slug":"big-data","permalink":"http://yoursite.com/tags/big-data/"}]},{"title":"神经网络","date":"2017-08-09T05:31:57.000Z","path":"2017/08/09/neural-network/","text":"本文是针对周志华《机器学习》一书第五章的读书笔记 神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。 Notes:神经网络是一种难解释的“黑箱模型”，当前有一些工作尝试改善其可解释性，主要是从神经网络中抽取易于理解的符号规则 神经元定义 神经网络中最基本的成分是神经元模型常用’M-P’神经元模型，在该模型中，神经元接收到来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过“激活函数”处理以产生神经元的输出。 Notes:’M-P’神经元模型是应用最为广泛的神经元模型，此外还有脉冲神经元模型等 激活函数 理想中激活函数是阶跃函数，但由于其不连续、不光滑，所以常常使用Sigmoid函数作为替代 近来人们在使用卷积神经网络时常常使用修正线性函数作为神经元的激活函数，对应的神经元被称为ReLU(Rectified Linear Unit) 感知机与多层网络感知机 感知机由两层神经元组成，输入层接收外界输入信号后传递给输出层，输出层为M-P神经元，亦称为“阈值逻辑单元” 感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元，其学习能力非常有限 学习过程：若将阈值$\\theta$看做一个固定输入为-1的哑结点所对应的连接权重$w_{n+1}$，则可统一为权重的学习，针对训练样例$(x,y)$若当前感知机的输出为$\\hat{y}$$$w_i\\leftarrow w_i+\\Delta w_i$$$$\\Delta w_i=\\eta(y-\\hat{y})x_i$$ Notes:关于感知机更详细的内容可参照感知机 多层前馈神经网络 要解决非线性可分问题，需考虑使用多层功能神经元，在输入层和输出层之间添加隐含层，隐层和输出层神经元都是拥有激活函数的功能神经元 多层前馈神经网络(multi-layer feedforward neural networks)：每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接（网络信号可往后传，但是网络拓扑结构上不存在环或回路），其中输入层神经元接收外界输入，隐层与输出层神经元对信号进行加工，最终结果由输出层神经元输出 反向传播算法(BP)概述反向传播算法(error BackPropagation)(误差逆传播算法)是用来训练多层网络的一种强大算法 标准BP算法推导神经网络的学习过程，就是根据训练数据来调整神经元之间的连接权以及每个功能神经元的阈值，下面就简单叙述下BP算法中对于权重更新的推导 算法-BackPropagation 标准BP算法vs.累积BP算法BP算法的目标是要最小化训练集D上的累积误差$$E=\\frac{1}{m}\\sum_{k=1}^mE_k$$ 标准BP算法每次仅针对一个训练样例更新连接权重，参数更新的更加频繁，而且对不同样例进行更新的效果可能出现“抵消”现象，所以一般为了达到同样的累计误差极小点，迭代次数较多 累积BP算法则是直接针对累积误差最小化，在读取整个训练集D一遍以后才对参数进行更新，更新频率较低，但是在累积误差下降到一定程度后，进一步下降会非常缓慢，此时标准BP往往会更快会的较好的解（尤其对于训练集D非常大时） Notes: 标准BP算法与累积BP算法的区别类似于随机梯度下降和标准梯度下降 BP算法实质是LMS(Least Mean Square)算法的推广 过拟合问题由于强大的表示能力，BP神经网络经常遭遇过拟合，一般有两种策略来缓解其过拟合 early stopping：将数据集分为训练集和验证集，训练集用来计算梯度、更新连接权和阈值，验证集则用来估计误差，若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值 正则化：基本思想是在误差目标函数中增加一个用于描述网络复杂度的部分例如连接权和阈值的平方和，假设$E_k$表示第k个训练样例上的误差，$w_i$表示连接权和阈值，则误差目标函数改变为$$E=\\lambda\\frac{1}{m}\\sum_{k=1}^mE_k+(1-\\lambda)\\sum_iw_i^2$$,其中$\\lambda \\in (0,1)$用于对经验误差与网络复杂度两项进行折中，常通过交叉验证法来估计 全局最小与局部极小常用处理局部极小的策略 以多组不同参数初始化多个神经网络，相当于从多个不同初始点开始搜索 模拟退火计数，在每一步以一定的概率接收比当前解更差的结果，从而有助于跳出局部极小（但也会跳出全局最小），该概率随着时间推移而降低 随机梯度下降，随机梯度下降在计算梯度时加入了随机因素，所以即使陷入局部极小点，它计算出的梯度仍可能不为0，于是就有机会跳出局部极小继续搜索 遗传算法 Notes:上述方法大多是启发式，理论上尚缺乏保障 其他常见神经网络RBF网络RBF(Radial Basis Function)(径向基函数)网络，单隐层前馈神经网络，使用径向基函数作为隐层神经元的激活函数，输出层是对隐层神经元输出的线性组合 模型假定输入为d维向量$x$,q为隐层神经元个数，$c_i,w_i$分别为第i个隐层神经元所对应的中心和权重，$\\rho(x,c_i)$为径向基函数，通常定义为样本$x$到数据中心$c_i$之间欧氏距离的单调函数$$\\phi(x)=\\sum_{i=1}^qw_i\\rho(x,c_i)$$ 训练过程 确定神经元中心$c_i$，常用方式包括随机采样、聚类等 利用BP算法等来确定参数 ART网络竞争性学习 竞争性学习是神经网络中一种常用的无监督学习的策略，使用该策略时，网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元的状态被抑制(胜者通吃原则) ART网络定义ART(Adaptive Resonance Theory)(自适应谐振理论)网络是竞争型学习的代表，该网络由比较层、识别层、识别阈值和重置模块构成 比较层负责接收输入样本，并将其传递给识别层神经元 识别层，每个神经元对应一个模式类，神经元数目可在训练过程中动态增长以增加新的模式类 训练过程接收到比较层的输入信号后，识别层神经元之间相互竞争以产生获胜神经元 竞争方式：计算输入向量与每个识别层神经元所对应的模式类的代表向量之间的距离，距离最小者获胜 若输入向量与获胜神经元所对应的代表向量间的相似度大于识别阈值，则当前输入样本将被归为该代表向量所属的类别，同时网络的连接权将会更新使以后接收到相似输入样本时该获胜神经元有更大可能获胜 若相似度不大于识别阈值，则重置模块将在识别层增设一个新的神经元，其代表向量就设置为当前输入向量 优点ART比较好的缓解了竞争型学习中的“可塑性-稳定性窘境” 可塑性：神经网络学习新知识的能力 稳定性：神经网络在学习新知识时要保持对旧知识的记忆 因此ART可进行增量学习或在线学习 增量学习：在学得模型后，再接受到训练样例时仅需根据新样例对模型进行更新，不必重新训练整个模型 在线学习：每获得一个新样本就进行一次模型更新，增量学习可看为“批模式”的在线学习 发展早期ART网络仅能处理布尔型输入数据，后续发展成一个算法族 能处理实值输入的ART2网络 结合模糊处理的FuzzyART网络 可进行监督学习的ARTMAP网络 SOM网络概述 SOM(Self-Organizing Map)(自组织映射)网络，一种竞争学习型的无监督神经网络，能将高维输入数据映射到低维空间，同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元 SOM的训练目标是为每个输出层神经元找到合适的权向量，以达到保持拓扑结构的目的 SOM网络在聚类、高维数据可视化、图像分割等方面有广泛应用 训练过程 接收到一个训练样本后，每个输出层神经元会计算该样本与自身携带的权重向量之间的距离，距离最近的神经元获胜，称为最佳匹配单元 最佳匹配单元及其邻近神经元的权向量将被调整，以使这些权向量与当前输入样本的距离缩小，该过程不断迭代直至收敛 级联相关网络结构自适应网络不同于一般神经网络模型通常假定网络结构事先固定，训练的目的是利用训练样本来确定合适的连接权、阈值等参数，结构自适应网路将网络结构也当做学习的目标之一，期望能在训练过程中找到最符合数据特点的网络结构 Notes: 结构自适应网络也称为构造性神经网络 ART网络也属于一种结构自适应网络 概述级联相关网络(Cascade-Correlation) 级联:建立层次连接的层级结构，开始训练时，网络仅有输入层和输出层，处于最小拓扑结构，随着训练的进行，新的隐层神经元逐渐加入，从而创建起层级结构，当新的隐层神经元加入时，其输入端连接权值是冻结固定的 相关：指通过最大化新神经元的输出与网络误差之间的相关性来训练相关的参数 特点与一般的前馈神经网络相比，级联相关网络无需设置网络层数、隐层神经元数目，且训练速度较快，但其在数据较小时易陷入过拟合 Elman网络RNN递归神经网络(recurrent neural networks)允许网络中出现环形结构，从而可让一些神经元的输出反馈回来作为输入信号，这样的结构与信息反馈过程使得网络在t时刻的输出状态不仅与t时刻的输入有关，还与t-1时刻的网络状态有关，从而能处理与时间有关的动态变化 概述 Elman网络是最常用的递归神经网络之一，结构类似多层前馈网络，但隐层神经元的输出被反馈回来与下一时刻输入层神经元提供的信号一起作为隐层神经元在下一时刻的输入 隐层神经元常采用Sigmoid激活函数，网络的训练常通过推广的BP算法进行 Boltzmann机概述 基于能量的模型：为网络状态定义一个能量，能量最小化时网络达到理想状态，而网络的训练就是在最小化这个能量函数 一种递归神经网络 神经元分为两层 显层：用于表示数据的输入与输出 隐层：数据的内在表达 Boltzmann机中的神经元都是布尔型的，即只能取0,1两种状态 若网络中的神经元以任意不依赖于输入值的顺序进行更新，则网络最终将达到Boltzmann分布(亦称平衡态、平稳分布)，此时状态向量s出现的概率将仅由其能量与所有可能的状态向量的能量确定 训练过程 将每个训练样本视为一个状态向量，使其出现的概率尽可能大 标准的Boltzmann机是一个全连接图，训练的复杂度很高而难以用于解决现实任务，现实中常用受限Boltzmann机(RBM,Restricted Boltzmann Machine),仅保留显层与隐层之间的连接 深度学习大数据+云计算$\\to$深度学习 训练算法典型的深度学习模型就是很深层的神经网路（在神经网络里添加更多的隐层），这样会带来训练上的问题，若直接使用经典算法(BP算法)进行训练，误差在多隐层内逆传播时往往会发散而不能收敛到稳定状态，常有以下方法来解决这一问题 无监督逐层训练：基本思想是每次训练一层隐节点，采用预训练的方法逐层全部训练完成后，在对整个网络进行微调，代表有深度信念网络(deep belief network,DBN) 权共享：让一组神经元使用相同的连接权，代表是卷积神经网络(Convolutional Neural Network,CNN) 理解深度学习 深度学习可以看做是经过多层处理逐渐将初始的“低层”的特征表示转化成“高层”特征表示后，用“简单模型”(此处将网络中前若干层处理都看作是在进行特征表示，只把最后一层处理看作是在进行分类)即可完成复杂的分类等学习任务，由此可将深度学习理解为进行“特征学习(feature learning)”或“表示学习(representation learning)” 机器学习vs.深度学习：承接上面的理解的话，机器学习中描述样本的特征的工作通常由人类来进行(特征工程)，而深度学习若视为特征学习则是通过机器学习技术自身来产生好特征，相当于使机器学习向“全自动数据分析”又前进了一步","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"Neural Networks","slug":"Neural-Networks","permalink":"http://yoursite.com/tags/Neural-Networks/"}]},{"title":"推荐系统小记","date":"2017-08-04T11:11:52.000Z","path":"2017/08/04/recommend/","text":"本文是基于探索推荐引擎内部的秘密系列文章的笔记 初探概述 背景人们需要从海量数据中快速获取目标信息 搜索引擎 vs. 推荐系统主要区别在于用户对于自己的需求是否明确以及该需求能否用简单的关键字概括 工作原理 推荐引擎利用特殊的信息过滤技术，将不同的物品或内容推荐给可能对它们感兴趣的用户。 基本流程：数据源–&gt;推荐系统–&gt;用户 数据源 要推荐的物品/内容的元数据：关键字、基因描述等 用户的基本信息 用户对物品/信息的偏好 显式的用户反馈：评分、评论等 隐式的用户反馈：交互记录如购买、浏览等 Notes:显式的用户反馈 vs. 隐式的用户反馈 显式的用户反馈能较为准确地反映用户对商品的真实喜好，但是需要用户付出额外的代价(个人理解是获取这部分数据的难度相对较大) 隐式的用户反馈经分析和处理也能反映用户喜好，但是相对不够精确，存在噪音，而且在不同应用中需要构建不同的行为特征 分类按用户群体划分 基于大众行为的推荐引擎：对每个用户给出同样的推荐，一般由系统管理员人为设定或者基于所有用户反馈统计得出：比如当下流行商品 个性化推荐引擎：对不同用户根据其偏好给出个性化的精准推荐 按数据源划分如何发现数据的相关性 基于人口统计学的推荐：根据系统用户的基本信息发现用户的相关程度 基于内容的推荐：根据所推荐物品或内容的元数据 基于协同过滤的推荐：根据用户对物品、信息的偏好发现物品或内容本身的相关性、或发现用户的相关性 按推荐模型的建立方式划分 基于物品和用户本身的，这种推荐引擎将每个用户和每个物品都当作独立的实体，预测每个用户对于每个物品的喜好程度，这些信息往往是用一个二维矩阵描述的。由于用户感兴趣的物品远远小于总物品的数目，这样的模型导致大量的数据空置，即我们得到的二维矩阵往往是一个很大的稀疏矩阵。同时为了减小计算量，我们可以对物品和用户进行聚类，然后记录和计算一类用户对一类物品的喜好程度，但这样的模型又会在推荐的准确性上有损失。 基于关联规则的推荐（Rule-based Recommendation）：关联规则的挖掘已经是数据挖掘中的一个经典的问题，主要是挖掘一些数据的依赖关系，典型的场景就是“购物篮问题”，通过关联规则的挖掘，我们可以找到哪些物品经常被同时购买，或者用户购买了一些物品后通常会购买哪些其他的物品，当我们挖掘出这些关联规则之后，我们可以基于这些规则给用户进行推荐。 基于模型的推荐（Model-based Recommendation）：这是一个典型的机器学习的问题，可以将已有的用户喜好信息作为训练样本，训练出一个预测用户喜好的模型，这样以后用户在进入系统，可以基于此模型计算推荐。这种方法的问题在于如何将用户实时或者近期的喜好信息反馈给训练好的模型，从而提高推荐的准确度。 Notes:在现在的推荐系统中，很少有只使用了一个推荐策略的推荐引擎，一般都是在不同的场景下使用不同的推荐策略从而达到最好的推荐效果 推荐机制各个推荐机制的工作原理、优缺点和应用场景承接上面按照数据源划分的分类方式 基于人口统计学的推荐 基于内容的推荐 基于协同过滤的推荐：根据用户对物品或者信息的偏好，发现物品或者内容本身的相关性，或者是发现用户的相关性，然后再基于这些关联性进行推荐。 基于用户的协同过滤推荐 基于项目的协同过滤推荐 基于模型的协同过滤推荐 混合的推荐机制 加权混合 切换的混合 分区的混合 分层的混合 对于混合推荐机制 加权的混合（Weighted Hybridization）: 用线性公式（linear formula）将几种不同的推荐按照一定权重组合起来，具体权重的值需要在测试数据集上反复实验，从而达到最好的推荐效果。 切换的混合（Switching Hybridization）：前面也讲到，其实对于不同的情况（数据量，系统运行状况，用户和物品的数目等），推荐策略可能有很大的不同，那么切换的混合方式，就是允许在不同的情况下，选择最为合适的推荐机制计算推荐。 分区的混合（Mixed Hybridization）：采用多种推荐机制，并将不同的推荐结果分不同的区显示给用户。其实，Amazon，当当网等很多电子商务网站都是采用这样的方式，用户可以得到很全面的推荐，也更容易找到他们想要的东西。 分层的混合（Meta-Level Hybridization）: 采用多种推荐机制，并将一个推荐机制的结果作为另一个的输入，从而综合各个推荐机制的优缺点，得到更加准确的推荐。 应用国内外的两个案例 电子商务–亚马逊 核心是通过数据挖掘算法和比较用户的消费偏好于其他用户进行对比，借以预测用户可能感兴趣的商品。 分区的混合的机制：将不同的推荐结果分不同的区显示给用户 今日推荐 (Today’s Recommendation For You): 通常是根据用户的近期的历史购买或者查看记录，并结合时下流行的物品给出一个折中的推荐。 新产品的推荐 (New For You): 采用了基于内容的推荐机制 (Content-based Recommendation)，将一些新到物品推荐给用户。在方法选择上由于新物品没有大量的用户喜好信息，所以基于内容的推荐能很好的解决这个“冷启动”的问题。 捆绑销售 (Frequently Bought Together): 采用数据挖掘技术对用户的购买行为进行分析，找到经常被一起或同一个人购买的物品集，进行捆绑销售，这是一种典型的基于项目的协同过滤推荐机制。 别人购买 / 浏览的商品 (Customers Who Bought/See This Item Also Bought/See): 这也是一个典型的基于项目的协同过滤推荐的应用，通过社会化机制用户能更快更方便的找到自己感兴趣的物品。 设计和用户体验 基于社会化的推荐，Amazon 会给你事实的数据，让用户信服，例如：购买此物品的用户百分之多少也购买了那个物品； 基于物品本身的推荐，Amazon 也会列出推荐的理由，例如：因为你的购物框中有 ，或者因为你购买过 ，所以给你推荐类似的 *。 基于用户的 profile，包括：用户的行为、评分、收藏夹、购物车等，并提供了让用户自主管理自己 profile 的功能，通过这种方式用户可以更明确的告诉推荐引擎他的品味和意图是什么。 社交网络–豆瓣 “豆瓣猜”：基于社会化的协同过滤的推荐，这样用户越多，用户的反馈越多，那么推荐的效果会越来越准确。 “看过”和“想看”：相较于亚马逊，模型相对简单，更加专注于用户的品味 “喜欢这个电影的人也喜欢的电影”：基于协同过滤 协同过滤针对深入推荐引擎相关算法 - 协同过滤的笔记 集体智慧 集体智慧是指在大量的人群的行为和数据中收集答案，帮助你对整个人群得到统计意义上的结论，这些结论是我们在单个个体上无法得到的，它往往是某种趋势或者人群中共性的部分。 典型应用 Wikipedia Google–PageRank 协同过滤核心(Collaborative Filtering) 协同过滤一般是在海量的用户中发掘出一小部分和你品位比较类似的，在协同过滤中，这些用户成为邻居，然后根据他们喜欢的其他东西组织成一个排序的目录作为推荐给你。 核心问题 如何确定一个用户是不是和你有相似的品位？ 如何将邻居们的喜好组织成一个排序的目录？ 协同过滤相对于集体智慧而言，它从一定程度上保留了个体的特征，就是你的品位偏好，所以它更多可以作为个性化推荐的算法思想。 收集用户偏好从用户的行为和偏好中发现规律，并基于此给予推荐 用户行为所反映的用户偏好 显式 评分 投票 转发 保存书签 标记标签 评论 隐式 点击 页面停留时间 购买 如何组合不同的用户行为 将不同的行为分组：一般可以分为“查看”和“购买”等等，然后基于不同的行为，计算不同的用户/物品相似度。类似于当当网或者 Amazon 给出的“购买了该图书的人还购买了…”，“查看了图书的人还查看了…” 根据不同行为反映用户喜好的程度将它们进行加权，得到用户对于物品的总体喜好。一般来说，显式的用户反馈比隐式的权值大，但比较稀疏，毕竟进行显示反馈的用户是少数；同时相对于“查看”，“购买”行为反映用户喜好的程度更大，但这也因应用而异。 数据与处理 减噪 归一化 寻找相似用户/物品根据用户喜好(用户偏好的二维矩阵)计算相似用户和物品，然后基于相似用户或者物品进行推荐 相似度计算基于向量（Vector），计算两个向量的距离，距离越近相似度越大。在推荐的场景中，在用户-物品偏好的二维矩阵中，我们可以将一个用户对所有物品的偏好作为一个向量来计算用户之间的相似度，或者将所有用户对某个物品的偏好作为一个向量来计算物品之间的相似度。 常用相似度计算方法 欧几里得距离假设 x，y 是 n 维空间的两个点，它们之间的欧几里德距离是：$$d(x,y)=\\sqrt{(\\sum(x_i-y_i)^2)}$$,对应的相似度计算表示为$$sim(x,y)=\\frac{1}{1+d(x,y)}$$ 皮尔逊相关系数皮尔逊相关系数一般用于计算两个定距变量间联系的紧密程度，它的取值在[-1，+1]之间。$$p(x,y)=\\frac{\\sum x_iy_i-n\\overline{xy}}{(n-1)s_xs_y}$$,其中$s_xs_y$是$x$和$y$的样品标准偏差 Cosine相似度Cosine相似度被广泛应用于计算文档数据的相似度$$T(x,y)=\\frac{x\\cdot y}{||x||^2 \\times ||y||^2}=\\frac{\\sum x_iy_i}{\\sqrt{\\sum x_i^2}\\sqrt{\\sum y_i^2}}$$ Tanimoto 系数（Tanimoto Coefficient）Tanimoto系数也称为Jaccard系数，是Cosine相似度的扩展，也多用于计算文档数据的相似度：$$T(x,y)=\\frac{x\\cdot y}{||x||^2+||y||^2-x\\cdot y}=\\frac{\\sum x_iy_i}{\\sqrt{\\sum x_i^2}+\\sqrt{\\sum y_i^2}-\\sum x_iy_i}$$ 相似邻居计算根据相似度找到用户-物品的邻居 常用的挑选邻居的原则 固定数量的邻居：K-neighborhoods 或者 Fix-size neighborhoods:不论邻居的“远近”，只取最近的K个,这种方法对于孤立点的计算效果不好，因为要取固定个数的邻居，当它附近没有足够多比较相似的点，就被迫取一些不太相似的点作为邻居，这样就影响了邻居相似的程度 基于相似度门槛的邻居：Threshold-based neighborhoods:与计算固定数量的邻居的原则不同，基于相似度门槛的邻居计算是对邻居的远近进行最大值的限制，落在以当前点为中心，距离为K的区域中的所有点都作为当前点的邻居，这种方法计算得到的邻居个数不确定，但相似度不会出现较大的误差。这种方法计算出的邻居的相似度程度比前一种优，尤其是对孤立点的处理。 计算推荐在得到了相邻用户和相邻物品后，可以基于这些信息为用户进行推荐 基于用户的CF基于用户对物品的偏好找到相邻邻居用户，然后将邻居用户喜欢的推荐给当前用户。计算上，就是将一个用户对所有物品的偏好作为一个向量来计算用户之间的相似度，找到K邻居后，根据邻居的相似度权重以及他们对物品的偏好，预测当前用户没有偏好的未涉及物品，计算得到一个排序的物品列表作为推荐。 基于物品的CF基于用户对物品的偏好找到相似的物品，然后根据用户的历史偏好，推荐相似的物品给他。从计算的角度看，就是将所有用户对某个物品的偏好作为一个向量来计算物品之间的相似度，得到物品的相似物品后，根据用户历史的偏好预测当前用户还没有表示偏好的物品，计算得到一个排序的物品列表作为推荐。 User CF vs. Item CF 计算复杂度：两个算法在不同的系统中各有优势 电子商务网站：Item CF 从性能和复杂度上比 User CF 更优，其中的一个主要原因就是对于一个在线网站，用户的数量往往大大超过物品的数量，同时物品的数据相对稳定，因此计算物品的相似度不但计算量较小，同时也不必频繁更新。 新闻，博客或者微内容的推荐系统，情况往往是相反的，物品的数量是海量的，同时也是更新频繁的，因而效率角度而言User CF 更优 适用场景 Item CF更适宜非社交网络，在非社交网络的网站中，内容内在的联系是很重要的推荐原则，同时 Item CF 便于为推荐做出解释 User CF更适宜社交网络，User CF 加上社会网络信息，可以增加用户对推荐解释的信服程度。 推荐多样性与精度 推荐多样性的度量方法 从单个用户的角度度量，就是说给定一个用户，查看系统给出的推荐列表是否多样，也就是要比较推荐列表中的物品之间两两的相似度，不难想到，对这种度量方法，Item CF的多样性显然不如User CF的好，因为Item CF的推荐就是和以前看的东西最相似的。 考虑系统的多样性，也被称为覆盖率 (Coverage)，它是指一个推荐系统是否能够提供给所有用户丰富的选择。在这种指标下，Item CF的多样性要远远好于User CF, 因为 User CF总是倾向于推荐热门的，从另一个侧面看，也就是说，Item CF的推荐有很好的新颖性，很擅长推荐长尾里的物品。所以，尽管大多数情况，Item CF的精度略小于User CF，如果考虑多样性，Item CF却比 User CF好很多 两种算法具有很好的互补性：结合的基本原则就是当采用 Item CF 导致系统对个人推荐的多样性不足时，我们通过加入 User CF 增加个人推荐的多样性，从而提高精度，而当因为采用 User CF 而使系统的整体多样性不足时，我们可以通过加入 Item CF 增加整体的多样性，同样同样可以提高推荐的精度。 例子：假设每个用户兴趣爱好都是广泛的，喜欢好几个领域的东西，不过每个用户肯定也有一个主要的领域，对这个领域会比其他领域更加关心。给定一个用户，假设他喜欢 3 个领域 A,B,C，A 是他喜欢的主要领域，这个时候我们来看User CF和Item CF倾向于做出什么推荐：如果用User CF,它会将A,B,C三个领域中比较热门的东西推荐给用户；而如果用ItemCF，它会基本上只推荐 A领域的东西给用户。所以我们看到因为User CF只推荐热门的，所以它在推荐长尾里项目方面的能力不足；而Item CF只推荐A领域给用户，这样他有限的推荐列表中就可能包含了一定数量的不热门的长尾物品，同时Item CF的推荐对这个用户而言，显然多样性不足。但是对整个系统而言，因为不同的用户的主要兴趣点不同，所以系统的覆盖率会比较好。 用户对推荐算法的适应度 对于 User CF，推荐的原则是假设用户会喜欢那些和他有相同喜好的用户喜欢的东西，但如果一个用户没有相同喜好的朋友，那 User CF 的算法的效果就会很差，所以一个用户对的 CF 算法的适应度是和他有多少共同喜好用户成正比的。 Item CF 算法也有一个基本假设，就是用户会喜欢和他以前喜欢的东西相似的东西，那么我们可以计算一个用户喜欢的物品的自相似度。一个用户喜欢物品的自相似度大，就说明他喜欢的东西都是比较相似的，也就是说他比较符合 Item CF 方法的基本假设，那么他对 Item CF 的适应度自然比较好；反之，如果自相似度小，就说明这个用户的喜好习惯并不满足 Item CF 方法的基本假设，那么对于这种用户，用 Item CF 方法做出好的推荐的可能性非常低。 基于Apache Mahout实现高效的协同过滤推荐 Apache Mahout中提供的一个协同过滤算法的高效实现，它是一个基于Java实现的可扩展的，高效的推荐引擎。 原作者简要介绍了使用Apache Mahout搭建推荐系统的输入数据格式，构建的数据对象、数据结构以及分别从User CF、Item CF、Slope One这三种协同过滤的推荐算法介绍了下如何调用 User CF 通过输入数据构建数据模型 计算用户相似度(各种相似度计算方法) 计算用户的近邻(两种计算方法：数量/阈值) 构建推荐系统 Item CF 通过输入数据构建数据模型 计算物品相似度(各种相似度计算方法) 构建推荐系统 Slope One主要是针对大数据量提供的一种基于评分的轻量级的协同过滤算法，基本思想是将用户的评分之间的关系看作简单的线性关系 根据Data Model创建数据之间线性关系的模型DiffStorage。 基于Data Model和DiffStorage创建SlopeOneRecommender，实现Slope One推荐策略。 聚类聚类分析 聚类 (Clustering) 就是将数据对象分组成为多个类或者簇 (Cluster)，它的目标是：在同一个簇中的对象之间具有较高的相似度，而不同簇中的对象差别较大。所以，在很多应用中，一个簇中的数据对象可以被作为一个整体来对待，从而减少计算量或者提高计算质量。 聚类问题分类 聚类结果是排他的还是可重叠的:对于一个元素，他是否可以属于聚类结果中的多个簇中，如果是，则是一个可重叠的聚类问题，如果否，那么是一个排他的聚类问题。 基于层次还是基于划分 基于划分：拿到一组对象，按照一定的原则将它们分成不同的组。 基于层次：将这些对象分等级，在顶层将对象进行大致的分组，随后每一组再被进一步的细分，也许所有路径最终都要到达一个单独实例，这是一种“自顶向下”的层次聚类解决方法，对应的，也有“自底向上”的。 簇数目固定的还是无限制的聚类：聚类问题是在执行聚类算法前已经确定聚类的结果应该得到多少簇，还是根据数据本身的特征，由聚类算法选择合适的簇的数目。 基于距离还是基于概率分布模型 基于距离的聚类问题：就是将距离近的相似的对象聚在一起。 基于概率分布模型的聚类问题：在一组对象中，找到能符合特定分布模型的点的集合，他们不一定是距离最近的或者最相似的，而是能完美的呈现出概率分布模型所描述的模型。 聚类算法基于距离的聚类算法K均值聚类算法基于距离的排他的划分方法 基本原理 首先创建一个初始划分，随机地选择k个对象，每个对象初始地代表了一个簇中心。对于其他的对象，根据其与各个簇中心的距离，将它们赋给最近的簇。 然后采用一种迭代的重定位技术，尝试通过对象在划分间移动来改进划分。所谓重定位技术，就是当有新的对象加入簇或者已有对象离开簇的时候，重新计算簇的平均值，然后对对象进行重新分配。这个过程不断重复，直到没有簇中对象的变化。 优点： 当结果簇是密集的，而且簇和簇之间的区别比较明显时，K均值的效果比较好。 对于处理大数据集，这个算法是相对可伸缩的和高效的，它的复杂度是 O(nkt)，n是对象的个数，k是簇的数目，t是迭代的次数 原理简单，实现起来也相对简单 缺点： 必须事先给出k的个数，k的选择一般都基于一些经验值和多次实验结果，对于不同的数据集，k的取值没有可借鉴性。 K均值对“噪音”和孤立点数据是敏感的，少量这类的数据就能对平均值造成极大的影响。 Canopy聚类算法基本原理：首先应用成本低的近似的距离计算方法高效的将数据分为多个组，这里称为一个 Canopy，Canopy之间可以有重叠的部分；然后采用严格的距离计算方式准确的计算在同一Canopy中的点，将他们分配与最合适的簇中。Canopy聚类算法经常用于K均值聚类算法的预处理，用来找合适的k值和簇中心。 模糊K均值聚类算法基本原理：和K均值一样，只是它的聚类结果允许存在对象属于多个簇，也就是说它属于可重叠聚类算法，与K均值聚类原理类似，模糊K均值也是在待聚类对象向量集合上循环，但是它并不是将向量分配给距离最近的簇，而是计算向量与各个簇的相关性（Association）。 基于概率分布模型的聚类算法狄利克雷聚类算法基本原理：首先需要定义一个分布模型，简单的例如：圆形，三角形等，复杂的例如正则分布，泊松分布等；然后按照模型对数据进行分类，将不同的对象加入一个模型，模型会增长或者收缩；每一轮过后需要对模型的各个参数进行重新计算，同时估计对象属于这个模型的概率。 参考资料 探索推荐引擎内部的秘密","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"Recommand System","slug":"Recommand-System","permalink":"http://yoursite.com/tags/Recommand-System/"}]},{"title":"随机森林","date":"2017-08-03T07:05:02.000Z","path":"2017/08/03/rf/","text":"概述理解bagging+decision tree+randomness简言之，随机森林是将多棵决策树模型以bagging的集成方式组合在一起并引入随机性的一种集成模型 bagging从bias-variance分解的角度来看倾向于减少模型的方差 随机森林的偏差值往往趋向于所有基学习器的偏差的期望值，也就意味着随机森林的偏差难以进一步提高，所以只有从方差的角度来改善模型性能 随机森林改善模型的方差的方法主要是减小基学习器之间的相关性，通过特征扰动来实现 关于随机森林中随机选取的特征子集空间大小的取值，直观而言，值越小则基学习器之间的相关性越小(因为特征重合度越低)，所以方差越小 并非所有的模型都适合作为bagging的基学习器，一般高度非线性模型特别是树模型比较合适 决策树模型往往方差较大(特别对于完全生长的决策树) 随机森林中的基分类器$g_t$就是决策树模型，最终以bagging的方式组合在一起(voting/averaging) 算法图片来源于“机器学习技法”课件 由于每棵树可以独立训练，所以随机森林模型的训练过程是一个并行的过程，相对高效 随机森林模型继承了决策树的优点并克服了完全生长的决策树的缺点 特征随机性随机森林的随机性除了使用bagging中的bootstrap方法采样训练出不同决策树以外，进一步地针对每棵决策树是随机的选择部分特征作为一个子特征集合来让该决策树进行学习，从而增强各个决策树的多样性 一般随机抽取的特征子集是极小于原始特征空间的，这样训练模型的效率更高 特征扰动的方式增强了基分类器之间的多样性，从而使得在最终bagging集成时使用voting或者averaging方法时更易于减小模型本身的方差 可以看作是一个将原始特征空间随机投影到不同的低维特征空间上进行模型训练，对于每棵树中的分支而言类似于一个感知机分类的过程(原始输入样本按照某一个随机选取的特征空间映射后根据某个阈值进行分类) out of bag example与bagging的过程相关图片来源于“机器学习技法”课件由图可知，经过N次bootstrap采样后，某一样本不被选中的概率为$(1-\\frac{1}{N})^N$，当$N$足够大时，取极限则该概率近似为$\\frac{1}{e}$，换句话说最后大约有$\\frac{1}{e}$的样本是没有被使用过的，于是我们可以利用这些样本进行模型的性能评估 对比设置验证集，out of bag(OOB)有以下特点 对于每一个基分类器我们都有部分样本是没有被选中的，所以我们可以使用这些样本来衡量基分类器的表现，但是实际中一般不这么做，因为我们往往不是很关心基分类器的表现，而只在乎最终集成模型的表现 对于最终的集成模型而言我们也可以使用OOB的方法进行模型评估，不过需要注意不要误选了被用于训练的样本来进行验证，具体做法可以选取某些未使用特定样本的模型组合起来的模型来对该部分样本进行验证评分，如此选取多次取平均表现即可作为最终集成模型的一个评估 因此随机森林可以不需要额外设置验证集来进行验证，我们训练好模型后就可以根据OOB的得分来得到对最终模型的一个评估，也就不必像使用验证集那样再次使用所有训练集重新训练模型，实际应用中，OOB往往能准确的评价最终模型 feature selection需要筛选的特征 冗余特征 不相关特征 特征筛选的优劣 优点 高效：减少了大量不必要的特征可以减少模型训练和预测的时间 泛化能力好：一般特征适当少的情况下，模型不易过拟合 可解释性：较少的特征便于结合业务场景进行解读 缺点 特征选择的计算量大，就其本身而言，是一个排列组合的数学问题，原始特征越多，可能性越多，直接穷举的话计算量很大 可能会过拟合：可能选取的特征恰好是使模型过拟合的特征 解释性不一定合理：承接上一条，若选取的是过拟合的特征，那么得出的结论本身是有问题的 特征选择的方法 线性模型可以根据线性模型中的权重值大小来判定一个特征的值 非线性模型本身非线性模型要想评价一个特征是否重要是比较困难的，因为各个特征之间可能存在未知的交叉关系，所以我们一般将问题简化为考虑各个特征各自的重要性，也就是对特征独自的重要性进行一个排序，取较重要的若干特征，一种对特征重要性进行打分的方式是random test random test考虑到测试一个特征是否重要可以通过在原始样本中加入噪声数据进行扰动的方法，来对比前后模型表现的差异，如果差异较大，证明该特征是重要的，具体而言，有三种常用方法 修改样本特征值的分布状况，比如修改为正态分布 使用bootstrap对原始特征值进行采样来进行扰动 最常用的一种方法是permutation test，即将原始特征值重新排列，举例来说比如在特征i下两个样本$x_{1,i},x_{2,i}$原始特征值分别为0,1,现在将两个样本的该特征值进行交换，然后重新投入训练并对模型进行打分，比较前后模型性能就可以判断该特征的重要性； 针对permutation test的模型评估，可以不必修改训练样本的特征值，而是重新排列验证集的特征值，这样就避免了重新训练模型的时间消耗，而且效果是类似的 random forest vs. decision tree相比较于单棵决策树而言，随机森林往往具有如下优势 分类后的正反例之间的间隔较大，如图所示(对比第一(决策树)和第三幅(随机森林))图片来源于“机器学习技法”课件 随机森林的分割边界更平滑 随机森林更加稳定，相较之下对于噪声点不太敏感，如图(左为决策树，右为随机森林) Notes:实际应用中随机森林的一个缺点是，在模型未达到一个较为稳定的状态时，模型对于随机性较为敏感，比如略微增减树的数目可能会影响模型的性能，这个时候比较建议适当增加树的数目","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"Random Forest","slug":"Random-Forest","permalink":"http://yoursite.com/tags/Random-Forest/"},{"name":"机器学习技法","slug":"机器学习技法","permalink":"http://yoursite.com/tags/机器学习技法/"}]},{"title":"模型比较小记之boosting内战","date":"2017-08-02T07:06:18.000Z","path":"2017/08/02/model-cmp/","text":"本文主要简要的比较了常用的boosting算法的一些区别，从AdaBoost到LightGBM,包括AdaBoost,GBDT,XGBoost,LightGBM四个模型的简单介绍，一步一步从原理到优化对比。 AdaBoost原理原始的AdaBoost算法是在算法开始的时候，为每一个样本赋上一个权重值，初始的时候，大家都是一样重要的。在每一步训练中得到的模型，会使得数据点的估计有对有错，我们就在每一步结束后，增加分错的点的权重，减少分对的点的权重，这样使得某些点如果老是被分错，那么就会被“重点关注”，也就被赋上一个很高的权重。然后等进行了N次迭代（由用户指定），将会得到N个简单的分类器（basic learner），然后我们将它们组合起来（比如说可以对它们进行加权、或者让它们进行投票等），得到一个最终的模型。 关键字：样本的权重分布 GBDT概述GBDT（Gradient Boosting Decison Tree）中的树都是回归树，GBDT用来做回归预测，调整后也可以用于分类（设定阈值，大于阈值为正例，反之为负例），可以发现多种有区分性的特征以及特征组合。GBDT是把所有树的结论累加起来做最终结论的，GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差(负梯度)，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。 Boosting的最大好处在于，每一步的残差计算其实变相地增大了分错instance的权重，而已经分对的instance则都趋向于0。这样后面的树就能越来越专注那些前面被分错的instance。 Gradient Boost与AdaBoost的区别 Gradient Boost每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度(Gradient)方向上建立一个新的模型。所以说，在Gradient Boost中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少。Shrinkage（缩减）的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。本质上，Shrinkage为每棵树设置了一个weight，累加时要乘以这个weight，但和Gradient并没有关系。 Adaboost是另一种boost方法，它按分类对错，分配不同的weight，计算cost function时使用这些weight，从而让“错分的样本权重越来越大，使它们更被重视”Gradient Boost优缺点 优点： 它的非线性变换比较多，表达能力强，而且不需要做复杂的特征工程和特征变换。 缺点:Boost是一个串行过程，不好并行化，而且计算复杂度高，同时不太适合高维稀疏特征。 XGBoostXGBoost能自动利用cpu的多线程，而且适当改进了gradient boosting，加了剪枝，控制了模型的复杂程度 传统GBDT以CART作为基分类器，特指梯度提升决策树算法，而XGBoost还支持线性分类器(gblinear)，这个时候XGBoost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。 xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。 Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（传统GBDT的实现也有学习速率） 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。 xgboost工具支持并行。注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。(特征粒度上的并行，block结构，预排序)xgboost中树节点分裂时所采用的公式：这个公式形式上跟ID3算法、CART算法是一致的，都是用分裂后的某种值减去分裂前的某种值，从而得到增益。为了限制树的生长，我们可以加入阈值，当增益大于阈值时才让节点分裂，上式中的gamma即阈值，它是正则项里叶子节点数T的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝。另外，上式中还有一个系数lambda，是正则项里leaf score的L2模平方的系数，对leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性。 XGBoost实现层面 内置交叉验证方法 能够输出特征重要性文件辅助特征筛选 XGBoost优势小结： 显式地将树模型的复杂度作为正则项加在优化目标 公式推导里用到了二阶导数信息，而普通的GBDT只用到一阶 允许使用列抽样(column(feature) sampling)来防止过拟合，借鉴了Random Forest的思想，sklearn里的gbm好像也有类似实现。 实现了一种分裂节点寻找的近似算法，用于加速和减小内存消耗。 节点分裂算法能自动利用特征的稀疏性。 样本数据事先排好序并以block的形式存储，利于并行计算 penalty function Omega主要是对树的叶子数和叶子分数做惩罚，这点确保了树的简单性。 支持分布式计算可以运行在MPI，YARN上，得益于底层支持容错的分布式通信框架rabit。 LightGBMlightGBM：基于决策树算法的分布式梯度提升框架。lightGBM与XGBoost的区别： 切分算法(切分点的选取) XGBoost使用的是pre-sorted算法（对所有特征都按照特征的数值进行预排序，基本思想是对所有特征都按照特征的数值进行预排序；然后在遍历分割点的时候用O(#data)的代价找到一个特征上的最好分割点最后，找到一个特征的分割点后，将数据分裂成左右子节点。优点是能够更精确的找到数据分隔点；但这种做法有以下缺点 空间消耗大，需要保存数据的特征值以及特征排序的结果(比如排序后的索引，为了后续快速计算分割点)，需要消耗两倍于训练数据的内存 时间上也有较大开销，遍历每个分割点时都需要进行分裂增益的计算，消耗代价大 对cache优化不友好，在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。 LightGBM使用的是histogram算法，基本思想是先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点；优点在于 占用的内存更低，只保存特征离散化后的值，而这个值一般用8位整型存储就足够了，内存消耗可以降低为原来的1/8。 降低了计算的代价：预排序算法每遍历一个特征值就需要计算一次分裂的增益，而直方图算法只需要计算k次（k可以认为是常数），时间复杂度从O(#data#feature)优化到O(k#features)。(相当于LightGBM牺牲了一部分切分的精确性来提高切分的效率，实际应用中效果还不错) 决策树生长策略上： XGBoost采用的是带深度限制的level-wise生长策略，Level-wise过一次数据可以能够同时分裂同一层的叶子，容易进行多线程优化，不容易过拟合；但不加区分的对待同一层的叶子，带来了很多没必要的开销（因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂） LightGBM采用leaf-wise生长策略，每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环；但会生长出比较深的决策树，产生过拟合（因此 LightGBM 在leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合）。 histogram 做差加速。一个容易观察到的现象：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。 直接支持类别特征：LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。并在决策树算法上增加了类别特征的决策规则。 分布式训练方法上(并行优化) 原始 特征并行的主要思想是在不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。 数据并行则是让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。 LightGBM针对这两种并行方法都做了优化， 在特征并行算法中，通过在本地保存全部数据避免对数据切分结果的通信； 在数据并行中使用分散规约(Reduce scatter)把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。基于投票的数据并行(Parallel Voting)则进一步优化数据并行中的通信代价，使通信代价变成常数级别。 Cache命中率优化 基于直方图的稀疏特征优化 DART(Dropout + GBDT) GOSS(Gradient-based One-Side Sampling):一种新的Bagging(row subsample)方法,前若干轮(1.0f / gbdtconfig-&gt;learning_rate)不Bagging;之后Bagging时, 采样一定比例g(梯度)大的样本 LightGBM优点小结(相较于XGBoost) 速度更快 内存消耗更低 参考文献 http://www.msra.cn/zh-cn/news/features/lightgbm-20170105 http://blog.csdn.net/xwd18280820053/article/details/68927422 http://www.flickering.cn/machine_learning/2016/08/gbdt%E8%AF%A6%E8%A7%A3%E4%B8%8A-%E7%90%86%E8%AE%BA/ http://wepon.me/files/gbdt.pdf","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"个人错题难题仓库","date":"2017-07-30T01:29:10.000Z","path":"2017/07/30/error/","text":"记录个人的一些疑难杂症 排列组合计算排列组合数根据相应的排列组合公式计算 排列$$A_n^m=n(n-1)(n-2)\\cdot\\cdot\\cdot(n-m+1)=\\frac{n!}{(n-m)!}$$ 组合$$C_n^m=\\frac{A_n^m}{m!}=\\frac{n!}{m!(n-m)!}$$12345678910111213141516171819202122232425262728/** * 计算阶乘数，即n! = n * (n-1) * ... * 2 * 1 * @param n * @return */ private static long factorial(int n) &#123; return (n &gt; 1) ? n * factorial(n - 1) : 1; &#125; /** * 计算排列数，即A(n, m) = n!/(n-m)! * @param n * @param m * @return */ public static long arrangement(int n, int m) &#123; return (n &gt;= m) ? factorial(n) / factorial(n - m) : 0; &#125; /** * 计算组合数，即C(n, m) = n!/((n-m)! * m!) * @param n * @param m * @return */ public static long combination(int n, int m) &#123; return (n &gt;= m) ? factorial(n) / factorial(n - m) / factorial(m) : 0; &#125; 计算排列组合 排列题目描述来源于牛客网 27、字符串的排列 输入一个字符串,按字典序打印出该字符串中字符的所有排列。 例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。 思路 基本思路：递归的思想，将问题分解成小问题，假设我们现在写的一个函数的功能是返回传入的字符串的所有排列情况，而对于一个字符串而言，若固定第一个字符，则后续字符的所有排列情况就是当前以该字符为首的字符串的所有排列状况，而接下来一面是递归的求解余下字符串的排列状况，另一面则是将首字符与后续不等字符交换来重复上述求排列的过程，递归的终止条件是需要排列的子串只有一个元素时，则直接返回该情形下的字符串，而此题需要用ArrayList来收集字符串，所以还需要一个额外的ArrayList来传递并收集字符串,但是注意这种写法无法处理一种重复元素的情形，对于只是判断首元素与后续元素是否相同可以处理，但是对于不同字符多次出现的状况不能处理，比如aabb,此外一般要求是按顺序输出，所以综合这两点要求，将ArrayList改为TreeSet最为合适 Java代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546import java.util.ArrayList;import java.util.TreeSet;public class Permutation &#123; public static void main(String[] args) &#123; String s=\"abc\"; System.out.println(Permutation(s)); String s2=\"aa\"; System.out.println(Permutation(s2)); String s3=\"aabb\"; System.out.println(Permutation(s3)); &#125; public static ArrayList&lt;String&gt; Permutation(String str) &#123; ArrayList&lt;String&gt; list=new ArrayList&lt;&gt;(); if(str==null || str.length()==0)&#123; return list; &#125; char[] chr=str.toCharArray(); TreeSet&lt;String&gt; set=new TreeSet&lt;&gt;(); permutation(chr,0,set); list.addAll(set); return list; &#125; public static void permutation(char[] chr, int index, TreeSet&lt;String&gt; set)&#123; if(index&gt;chr.length-1)&#123; return; &#125; //当指针移到最后一个元素时即排列只有当前一种情形，为终止条件 if(index==chr.length-1)&#123; set.add(String.valueOf(chr)); &#125; permutation(chr,index+1,set); for(int i=index+1;i&lt;chr.length;i++)&#123; if(chr[i]!=chr[index])&#123; exchange(chr,i,index); permutation(chr,index+1,set); exchange(chr,i,index); &#125; &#125; &#125; private static void exchange(char[] chr, int i, int j) &#123; char temp=chr[i]; chr[i]=chr[j]; chr[j]=temp; &#125;&#125; 排列 组合题目描述来源于leetcode Given two integers n and k, return all possible combinations of k numbers out of 1 … n.For example,If n = 4 and k = 2, a solution is:[ [2,4], [3,4], [2,3], [1,2], [1,3], [1,4],] 思路 思路，仍然是想利用递归的方法来做，假设我们现有一个方法的功能是返回n个数中取k个值的组合，那么，若当前固定了第一个数剩下要做的就是从剩余n-1个数中取出k-1个数的组合即为当前首位的组合情形，所以同样的接下来要做两件事，其一递归的在后续子数序列中取得k-1个数的组合，另外就是更改首位值的的处理，与排列类似却不同的是，这里不是替换首位值，而是让指向首位的指针后移一位得到不同的值并再次计算以该值固定时从n个数中取k个数的组合的情况终止条件是当k为0时,递推关系可表示为C(n,k)=C(n-1,k-1)+C(n-1,k)，这里n对应的用下表的移动来进行变换 Java代码12345678910111213141516171819202122232425262728293031323334import java.util.ArrayList;import java.util.HashSet;import java.util.List;import java.util.Set;public class Combination &#123; public static void main(String[] args) &#123; System.out.println(combine(5,3)); &#125; public static List&lt;List&lt;Integer&gt;&gt; combine(int n, int k) &#123; List&lt;List&lt;Integer&gt;&gt; totalList=new ArrayList&lt;List&lt;Integer&gt;&gt;(); if(n&lt;=0 || k&lt;=0 || n&lt;k)&#123; return totalList; &#125; List&lt;Integer&gt;list=new ArrayList&lt;&gt;(); Set&lt;List&lt;Integer&gt;&gt; set=new HashSet&lt;&gt;(); combine(n,k,1,set,list); totalList.addAll(set); return totalList; &#125; public static void combine(int n, int k, int index, Set&lt;List&lt;Integer&gt;&gt; set, List&lt;Integer&gt; list)&#123; if(k==0)&#123; set.add(new ArrayList(list)); return; &#125; list.add(index); combine(n,k-1,index+1,set,list); if(list.size()!=0) list.remove(list.size()-1); for(int i=index;i&lt;n-k+1;i++)&#123; combine(n,k,i+1,set,list); &#125; &#125;&#125; Notes:似乎与回溯法、DFS有关 参考资料 http://shukuiyan.iteye.com/blog/1095637 http://www.cnblogs.com/lifegoesonitself/p/3225803.html 整数中1出现的次数来源：牛客 题目描述 从1到n的整数中1出现的次数，例如：求出1~13的整数中1出现的次数，其中包含1的数字有1、10、11、12、13因此共出现6次（11表示两次） 思路解析阅读了一些书籍和博客的解析，感觉比较繁琐，反而是leetcode的高票答案简洁明了的解释了，现转述如下: 本题的暴力解法是对每一个数逐一遍历数位统计1的个数，然后又注意统计1~n的每个数的1的个数并最后加和 高效率解法 第i位为1的可能情形由第i位前后的数值范围共同决定 首先这道题若是从数值大小的方向找规律的话比较繁琐，虽然也有方法，但个人觉得不是很好用，比较好的一种方法是通过数位的角度来看数，因为本身这道题就是统计所有1出现的数目，像11这种数就表示1出现了两次，若执迷于按照数值大小统计很容易陷入数值范围与数位范围的混沌；下面就通过逐个数位的统计来说明，首先我们定义几个变量 原始传入数据n m用来统计当前的数位，从每次以10倍倍增，也就是说$m=1,10,100,…$ 当前数位划分的高位数据a=n/m,以及对应的剩余数据b=n%m 接下来对每一数位i，我们将该数位的可能取值范围0~9划分为3种情况并分a,b两部分统计该位为1时的a,b变化范围即对应的1的数目 i&gt;1时，因为此时i可以取1，所以a,b都能在自己的取值范围任意取值，举个例子，比如12345，假设我们现在统计百位上为1的情形即m=100 对应的a=123,b=45;假设置百位为1则简化表示为a1b，而对于a而言它的取值范围是0~12共有13种可能，而由于我们此刻是在统计百位为1的情形，所以一共就有13100种可能，细化下来分别为100~199,1100~1199,2100~2199,…,12100~12199,用一个一般性的式子表示就是$$(a/10+1)m$$ i=1时，当a取最大值时,对于b而言，有一部分值是取不到的，仍然举个例子比如12145，仍然统计百位，于是a=121,b=45,对于而对于a而言它的取值范围是0~12共有13种可能，但是这里需要注意当a=12时会受到b的允许取值的影响，具体说来对于121b而言，此时b的取值范围为0~45也就是12100~12145，对于12146~12199就超出了原始值n的范围了，而对于a其他取值依然是能够满足的，细化为100~199,1100~1199,2100~2199,…,11100~11199,12100~12145,用一个式子表示就是$$(a/10)*m+b+1$$; i=0时，这个时候a就不能取最大值了，其余情形都可以取得，比如12045，a=120,b=45,此时a的取值范围是0~11，而对于a的任一取值，b的取值不受限制，细化为100~199,1100~1199,2100~2199,…,11100~11199，用式子表示就是$$(a/10)*m$$ 最后只需要逐位统计该数位为1时的取值范围情况就能保证每个数位的1不被漏掉，因为我们是按照数位独立统计1的数目，所以也不会出现重复的情形，比如对于111而言，我们在统计个位时，是不考虑前面数位是否为1的，只考虑前面数位的取值范围，因为只要个位为1我们才会对前面的有效值进行1次记录，比如说111我们分别在个位、十位、百位都会对这个数进行统计，但是每次统计只是记录该数位的情况，所以对于这个数而言虽然是重复出现了，但是对于1的统计而言没有重复统计； Java代码1234567891011121314151617181920public int NumberOf1Between1AndN(int n) &#123; if(n&lt;=0)&#123; return 0; &#125; int sum=0; for(int m=1;m&lt;=n;m*=10)&#123; int a=n/m,b=n%m; int i=a%10; if(i==0)&#123; sum+=(a/10*m); &#125; else if(i==1)&#123; sum+=(a/10*m+b+1); &#125; else&#123; sum+=((a/10+1)*m); &#125; &#125; return sum;&#125;","tags":[{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"校招","slug":"校招","permalink":"http://yoursite.com/tags/校招/"},{"name":"面试","slug":"面试","permalink":"http://yoursite.com/tags/面试/"},{"name":"Coding","slug":"Coding","permalink":"http://yoursite.com/tags/Coding/"},{"name":"剑指offer","slug":"剑指offer","permalink":"http://yoursite.com/tags/剑指offer/"}]},{"title":"java_concepts","date":"2017-07-29T02:01:55.000Z","path":"2017/07/29/java-concepts/","text":"记录一些容易出现错误、混淆的点 Java函数间的参数传递问题踩过的坑剑指offer的一道题 二叉搜索树与双向链表:输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 调用部分 12345678910111213public static TreeNode Convert2(TreeNode pRootOfTree) &#123; if(pRootOfTree==null)&#123; return null; &#125; TreeNode[] curLast=new TreeNode[1]; lastOrder(pRootOfTree,curLast); //由于是当前最后一位，所以需要向左寻找到首位 TreeNode head=curLast[0]; while(head!=null &amp;&amp; head.left!=null)&#123; head=head.left; &#125; return head;&#125; 错误代码 123456789101112131415161718192021222324public static void lastOrder(TreeNode pRootOfTree, TreeNode curLast)&#123; if(pRootOfTree==null)&#123; return; &#125; TreeNode cur=pRootOfTree; //获得左子树的最后一位(最大值) if(cur.left!=null)&#123; lastOrder(cur.left,curLast); &#125; //修改当前节点的左指针 cur.left=curLast; if(curLast!=null)&#123; //修改左子树最后一位的右指针 curLast.right=cur; &#125; //更新当前最后一位指针 curLast=cur; //无效，因为相当于在递归函数中创建的了一个局部变量指向了curLast， //所以这里修改也只是将局部变量的引用替换为一个新的地址，而原始的值并未改变，所以返回上一层并没有用 //一种替代的方法是使用一个大小为1的数组来传参 //继续处理右子树 if(cur.right!=null)&#123; lastOrder(cur.right,curLast); &#125;&#125; 修改方案 12345678910111213141516171819202122public static void lastOrder(TreeNode pRootOfTree, TreeNode[] curLast)&#123; if(pRootOfTree==null)&#123; return; &#125; TreeNode cur=pRootOfTree; //获得左子树的最后一位(最大值) if(cur.left!=null)&#123; lastOrder(cur.left,curLast); &#125; //修改当前节点的左指针 cur.left=curLast[0]; if(curLast[0]!=null)&#123; //修改左子树最后一位的右指针 curLast[0].right=cur; &#125; //更新当前最后一位指针 curLast[0]=cur; //继续处理右子树 if(cur.right!=null)&#123; lastOrder(cur.right,curLast); &#125;&#125; Java参数传递(超经典) 知乎","tags":[{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"校招","slug":"校招","permalink":"http://yoursite.com/tags/校招/"},{"name":"面试","slug":"面试","permalink":"http://yoursite.com/tags/面试/"},{"name":"Coding","slug":"Coding","permalink":"http://yoursite.com/tags/Coding/"},{"name":"剑指offer","slug":"剑指offer","permalink":"http://yoursite.com/tags/剑指offer/"}]},{"title":"懒死骆驼的代码小黑本","date":"2017-07-27T03:24:59.000Z","path":"2017/07/27/tricks/","text":"记录一些容易忽略问题以及奇技淫巧 浮点数类型的精度问题 精度问题及控制 12345678910double result = 1.0 - 0.9;//坏代码 // 方法一 double result = 1.0 - 0.9; NumberFormat nf = NumberFormat.getInstance();// 根据自己的需求格式化 String resultStr = nf.format(result); //方法二int resultInt = 10 - 9; double result = (double) resultInt / 100;//最终时候自己控制小数点 //方法三String result = new BigDecimal(\"1\").subtract(new BigDecimal(\"0.9\")) .toString(); 浮点类型的比较，不可直接用==来判断 12345double d1 = 0.1, d2 = 0.1; if (d1 == d2) &#123;&#125;// 坏代码 if (Double.compare(d1, d2) == 0) &#123;&#125;// 好代码 if (Double.doubleToLongBits(d1) == Double.doubleToLongBits(d2)) &#123;&#125;// 好代码 if (Double.valueOf(d1).equals(d2)) &#123;&#125;// 好代码，1.5以上 此外，如果是考虑在一定精度范围内，一般也可以判断两数差值绝对值是否在某一范围内 1234567bool Equal(double num1,double num2)&#123; if((num1-num2&gt;-0.0000001) &amp;&amp; (num1-num2&lt;0.0000001))&#123; return true; else return false; &#125;&#125; Notes:普通的计算最好使用BigDecimal类，这个类可以非常精确的计算出结果，而且你可以完全控制精度，不用额外其他操作，而且与基本类型转换都非常方便。 不调用函数求解平方根(sqrt()) 使用牛顿迭代法求解平方根 1234567891011121314/** * 求平方根 * @param d 待开方数 * @param precision 算术平方根的精度 * @return */ double sqrt(double d,double precision) &#123; double x1 = d/2, x2 =(x1 + d/x1)/2; while(Math.abs(x2 -x1)&gt;precision) &#123; x1 = x2; x2 =(x1 + d/x1)/2; &#125; return x1; &#125; 参考 http://www.javashuo.com/content/p-3097973.html http://liuqing-2010-07.iteye.com/blog/1396859 二分查找的方法另一种仅精确到整数的二分查找式的求平方根的算法 123456789101112131415161718public static int mySqrt(int x) &#123; long left = 1; long right = x; while (left + 1 &lt; right) &#123; long mid = left + (right - left)/2; if (mid * mid &lt; x) &#123; left = mid; &#125; else if (mid * mid &gt; x) &#123; right = mid; &#125; else &#123; return (int) mid; &#125; &#125; if (right * right &lt;= x) &#123; return (int) right; &#125; return (int) left;&#125; 参考 快速判断奇偶性判断奇数和偶数的方法，一般是除以2或者对2取模运算，结果为0则是偶数反之则是奇数。在计算机内部数都是用二进制表示的，奇数最低位上一定是1，偶数为0。基于这个特点可以利用按位与运算进行奇偶数判断。 123boolean isOdd(long l) &#123; return ((l &amp; 0x01) == 1); &#125; 参考 double类型的精度问题 《剑指offer》P151 经典算法的Java实现","tags":[{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"校招","slug":"校招","permalink":"http://yoursite.com/tags/校招/"},{"name":"面试","slug":"面试","permalink":"http://yoursite.com/tags/面试/"},{"name":"Coding","slug":"Coding","permalink":"http://yoursite.com/tags/Coding/"},{"name":"剑指offer","slug":"剑指offer","permalink":"http://yoursite.com/tags/剑指offer/"}]},{"title":"栈与队列小记","date":"2017-07-22T07:39:22.000Z","path":"2017/07/22/stack-queue/","text":"主要记录一些关于栈、队列的变体和特殊操作的算法题 队列Queue 队列允许重复元素，因为队列的基本特征是保持元素的插入顺序，重复的元素在队列中被认为是不同的元素，任意两个元素的顺序是不一样的。 Java接口 接口 包 实现 备注 Deque java.util LinkedList double ended queue，允许两端增删元素 BlockingDeque java.util.concurrent - 同BlockingQueue BlockingQueue java.util.concurrent - BlockingQueue只有在队列不空时才查找元素，只有在队列有空余空间时才插入元素、此外还支持延时插入删除操作 TransferQueue java.util.concurrent - 特殊的BlockingQueue，会等候其他的线程在队列中查询元素 参考 操作常用操作 继承了Collection的核心操作比如add(), contains(), remove(), clear(), isEmpty() 对于队列而言，在头部的操作更快比如offer()和remove()而对中间元素的操作就相对较慢比如：contains(e)和remove(e) 单元素操作: add(e),contains(e), iterator(), clear(), isEmpty(), size() 和 toArray(). 批量操作: addAll(), containsAll(), removeAll() 和 retainAll(). 基本操作 尾部添加元素 头部删除元素 从头部取元素（但不删除） 按操作失败时输出划分 其中插入操作，针对限制容量的一些实现会有所不同，包括ArrayBlockingQueue, LinkedBlockingQueue 和 LinkedBlockingDeque classes 实现 常规 LinkedList:List+Deque，适用于需要快速增删两端的元素，以及通过下标访问元素 PriorityQueue::通过自然顺序或者根据Comparator的规则对元素排序，适用于需要对元素自然排序以及快速在尾端插入元素或在头部删除元素 ArrayDeque:Deque的简单实现，当只需要双端队列的特性而不需要列表的特性时 Notes:对上述实现，如果想使用同步，则使用如下代码1List list = Collections.synchronizedList(new LinkedList&lt;&gt;()); 多线程 ArrayBlockingQueue:用数组实现的blocking queue,容量有限的简单的blocking queue实现 PriorityBlockingQueue:PriorityQueue+BlockingQueue DelayQueue:基于时间调度的blocking queue,需要实现Delayed接口，意味着只有当一个元素的delay到期了才能被从队列头部删除 栈与队列的转换用两个栈实现队列的基本操作123456789101112131415161718192021222324252627class MyQueue&#123; Stack&lt;Integer&gt; in=new Stack&lt;&gt;(); Stack&lt;Integer&gt; out=new Stack&lt;&gt;(); public void enQueue(int e)&#123; in.push(e); &#125; public int deQueue()&#123; if(!out.isEmpty())&#123; return out.pop(); &#125; else&#123; if(in.isEmpty())&#123; throw new RuntimeException(\"empty\"); &#125; while(!in.isEmpty())&#123; out.push(in.pop()); &#125; return out.pop(); &#125; &#125; public boolean isEmpty()&#123; return in.isEmpty()&amp;&amp;out.isEmpty(); &#125;&#125; 附：练习链接 特殊的栈O(1)时间取得最大/最小值的栈以最小值为例 123456789101112131415161718192021222324252627class MyStack&#123; Stack&lt;Integer&gt; stack1=new Stack&lt;&gt;(); //实现栈的常规功能 Stack&lt;Integer&gt; stack2=new Stack&lt;&gt;(); //辅助栈，栈顶保存当前最小元素 public void push(int node) &#123; stack1.push(node); if(!stack2.isEmpty())&#123; stack2.push((min()&lt;=node?min():node)); &#125; else&#123; stack2.push(node); &#125; &#125; public void pop() &#123; stack1.pop(); stack2.pop(); &#125; public int top() &#123; return stack1.peek(); &#125; public int min() &#123; return stack2.peek(); &#125;&#125; 附：练习链接 拓展应用股票的最大利润附：练习链接一种解法是，先用一个能快速获取最大值能力的栈倒着获取一遍所有的当前元素的最大元素，然后顺序遍历，不断的加入元素到一个TreeSet并从栈中弹出元素，这样一来，set里保留的就是前面部分的最小值，而stack保留剩余部分的最大值，所以，两相作差，我们可以从中取出最大的差值即最大利润，当然针对这个问题这个方法并不好，时间上对于大型数组会超时，而且还用了额外空间；比较好的方法应该使用动态规划 使用特殊栈与TreeSet的解法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import java.util.Stack;import java.util.TreeSet;/**剑指offer第二版面试题63：股票的最大利润*/public class MaxStock &#123; public static void main(String[] args) &#123; int[] prices=&#123;7, 1, 5, 3, 6, 4&#125;; System.out.println(maxProfit(prices)); int[] prices2=&#123;9,11,8,5,7,12,16,14&#125;; System.out.println(maxProfit(prices2)); int[] prices3=&#123;7, 6, 4, 3, 1&#125;; System.out.println(maxProfit(prices3)); &#125; //基本思路：用两个特殊的数据结构，先用一个能快速得到最大值的栈逆序遍历数据得到各序列当前最大值，然后逐个遍历数组， //左侧添加到一个TreeSet中保留当前最小值，右侧则不停的出栈，表示剩余的最大值，两项做差表示当前的利润，用一个max变量比较保存最大利润 public static int maxProfit(int[] prices) &#123; if(prices==null || prices.length==0)&#123; return 0; &#125; //use stack with max to reverse collect the current max prices MaxStack stack=new MaxStack(); for(int i=prices.length-1;i&gt;0;i--)&#123; stack.push(prices[i]); &#125; TreeSet&lt;Integer&gt; set=new TreeSet&lt;&gt;(); int maxProfit=0; for(int i=0;i&lt;prices.length-1;i++)&#123; set.add(prices[i]); int maxLeft=stack.max(); stack.pop(); int diff=maxLeft-set.first(); maxProfit=maxProfit&gt;diff?maxProfit:diff; &#125; return maxProfit; &#125;&#125;class MaxStack&#123; Stack&lt;Integer&gt; stack1=new Stack&lt;&gt;(); //实现栈的常规功能 Stack&lt;Integer&gt; stack2=new Stack&lt;&gt;(); //辅助栈，栈顶保存当前最小元素 public void push(int node) &#123; stack1.push(node); if(!stack2.isEmpty())&#123; stack2.push((max()&gt;=node?max():node)); &#125; else&#123; stack2.push(node); &#125; &#125; public void pop() &#123; stack1.pop(); stack2.pop(); &#125; public int top() &#123; return stack1.peek(); &#125; public int max() &#123; return stack2.peek(); &#125;&#125; 使用动态规划的解法 1234567891011121314151617public int maxProfit(int[] prices) &#123; if(prices.length&lt;=0)&#123; return 0; &#125; int soFarMin=prices[0]; int maxDiff=0; for(int i=1;i&lt;prices.length;i++)&#123; if(prices[i]&gt;soFarMin)&#123; maxDiff=maxDiff&gt;(prices[i]-soFarMin)?maxDiff:(prices[i]-soFarMin); &#125; else&#123; soFarMin=prices[i]; &#125; &#125; return maxDiff;&#125; 特殊的队列","tags":[{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"algorithm","slug":"algorithm","permalink":"http://yoursite.com/tags/algorithm/"},{"name":"校招","slug":"校招","permalink":"http://yoursite.com/tags/校招/"},{"name":"面试","slug":"面试","permalink":"http://yoursite.com/tags/面试/"},{"name":"Coding","slug":"Coding","permalink":"http://yoursite.com/tags/Coding/"}]},{"title":"方差、偏差、噪声和期望泛化误差小记","date":"2017-07-20T13:52:17.000Z","path":"2017/07/20/bias-variance/","text":"概念辨析一般我们希望最小化的模型的期望泛化误差可以分解为偏差、方差、噪声的加和，比较宽泛的表示为$$期望泛化误差=偏差+方差+噪声$$，引用《数据挖掘导论》上的一幅比较形象的类比为迫击炮的图来看看 名词 定义 理解 类比 偏差 算法的期望预测与真实结果的偏离程度 学习算法本身的拟合能力 炮弹落地点的平均位置距离到目标点的距离，受到迫击炮的角度影响 方差 同样大小的训练集的变动所导致的学习性能的变化 数据扰动所造成的影响 任意一次炮击落地点与炮弹落地点的平均位置之间的差距，受到每次对迫击炮的施加的力度影响 噪声 （给定）样本数据的标记与真实结果的差别（自身的错误） 学习问题本身的难度（期望泛化误差的下界） 目标本身的不稳定性，比如在一定范围内活动的目标 再附一张经典的打靶图 一般而言，噪声不可避免，属于数据集以及所需解决问题本身的限制；所以我们只能从偏差和方差的角度来尽可能的减小期望泛化误差的值，而偏差和方差本身存在冲突，表现为：给定一个学习任务，假定我们能控制学习算法的训练程度，则在训练不足时，学习器的拟和能力不强，训练数据的扰动不足以使学习器产生显著变化，此时偏差主导了泛化错误率；而随着训练程度的加深，学习器的拟合能力也逐渐加强，训练数据发生的扰动渐渐能被学习器学到，方差逐渐主导了泛化错误率，在训练程度充足后，学习器的拟合能力已经非常强了，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学到，则将发生过拟合 训练过程的方差-偏差矛盾对此理解bagging、boosting;对于bagging而言，通过不同训练子集来训练基分类器，这样首先就有较大的数据扰动，然后由于针对每一个基分类器都是“尽力地”去拟合各子训练集的（比如随机森林不剪枝生成完整的决策树的做法），所以对每一个子训练集而言，对应学得的基分类器拟合程度相当高，甚至可以认为对子训练集都是过拟合了的，所以针对bagging而言它的起点就是一个偏差小、方差大的集成，而他接下来的目的就是为了降低方差，减小由数据扰动带来的影响；而反观boosting，他的每一个基分类器是比较简单的弱分类器，而数据集其实一直都是整个样本全集，只是每一轮迭代修改了部分样本的权重（使整体的样本分布变化），所以初始的基分类器的拟合效果自然不好，也就是起始的时候偏差大、方差相对小，而接下来每一个学习器都是针对前一个学习器的错误进行修正，也就是修正预测结果与真实结果的偏离，所以目的是在减小偏差； 交叉验证与偏差、方差对应到交叉验证上来分析的话，首先我们知道每组用于训练的样本大小为$N\\frac{k-1}{k}$,而对应的每组测试集大小为$\\frac{N}{k}$如果我们选的k值比较小，比如极端情况下令$k=2$，此时共进行两次测试，每次都是取一半样本进行训练模型，一半样本用来测试，这样一来的话可以看到用于训练的样本数相对就比较少了，可能造成的结果是模型并不能学到所有样本的真实规律，也就是产生了欠拟和的问题，此时偏差应该主导了主要的期望泛化误差；另一面假设另一个极端我们令$k=N$，此时每一次验证的训练样本数为$\\frac{N-1}{N}$，而测试集大小为$\\frac{1}{N}$,此时用来训练的样本是足够了，而且基本接近于用所有样本来训练，所以模型的拟合效果应该是过犹不及的，所以此时偏差应该是比较小的，但是可能每一组训练出来的模型反而学到了样本中的一些噪声数据，也就是过拟合了该组样本，而测试集却又太少，所以测试集上的结果可能不够稳定准确，因此方差就比较大了；但是回过头来看我觉得cv里的偏差、方差和k值的选择似乎不能单纯的说大、小导致两者的就绝对的好或者差，譬如知乎那里大象的解答首先把问题分开来还讲的挺好的，包括cv里的那个式子，但是在cv部分的偏差和方差我感觉还是有点疑惑 再次阅读大象的解答，似乎有点收获，主要观点在于cv中的偏差和方差与bias-variance分解中的偏差方差是不同的两种概念；首先强调下两者的差异： bias-variance分解主要针对预测值与真实值之间的误差，EPE（期望预测误差）是由随机误差，预测模型与真实模型的偏差以及预测模型本身的变异三部分组成的。(而这部分是不涉及模型选择的，因为是已经得到模型以后来计算模型与真实结果之间的差异的) Cross Validation是为了得到预测误差的估计值，bias是CV统计量的期望与预测误差的距离，variance是CV统计量自己的方差。 Notes:对比一下上述叙述可以发现两个概念里的偏差和方差针对的对象是完全不一样的，而且目的也不一样 cv的作用是用来进行模型选择的，因为我们不能直接拿测试集来选择模型，所以需要用其它的方法来作为测试误差的估计值，用来做这种式的方法有很多，比如基于Optimism的Cp, AIC,基于贝叶斯方法的BIC，还有Vapnik-Chervonenkis维度, 验证误差等，验证误差是其中的一种方法，因为受到数据量的限制，所以我们采用cv来进行预估 因此我们使用Cross Validation的目的是获得预测误差的无偏估计量CV，从而可以用来选择一个最优的\\theta^*,使得CV最小,而k折交叉验证中的k值又是与cv中的偏差、方差相关联的，一般而言，当K较大时，m较小，模型建立在较大的N-m上，因此CV与Testing Error的差距较小，所以说CV对Testing Error估计的Bias较小。同时每个Subsets重合的部分较大，相关性较高，如果在大量不同样本中进行模拟，CV统计量本身的变异较大，所以说Variance高。反之亦然。 用岭回归模型举个例子，如果K=2，K=10，K=N最终都选择了相同的参数$\\lambda$，那么训练出来的模型也都一样，那么模型的实际testing error，bias，variance也全都一样了。然而在模型选择时，不同K所对应的CV统计量的期望和方差是存在变化关系的。 参考 周志华《机器学习》2.2以及2.5 《数据挖掘导论》5.6.3 知乎问答 图片来自上述图书或者网络","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"方差","slug":"方差","permalink":"http://yoursite.com/tags/方差/"},{"name":"偏差","slug":"偏差","permalink":"http://yoursite.com/tags/偏差/"}]},{"title":"统计学习方法笔记(七) —— 支持向量机","date":"2017-07-12T09:11:48.000Z","path":"2017/07/12/svm/","text":"概览定义 支持向量机是一种二类分类模型 基本模型：定义在特征空间上的间隔最大的线性分类器 +核函数$\\to$ 非线性分类器 学习策略间隔最大化：最大化实例点到分类超平面的最小距离，或者说最大化数据集到分离超平面的距离 对偶问题：求解凸二次规划问题 等价损失函数：正则化的合页(hinge)损失函数的最小化问题 学习算法求解凸二次规划的最优化算法：常用SMO(序列最小最优化问题) 模型分类 训练数据 通过 学习 线性可分 硬间隔最大化 线性可分支持向量机(硬间隔支持向量机) 近似线性可分 软间隔最大化 线性支持向量机(软间隔支持向量机) 线性不可分 软间隔最大化+核技巧 非线性支持向量机 线性可分支持向量机概述 输入都由输入空间转换到特征空间，支持向量机的学习是在特征空间进行的，而学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类 当训练数据集线性可分时，感知机利用误分类最小的策略，求得无穷多的分离超平面；而线性可分支持向量机利用间隔最大化求最优分离超平面，此时，解是唯一的 定义：给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为:$$w^*\\cdot x+b^*=0$$，以及相应的分类决策函数$$f(x)=sign(w^*\\cdot x+b^*)$$，称为线性可分支持向量机。 函数间隔&amp;几何间隔假设给定一个特征空间上的训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace, x_i\\in R^n,y_i \\in \\lbrace -1,+1 \\rbrace$ 函数间隔 超平面$(w,b)$关于训练数据集$T$的样本点$(x_i,y_i)$的函数间隔：$$\\widehat{\\gamma_i}=y_i(w\\cdot x_i+b)$$ 表示分类预测的： 确信程度$\\to |w\\cdot x+b|$:一个点距离分离超平面的远近 正确性$\\to w\\cdot x+b$的符号与类标记$y$的符号是否一致 超平面$(w,b)$关于训练数据集$T$的函数间隔为：$$\\widehat{\\gamma}=min_{i=1,…,N}\\widehat{\\gamma_i}$$,但是选择分离超平面时，仅有函数间隔是不够的，因为$w,b$可以成比例变化使得函数间隔为无穷大而不具备可比性，所以需要对$w$加约束，如规范化，令$||w||=1$，于是得到几何间隔 几何间隔 超平面$(w,b)$关于训练数据集$T$的样本点$(x_i,y_i)$的几何间隔：$$\\gamma_i=y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})$$ 超平面$(w,b)$关于训练数据集$T$的几何间隔为：$$\\gamma=min_{i=1,…,N}\\gamma_i$$，超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔一般是实例点到超平面的带符号的距离。 间隔最大化 基本想法：求解能够正确划分训练数据集并且几何间隔最大的分离超平面，即以充分大的确信度对训练数据进行分类 Notes:原始问题：求一个几何间隔最大的分离超平面。 数学表达为一个约束最优化问题$$max_{w,b} \\gamma$$$$ s.t. y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||}) \\geq \\gamma i=1,2,…,N$$，其中$\\gamma$即表示超平面关于数据集的几何间隔，是超平面关于数据集中的样本点的几何间隔$\\gamma_i$的最小值，因此有上述约束条件，而我们的目的是得到一个使该值最大的超平面 由几何间隔与函数间隔之间的转化关系$\\gamma_i = \\frac{\\widehat{\\gamma_i}}{||w||}, \\gamma = \\frac{\\widehat{\\gamma}}{||w||}$，上述问题可以进一步转化为$$max_{w,b} \\frac{\\widehat{\\gamma}}{||w||}$$$$ s.t. y_i(w\\cdot x_i+b) \\geq \\gamma i=1,2,…,N$$ 由于函数间隔$\\widehat{\\gamma}$并不影响最优化问题的解（将$w,b$按比例改变，函数间隔也随之等比变化），取$\\widehat{\\gamma}=1$,此外由于在该最优化问题中，最大化$\\frac{1}{||w||}$和最小化$\\frac{1}{2}||w||^2$等价，所以上述问题进一步转化为$$min_{w,b} \\frac{1}{2}{||w||^2}$$$$ s.t. y_i(w\\cdot x_i+b) - 1\\geq 0 i=1,2,…,N$$ Notes: 原始问题到这里就结束了，直接使用凸二次规划求解出$w,b$的值就可以得到分离超平面和最终的决策函数，这就是最大间隔法 书中例题7.1的凸二次优化的Python解法，参照凸优化 · 如何在 Python 中利用 CVXOPT 求解二次规划问题,针对该题可见我的github 算法-最大间隔法(线性可分支持向量机学习算法) 输入：线性可分训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace, x_i\\in R^n,y_i \\in \\lbrace -1,+1 \\rbrace, i=1,2,…,N$输出：最大间隔分离超平面和分类决策函数 (1)构造并求解约束最优化问题$$min_{w,b} \\frac{1}{2}{||w||^2}$$$$ s.t. y_i(w\\cdot x_i+b) - 1\\geq 0 i=1,2,…,N$$，求得最优解$w^*,b^*$ (2)由此可得分离超平面$$w^*\\cdot x+b^*=0$$,以及对应的分类决策函数$$f(x)=sign(w^*\\cdot x+b^*)$$ Notes:线性可分训练数据集的最大间隔分离超平面是存在且唯一的 支持向量与间隔边界在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量(使$y_i(w\\cdot x_i+b)-1=0$成立的点)如图所示，超平面$H_1,H_2$上的点即为支持向量，$H_1$与$H_2$之间的距离即为间隔，依赖于分离超平面的法向量$w=\\frac{2}{||w||}$，$H_1,H_2$称为间隔边界；Notes:在决定分离超平面时只有支持向量起作用，其他实例点并不起作用 学习的对偶算法承接前面的原始问题，引入拉格朗日对偶性 对偶问题往往更容易求解 自然引入核函数$\\to$推广到非线性分类问题 推导过程 对原问题引入拉格朗日乘子$\\alpha_i \\geq 0$,构建拉格朗日函数，得到$$L(w,b,\\alpha)=\\frac{1}{2}{||w||^2} - \\sum_{i=1}^{N}\\alpha_iy_i(w\\cdot x_i+b)+ \\sum_{i=1}^{N}\\alpha_i$$Notes:个人理解是将原始的约束条件中的不等式、等式以某种形式添加到目标函数中从而转化为新的目标函数求解 这样原始问题就是$$min_{w,b} max_{\\alpha_i \\geq 0}L(w, b, \\alpha)$$,根据拉格朗日对偶性可以转换为广义的拉格朗日的极大极小问题$$max_{\\alpha_i \\geq 0} min_{w,b}L(w, b, \\alpha)$$ 求解$min_{w,b}L(w, b, \\alpha)$,即分别对$w,b$求导并令其值为0，得到$$w = \\sum_{i=1}^{N}\\alpha_iy_ix_i$$$$\\sum_{i=1}^N \\alpha_iy_i=0$$ 代入原式得$$min_{w,b}L(w,b,\\alpha)=-\\frac{1}{2} \\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)+ \\sum_{i=1}^{N}\\alpha_i$$,所以求该式对$\\alpha$的极大，即求约束最优化问题$$max_{\\alpha} -\\frac{1}{2} \\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)+ \\sum_{i=1}^{N}\\alpha_i$$$$s.t. \\sum_{i=1}^N \\alpha_iy_i=0, \\alpha_i \\geq 0, i=1,2,…,N$$ 对偶问题即为$$min_{\\alpha} \\frac{1}{2} \\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)- \\sum_{i=1}^{N}\\alpha_i$$$$s.t. \\sum_{i=1}^N \\alpha_iy_i=0, \\alpha_i \\geq 0, i=1,2,…,N$$ 使用SMO算法可以求得最优的$\\alpha_j^*&gt;0 \\to w^*=\\sum_{i=1}^{N}\\alpha_i^*y_ix_i \\to b^*=y_j - \\sum_{i=1}^{N}\\alpha_i^*y_i(x_i\\cdot x_j) \\to$分离超平面$$w^*\\cdot x+b^*=0$$,对应的决策函数为$$f(x)=sign(w^*\\cdot x+b^*)=sign(\\sum_{i=1}^{N}\\alpha_i^*y_i(x\\cdot x_i)+b^*)$$ Notes: 分类决策函数只依赖于输入$x$与训练样本输入的内积 至此为线性可分支持向量机的对偶学习算法 上述结果体现出支持向量机的一个重要性质：训练完成后，大部分训练样本都不需要保留，最终模型仅与支持向量有关 SMO算法：分解为子问题求解 求解两个变量二次规划的解析方法 选择变量的启发式方法 为什么说对偶算法更易求解？效率更高 以前新来的要分类的样本首先根据$w$和$b$做一次线性运算，然后看求的结果是大于0还是小于0,来判断正例还是负例。现在有了$\\alpha_i$，我们不需要求出$w$，只需将新来的样本和训练数据中的所有样本做内积和即可。那有人会说，与前面所有的样本都做运算是不是太耗时了？其实不然，我们从KKT条件中得到，只有支持向量的$\\alpha_i&gt;0$，其他情况$\\alpha_i=0$。因此，我们只需求新来的样本和支持向量的内积，然后运算即可。这种写法为下面要提到的核函数（kernel）做了很好的铺垫。 算法——对偶学习算法 输入：线性可分训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace, x_i\\in R^n,y_i \\in \\lbrace -1,+1 \\rbrace, i=1,2,…,N$输出：分离超平面和分类决策函数 (1)构造并求解约束最优化问题$$min_{\\alpha} \\frac{1}{2} \\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)- \\sum_{i=1}^{N}\\alpha_i$$$$s.t. \\sum_{i=1}^{N}\\alpha_iy_i = 0 \\alpha_i \\geq 0, i=1,2,…,N$$求得最优解$\\alpha^* = (\\alpha_1^* … \\alpha_n^*)^T$ (2)计算$$w^* = \\sum_{i=1}^{N}\\alpha_i^*y_ix_i$$,并选择$\\alpha^*$的一个正分量$\\alpha_j^*&gt;0$,计算$$b^* = y_j - \\sum_{i=1}^{N}\\alpha_i^*y_i(x_i\\cdot x_j)$$ (3)求得分离超平面$$w^*\\cdot x+b^*=0$$,对应的决策函数为$$f(x)=sign(w^*\\cdot x+b^*)=sign(\\sum_{i=1}^{N}\\alpha_i^*y_i(x_i\\cdot x_j)+b^*)$$ Notes:$w^*,b^*$只依赖于训练数据中对应于$\\alpha_i^*&gt;0$的样本点$(x_i,y_i)$，这些实例点$x_i$即为支持向量 线性支持向量机定义训练样本线性不可分时的线性支持向量机，此处的线性不可分指的是近似线性可分，即训练数据集有一些特异点，去除掉这些特异点后剩下大部分样本点组成的集合是线性可分的 原始问题软间隔最大化问题：在硬间隔最大化的目标函数中添加松弛变量和惩罚参数，在约束条件中添加松弛变量，即$$min_{w,b,\\xi} \\frac{1}{2}{||w||^2}+C\\sum_{i=1}^{N}\\xi_i$$$$ s.t. y_i(w\\cdot x_i+b) \\geq 1 - \\xi_i; \\xi_i \\geq 0 i=1,2,…,N$$ Notes: 使间隔尽量大$\\to \\frac{1}{2}||w||^2$尽量小 误分类点个数尽量少，即松弛变量$\\xi_i$尽量小,$C$为调和两者的系数 对偶问题类似线性可分支持向量机中做法，引入拉格朗日乘子并构建拉格朗日函数，利用拉格朗日对偶性，问题转化为求解对偶问题$$min_{\\alpha} \\frac{1}{2} \\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)- \\sum_{i=1}^{N}\\alpha_i$$$$s.t. \\sum_{i=1}^{N}\\alpha_iy_i = 0; 0 \\leq \\alpha_i \\leq C, i=1,2,…,N$$ _Notes:可以对比线性可分支持向量机，主要区别在于约束条件$\\alpha_i$多了上界$C$_ 合页损失函数软间隔/线性支持向量机的原始问题可以等价于添加了正则化项的合页损失函数，即最小化以下目标函数$$min_{w,b} \\sum_{i=1}^{N}[1-y_i(w\\cdot x_i+b)]_++\\lambda ||w||^2$$ 第一项为合页损失函数$L(y(w\\cdot x+b))=[1-y_i(w\\cdot x_i+b)]_+$,一般对于函数$[z]_+$有：$$[z]_+=z if z&gt;0$$$$[z]_+=0 if z\\leq 0$$所以原式表明当样本点$x_i,y_i$被正确分类且函数间隔(确信度)$y_i(w\\cdot x_i+b)$大于1时损失为0，否则损失是$1-y_i(w\\cdot x_i+b)$ 第二项为正则化项，是系数为$\\lambda$的$w$的$L_2$范数 线性支持向量机的原始最优化问题与上述目标函数的最优化问题是等价的 那么为什么要使用合页损失函数？如图所示为常用的一些损失函数，可以看到，各个图中损失函数的曲线基本位于0-1损失函数的上方，所以可以作为0-1损失函数的上界； 由于0-1损失函数不是连续可导的，直接优化由其构成的目标损失函数比较困难，所以对于svm而言，可以认为是在优化由0-1损失函数的上界(合页损失函数)构成的目标函数，又称为代理损失函数 合页损失函数对学习有更高的要求 附：常用替代损失函数，通常具有较好的数学性质，比如凸的连续函数且是0/1损失函数的上界 hinge损失$$l_{hinge}(z)=max(0,1-z)$$ 指数损失$$l_{exp}(z)=exp(-z)$$ 对率损失$$l_{log}(z)=log(1+exp(-z))$$ 合页损失函数这里也看得到与前面所述的一些模型的讲解方式的差异，前面大部分模型的策略部分的目标函数是在做一个损失函数的最小化问题求解，而SVM一开始就假设特征空间线性可分，所以这种前提下是不存在损失的，因此目标是找到一个超平面使得将正反例最大区分度的分割开来，于是目标函数就成了求解最大间隔的问题，包括引入核函数也使得本身可能线性并不可分的样本能够在高位空间被划分开来，但是这样存在一点问题就是构成的模型对于噪声点比较敏感，因为理论上而言在无穷维度一定能将样本划分开来，所以假设样本本身存在一些噪声点，这个时候的模型就会出现过拟合的问题，于是就引入了松弛变量和软间隔理论（这里也可以从经验风险-结构风险的角度来理解）；这也是另一种讲解顺序，在周志华的《机器学习》一书以及博客jerrylead里都是按这种顺序来讲解的 非线性支持向量机非线性分类问题 非线性分类问题是指通过利用非线性模型才能很好的进行分类的问题 用线性分类方法求解非线性分类问题分为两步： 首先使用一个变换将原空间的数据映射到新空间； 在新空间里用线性分类学习方法从训练数据中学习分类模型；核技巧就属于这样的方法核技巧 核技巧应用到支持向量机，其基本想法是通过一个非线性变换将输入空间（欧式空间或离散集合）对应于一个特征空间（希尔伯特空间），使得在输入空间中的超曲面模型对应于特征空间中的超平面模型(支持向量机)；这样分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成了 核技巧：在核函数给定的条件下，可利用解线性分类问题的方法求解非线性分类问题的支持向量机；学习是隐式地在特征空间进行的，不需要显式地定义特征空间和映射函数 实际应用中，常依赖于领域知识直接选择核函数，其有效性通过实验验证 Notes:若原始空间是有限维的（属性数有限），则一定存在一个高维特征空间使样本可分 核函数 正定核通常所述的核函数就是正定核函数，证明一个函数为正定核函数的充要条件为证明该函数对应的Gram矩阵为半正定矩阵 常用核函数 多项式核函数–p次多项式分类器 高斯核函数–高斯径向基函数分类器 字符串核函数–定义在离散数据的集合上(字符串集合)–余弦相似度 图片来源 求解问题 选取适当的核函数$K(x,z)$和适当的参数$C$,构造并求解最优化问题：$$min_{\\alpha} \\frac{1}{2} \\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i\\alpha_jy_iy_jK(x_i,x_j)- \\sum_{i=1}^{N}\\alpha_i$$$$s.t. \\sum_{i=1}^{N}\\alpha_iy_i = 0$$$$0 \\leq \\alpha_i \\leq C i=1,2,…,N$$ 构造决策函数$$f(x)=sign(\\sum_{i=1}^{N}\\alpha_i^*y_iK(x\\cdot x_i)+b^*)$$ Notes:可以对比软间隔最大化的对偶学习问题，区别在于用核函数$K(x,z)$代替了原始目标函数中的内积 SMO算法 SMO算法是支持向量机学习的一种快速算法，其特点是不断的将原二次规划问题分解为只有两个变量的二次规划问题，并对子问题进行解析求解，直到所有变量满足KKT条件为止，这样通过启发式的方法得到原二次规划问题的最优解，因为子问题有解析解，所以每次计算子问题都很快，虽然计算子问题次数很多，但在总体上还是高效的 选取$\\alpha_i,\\alpha_j$的方法： 先选取违背KKT条件程度最大的变量作为第一个变量，然后启发式的选择与第一个变量对应样本间间隔最大的变量作为第二个变量 SVM vs.Logistic Regression 同：优化目标相近，通常情形下性能也相当 异： LR的优势在于其输出具有自然的概率意义，给出预测标记的同时也给出了概率，而SVM要想得到概率输出需特殊处理 LR能直接用于多分类任务，而SVM则需要进行推广 SVM的解具有稀疏性（仅依赖于支持向量），LR对应的对率损失则是光滑的单调递减函数，因此LR的解依赖于更多的训练样本，其预测开销更大参考《机器学习》 支持向量回归TODO 其他资料关于SVM的讲解有不少优秀资料，小结如下 从LR过渡到SVM的讲解方式 串讲拉格朗日对偶性与SVM对偶算法的推导 周志华《机器学习》","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"SVM","slug":"SVM","permalink":"http://yoursite.com/tags/SVM/"}]},{"title":"最优化算法小记","date":"2017-07-11T06:14:02.000Z","path":"2017/07/11/optimization/","text":"最优化： 构造一个合适的目标函数（策略），使得这个目标函数取到极值的解就是你所要求的东西； 找到一个能让这个目标函数取到极值的方法（算法） 这里主要讨论几种常用的求解最优化问题的算法 梯度下降法（最速下降法）梯度 在微积分里面，对多元函数的参数求偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度;从几何意义上讲，就是函数变化增加最快的地方。 梯度下降法 梯度下降法是求解无约束最优化问题的一种常用的一阶优化方法，是一种迭代算法，每一步需要求解目标函数的梯度向量。 缘由 对于无约束优化问题$min_xf(x)$，其中$f(x)$为连续可微函数，若能构造一个序列$x^0,x^1,…$,满足$$f(x^{t+1})&lt;f(x^t),t=0,1,2,..$$，则不断执行该过程即可收敛到局部极小点； 而为了达到这一目的，根据泰勒展开式有$$f(x+\\Delta x)\\approx f(x)+\\Delta x^T\\nabla f(x)$$,而为了满足$f(x+\\Delta x)&lt;f(x)$，可选择$$\\Delta x=-\\gamma \\nabla f(x)$$即负梯度,其中步长（即学习率）$\\gamma$是一个小常数（每步的步长可以不同）； 对于步长，可通过一维检索确定，即$$f(x^{(k)}-\\gamma \\nabla f(x))=min_{\\gamma\\geq 0}f(x^{(k)}-\\gamma \\nabla f(x))$$,但是这种方法会增加额外的计算开销，不过如果固定步长，也有可能导致糟糕的收敛，所以需要根据实际情况选择处理方法。 算法 输入：目标函数$f(x)$,梯度函数$g(x)=\\nabla f(x)$，计算精度$\\varepsilon$ 输出：$f(x)$的极小点$x^*$ (1)取初始值$x^{(0)}\\in R^n$,置$k=0$ (2)计算$f(x^{(k)})$ (3)计算梯度$g_k=g(x^{(k)})$,当$||g_k||&lt;\\varepsilon$时，停止迭代，令极小点$x^*=x^{(k)}$,否则令负梯度$p_k=-g(x^{(k)})$,求步长$\\lambda_k$，使$$f(x^{(k)}+\\lambda_kp_k)=min_{\\lambda\\geq 0}f(x^{(k)}+\\lambda p_k)$$ (4)迭代更新，置$x^{(k+1)}=x^{(k)}+\\lambda_kp_k$,计算$f(x^{(k+1)})$,当$||f(x^{(k+1)})-f(x^{(k)})||&lt;\\varepsilon$或$||x^{(k+1)}-x^{(k)}||&lt;\\varepsilon$时，停止迭代，令$x^*=x^{(k)}$ (5)否则,置$k=k+1$,转第(3)步 Notes: 一般情况下，步长可以取一个较小的值，而不必每次都计算当前的最佳步长 另外，该算法中停止迭代的条件出现了两次，第一次是刚计算出梯度时就与计算精度进行比较，第二次是分别把目标函数值以及两次迭代的输入$x$的差值与计算精度进行了比较，但其他的一些参考资料似乎只是检查了梯度下降的距离与计算精度的关系，即$||f(x^{(k+1)})-f(x^{(k)})||&lt;\\varepsilon$，比如博客 特点 梯度下降法靠近极小值时速度减慢（所以不必每次迭代刻意减小步长） 对某些目标函数可能会“之字型”地下降 若目标函数$f(x)$满足一些条件，则通过选取适当的步长能确保通过梯度下降收敛到局部极小点，比如若$f(x)$满足L-Lipschitz条件（对任意$x$存在常数$L$使得$||\\nabla f(x)||\\leq L$成立），则将步长设置为$1/2L$可确保收敛到局部极小点； 当目标函数是凸函数时，局部极小点即对应着函数的全局最小点 调优 梯度下降算法的步长会影响其收敛，步长太大可能难以收敛到局部最优值，步长太小则会导致收敛速度较慢； 初始值不同可能导致梯度下降算法收敛到不同的局部最优解，所以一般可以采取多次训练的方法求解 由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化$$\\frac{x-\\overline{x}}{std(x)}$$从而加快收敛速度 分类批量梯度下降法 Batch Gradient Descent(BGD):在更新参数时使用所有的样本来进行更新 $$\\theta_i=\\theta_i-\\alpha\\sum_{j=0}^{m}(h_\\theta(x_0^{j}, x_1^{j}, …x_n^{j}) - y_j)x_i^{j}$$其中$\\theta_i$为需要估计的达到局部最优时的参数值，步长为$\\alpha$，一共$m$个样本$n$个特征,求梯度的时候用了所有样本的梯度数据 随机梯度下降法 Stochastic Gradient Descent(SGD):计算梯度时仅选取一个样本来求梯度，适用于训练数据集数据量较大的情形 $$\\theta_i=\\theta_i-\\alpha(h_\\theta(x_0^{j}, x_1^{j}, …x_n^{j}) - y_j)x_i^{j}$$ 小批量梯度下降法 Mini-batch Gradient Descent(MBGD):批量梯度下降法和随机梯度下降法的折衷，也就是对于$m$个样本，我们采用$x$个样本来迭代，$1&lt;x&lt;m$ $$\\theta_i=\\theta_i-\\alpha\\sum_{j=t}^{t+x-1}(h_\\theta(x_0^{j}, x_1^{j}, …x_n^{j}) - y_j)x_i^{j}$$ Notes: 目标函数收敛速度来说：随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。 对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。 小批量梯度下降法则是批量梯度下降法和随机梯度下降法的折衷 最小二乘法严格来讲，最小二乘法其实并不能归类到最优化算法这里面来，主要是在涉及线性回归模型的目标函数求解时，不少资料里都提到过可以用最小二乘法或者梯度下降法求解，所以放到这里进行比较讨论，具体的一些讨论可以参照参考资料里的知乎的一系列问题 牛顿法拟牛顿法小结 对于梯度下降法，当目标函数$f(x)$二阶可微时，可将一阶泰勒展开式$f(x+\\Delta x)\\approx f(x)+\\Delta x^T\\nabla f(x)$替换为更精确的二阶泰勒展开式，这样就得到了牛顿法，牛顿法是典型的二阶方法，其迭代轮数远小于梯度下降法；但另一方面由于牛顿法使用了二阶导数，所以其每轮迭代都涉及了海森矩阵的求逆，计算复杂度较高，尤其在高维问题中几乎不可行；若能以较低的计算代价寻找海森矩阵的近似逆矩阵，则可显著降低计算开销，这就是拟牛顿法 参考资料 刘建平Pinard的博客 知乎-最优化问题的简洁介绍是什么？ 知乎-最小二乘法和梯度下降法有哪些区别？ 李航《统计学习方法》 周志华《机器学习》 Andrew Ng 机器学习课程（Coursera） 梯度下降法维基百科","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"最优化问题","slug":"最优化问题","permalink":"http://yoursite.com/tags/最优化问题/"}]},{"title":"机器学习笔记之线性模型","date":"2017-07-09T07:09:53.000Z","path":"2017/07/09/linear-model/","text":"基本形式 线性模型试图学得一个通过属性的线性组合来进行预测的函数，即$$f(x)=w_1x_1+w_2x_2+…w_dx_d+b$$ 线性模型形式简单、易于建模，具有很好的可解释性 很多功能强大的非线性模型可在线性模型的基础上通过引入层级结构或高维映射而得 线性回归线性回归 模型线性回归试图学得$f(x_i)=wx_i+b$,使得$f(x_i)\\approx y_i$ 均方误差如何衡量$f(x)$与$y$之间的差别？均方误差是回归任务中最常用的性能度量，所以关键在于使均方误差最小化，即$$(w^*,b^*)=argmin_{(w,b)}\\sum_{i=1}^m(f(x_i)-y_i)^2$$$$(w^*,b^*)=argmin_{(w,b)}\\sum_{i=1}^m(y_i-wx_i-b)^2$$基于均方误差来进行模型求解的方法称为最小二乘法，在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小 参数估计线性回归模型的最小二乘参数估计，即求解$w,b$使$E_{(w,b)}=\\sum_{i=1}^m(y_i-wx_i-b)^2$最小化的过程 对w,b分别求导，得 $$\\frac{dE_{(w,b)}}{dw}=2\\left(w\\sum_{i=1}^mx_i^2-\\sum_{i=1}^m(y_i-b)x_i\\right)$$ $$\\frac{dE_{(w,b)}}{db}=2\\left(mb-\\sum_{i=1}^m(y_i-wx_i)\\right)$$ 令导数为0可求得 $$w=\\frac{\\sum_{i=1}^my_i(x_i-\\overline{x})}{\\sum_{i=1}^mx_i^2-\\frac{1}{m}(\\sum_{i=1}^mx_i)^2}$$ $$b=\\frac{1}{m}\\sum_{i=1}^m(y_i-wx_i)$$其中$\\overline{x}=\\frac{1}{m}\\sum_{i=1}^mx_i$为$x$的均值 广义线性模型思想线性模型可以有丰富的变化，对于原始的线性模型，我们希望其预测值逼近真实标记$y$，于是得到了线性回归模型，而如果我们令模型预测值逼近y的衍生物，这样一来就可以推广到广义的线性模型 考虑单调可微函数$g(\\cdot)$,令$$y=g^{-1}(w \\cdot x+b)$$,这样得到的模型称为广义线性模型，其中函数$g(\\cdot)$称为联系函数。 实质上是在求取输入空间到输出空间的非线性函数映射 例子——对数线性回归假设认为示例所对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标，即$$ln y=w \\cdot x+b$$即对数线性回归，实际上是在试图让$e^{w\\cdot x+b}$逼近$y$,这里的对数函数就起到了将线性回归模型的预测值与真实标记联系起来的作用 Notes:此外，Logistic Regression模型可以看作是广义线性模型在分类问题上的延伸","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"LR","slug":"LR","permalink":"http://yoursite.com/tags/LR/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"机器学习校招常考知识点小记","date":"2017-07-08T10:47:00.000Z","path":"2017/07/08/ml-job/","text":"概念&amp;原理 高频话题是 SVM、LR、决策树（决策森林）和聚类算法，要重点准备； 算法要从以下几个方面来掌握 产生背景 适用场合（数据规模，特征维度，是否有 Online 算法，离散/连续特征处理等角度）； 原理推导（最大间隔，软间隔，对偶）； 求解方法（随机梯度下降、拟牛顿法等优化算法）； 优缺点，相关改进； 和其他基本方法的对比； 不能停留在能看懂的程度，还要对知识进行结构化整理，比如撰写自己的 cheet sheet，面试是在有限时间内向面试官输出自己知识的过程，如果仅仅是在面试现场才开始调动知识、组织表达，总还是不如系统的梳理准备；从面试官的角度多问自己一些问题，通过查找资料总结出全面的解答，比如如何预防或克服过拟合。 机器学习是什么以及机器学习的分类 类比人的学习-人根据观察事物，积累经验，从而习得技能，机器学习就是计算机根据大量数据通过计算获得某种可以被增进的表现（譬如是某种特征规律啥的）。再形式化一点说机器学习就是通过数据，利用学习算法（learning algorithm）从假设集合（hypothesis set）中去挑选一个和实际目标函数最接近的假设的过程。 wiki 感知机是什么？PLA算法和POCKET算法是什么？ LDA的原理和推导；SVD、LDA 无监督和有监督算法的区别？ 多分类怎么处理？（参见《机器学习》P64,通常切分为多个二分类，最后再集成） OVO OVR MVM–&gt;ECOC(纠错输出码) 为什么会产生过拟合，有哪些方法可以预防或克服过拟合？ 什么情况下一定会发生过拟合？ 训练集和测试集分布不一致的时候 采用EM算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？ 有哪些聚类方法？ K-均值聚类算法、K-中心点聚类算法、CLARANS、 BIRCH、CLIQUE、DBSCAN等 聚类算法中的距离度量有哪些？ 如何进行实体识别？ 聚类和分类有什么区别？分类是事先知道类标的，而聚类事先不知道类标。 k-means与knn的区别 两种算法之间有多种差异，如下： KNN为分类(也可做回归)，K-means为聚类； KNN为监督学习，K-means为无监督学习； KNN的输入样本为带label的，K-means的输入样本不带label； KNN没有训练过程，K-means有训练过程； K的含义不同，KNN中表示选取距离最近的k的样本，而K-Means则表示设置K个聚类中心 两种算法之间有一点相似：都是计算近邻，一般都用欧氏距离。 http://nohup.cc/article/167/ L1和L2的区别 生成与判别模型 ROC-AUC 链接 为什么roc曲线不随正负样例比变化而变化，而类似的precision-recall曲线却会变化？ 可画出混淆矩阵根据不同值的定义(横纵坐标)，分别对比将负例扩大到K倍或者将正例减小到M倍前后参数是否发生变化推算出来(N=TN+FP,P=TP+FN) 深度学习和机器学习的区别 数据挖掘和人工智能的区别 测试集和训练集的区别 PCA 什么是模糊聚类，还有划分聚类，层次聚类等 http://blog.csdn.net/xiahouzuoxin/article/details/7748823 http://www.cnblogs.com/guolei/p/3899509.html 线性分类器与非线性分类器的区别及优劣； 特征比数据量还大时，选择什么样的分类器？ 对于维度很高的特征，你是选择线性还是非线性分类器？ 对于维度极低的特征，你是选择线性还是非线性分类器？ 如何解决过拟合问题？ L1和L2正则的区别，如何选择L1和L2正则？ 线性回归的梯度下降和牛顿法求解公式的推导 如何处理类别不平衡问题？（参见《机器学习》P66） 欠采样–&gt;EasyEnsemble 过采样–&gt;SMOTRE 阈值移动/代价敏感学习–&gt;原始数据嵌入“负正比”，具体的比如在xgboost中可以调整scale_pos_weight参数为数据集的负正比 常见的最优化方法 牛顿法的原理 L1正则化的优化 0/1损失函数的常用替代损失函数（《机器学习》P130） hinge损失（合页损失）–&gt; SVM 指数损失 –&gt; AdaBoost 对率损失 –&gt; LR、最大熵 降维的方法有哪些 http://blog.csdn.net/bryan__/article/details/52488316 常见基于核的机器学习算法有哪些？基于核的算法把输入数据映射到一个高阶的向量空间， 在这些高阶向量空间里， 有些分类或者回归问题能够更容易的解决。 SVM-支持向量机 LDA-线性判别分析 RBF-径向基函数来源 深度学习的深有何意义？为什么倾向于更深层的网络而不是更胖的网络？ https://www.zhihu.com/question/25456959 http://blog.csdn.net/u010751535/article/details/53512499 模型简介 原理、推导、特性、优缺点、调参 统计学习的核心步骤：模型、策略、算法，你应当对logistic、SVM、决策树、KNN及各种聚类方法有深刻的理解。能够随手写出这些算法的核心递归步的伪代码以及他们优化的函数表达式和对偶问题形式。 SVM的原理、推导、特性，SVM里面的核，SVM非线性分类，核函数的作用；SVM详细过程，支持向量，几何间隔概念，拉格朗日函数如何求取超平面，非线性分类；写写SVM的优化形式，用拉格朗日公式推导SVM kernel变换；SVM：中文分词；RBF核与高斯核的区别（没区别，高斯核就是rbf核）;SVM的支持向量的数学表示 SVM中为什么要选取最大间隔分类器，从数学的角度说明： 几何间隔与样本的误分次数间存在关系$误分次数\\leq (\\frac{2R}{\\sigma})^2$,其中$\\sigma$为样本集合到分类面的间隔，$R$为所有样本中向量长度最长的值(代表样本分布有多广)，所以上式也就表明误分次数的上界由几何间隔决定 http://www.blogjava.net/zhenandaci/archive/2009/02/13/254519.html LR 的推导，特性？；为什么可以使用logistic回归;logistic regression为什么使用sigmoid函数？;linear regression 如何处理离散值的情况，如特征为：男1，女0 决策树的特性？;决策树分裂 用EM算法推导解释 Kmeans；K-means起始点；K-means的具体流程；如何优化K-means 解释密度聚类算法 解释贝叶斯公式和朴素贝叶斯分类；贝叶斯分类器的优化和特殊情况的处理 朴素贝叶斯核心思想利用先验概率得到后验概率，并且最终由期望风险最小化得出后验概率最大化，从而输出让后验概率最大化的值（具体概率与先验概率由加入拉普拉斯平滑的极大似然估计而成的贝叶斯估计得到），特征必须相互独立。 KNN（分类与回归） CART（回归树用平方误差最小化准则，分类树用基尼指数最小化准则） GBDT（利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值，拟合一个回归树） 随机森林（Bagging+CART） 神经网络,plsi的推导 关联分析、apriori em算法推导 随机森林的学习过程；随机森林中的每一棵树是如何学习的；随机森林学习算法中CART树的基尼指数是什么？ 谱聚类处理同心圆的情况如何处理 spectral clustering的实现 推导softmax regression模型 模型比较 naive bayes和logistic regression的区别 SVM、LR、决策树的对比？ LR vs. SVM（源于《机器学习》P132） 同：优化目标相近，通常情形下性能也相当 异： LR的优势在于其输出具有自然的概率意义，给出预测标记的同时也给出了概率，而SVM要想得到概率输出需特殊处理 LR能直接用于多分类任务，而SVM则需要进行推广 SVM的解具有稀疏性（仅依赖于支持向量），LR对应的对率损失则是光滑的单调递减函数，因此LR的解依赖于更多的训练样本，其预测开销更大 GBDT和随机森林的区别？ 哪些模型容易过拟合，模型怎么选择 改变随机森林的训练样本数据量，是否会影响到随机森林学习到的模型的复杂度 lr的正则化，与SVM的区别 LR与最大熵的关系 LR与最大熵模型都属于对数线性模型，形式类似 两个模型的学习一般采用极大似然估计，或正则化的极大似然估计，可以形式化为无约束最优化问题（意思然函数为目标函数的最优化问题，此时的目标函数是光滑的凸函数，可以保证找到全局最优解），求解该最优化问题的算法有改进的迭代尺度法、梯度下降法、拟牛顿法等 项目相关 用了什么算法？为什么用？有什么优缺点？ * [实际比赛中为什么tree-ensemble的机器学习方法更好](https://www.zhihu.com/question/51818176/answer/127637712) * 理论（VC维）：模型可控性更好，不易过拟合（用一些弱模型去提升） * 数据：一般基于树的算法的抗噪能力更强 * 树模型中易对缺失值处理 * 树模型对类别特征更友好 * 特征的多样性导致很少使用svm，因为因为 svm 本质上是属于一个几何模型，这个模型需要去定义 instance 之间的 kernel 或者 similarity（线性svm中的内积），而我们无法预先设定一个很好的similarity。这样的数学模型使得 svm 更适合去处理 “同性质”的特征 * 系统实现：——xgboost * 正确高效，C++实现 * 灵活（深度定制不同的分类器、选择不同的损失函数、支持各种语言）、可扩展性好，可以推及到大数据、分布式训练（跨平台） * 简单易用（early_stop,自带cv） * 自适应非线性：随着决策树的生长，能够产生高度非线性的模型，而SVM等线性模型的非线形化需要基于核函数等方法，要求在学习之前就要定好核函数，然而确定合适的核函数并非易事。 * 多分类器隐含正则：高度非线性的模型容易过拟合，因此几乎没有人会用单颗决策树。boosting和random forest等集成学习的方法都要训练多个有差异的学习器，近来有工作表明，这些有差异的学习器的组合，能够起到正则化的作用，从而使得总体复杂度降低，提高泛化能力。尤其对于random forest这样的“并行”集成方法，即使每一颗树都过拟合，直观的来讲，由于过拟合到不同的地方，总体投票或平均后并不会过拟合。 你在研究/项目/实习经历中主要用过哪些机器学习/数据挖掘的算法？ 最好是在项目/实习的大数据场景里用过，比如推荐里用过 CF、LR，分类里用过 SVM、GBDT； 一般用法是什么，是不是自己实现的，有什么比较知名的实现，使用过程中踩过哪些坑； 优缺点分析。 你熟悉的机器学习/数据挖掘算法主要有哪些？ 基础算法要多说，其它算法要挑熟悉程度高的说，不光列举算法，也适当说说应用场合； 你用过哪些机器学习/数据挖掘工具或框架？ 如何进行特征选择？ 用过哪些聚类算法？ 项目中的数据是否会归一化处理，哪个机器学习算法不需要归一化处理 量纲问题：归一化有利于优化迭代速度（梯度下降），提高精度（KNN） 各类算法优缺点、模型调优细节 特征提取的方法（无关键词也是一个特征） 自己实现过什么机器学习算法? 从项目中在哪一方面体会最深 自己项目中有哪些可以迁移到其他领域的东西 项目相比别人有什么优劣 项目的数据从哪里来 项目的特征向量的归一化与异常处理 目前在研究什么 业务 做广告点击率预测，用哪些数据什么算法 http://bbs.pinggu.org/thread-3182029-1-1.html 推荐系统的算法中最近邻和矩阵分解各自适用场景 http://www.doc88.com/p-3961053026557.html 今日头条的个性化推荐是如何实现的？ 推荐算法（基于用户的协同过滤，基于内容的协同过滤） 如何做一个新闻推荐 NLP Q1：给定一个1T的单词文件，文件中每一行为一个单词，单词无序且有重复，当前有5台计算机。请问如何高效地利用5台计算机完成文件词频统计工作？ Ans（有问题的）：将1T文件切分为5份，分配给5台计算机。每台计算机进行词频统计工作，输出一个结果为{单词：频数}的字典结果文件。将5台计算机生成的5个结果文件合并。 Q2：每台计算机需要计算200G左右的文件，内存无法存放200G内容，那么如何统计这些文件的词频？ Ans（不是最优）：首先将文件排序，然后遍历利用list存储结果即可。（不能用字典，因为200G统计出来的结果会很大，没有那么大的内存存放字典。由于经过排序操作，遍历存储并不会使结果丢失，所以用list存储结果即可，每当一个list即将占满内存，则将其写入文件，然后清空list继续存储结果。） Q3：如何将1T的文件均匀地分配给5台机器，且每台机器统计完词频生成的文件只需要拼接起来即可（即每台机器统计的单词不出现在其他机器中） Ans1（不是很好）：对1T文件中的单词进行抽样，获得其概率分布，遍历文件，然后根据首字母的概率均匀分配至5台计算机，如a到e的概率均为0.04, 0.04*5=0.2，则将所有以a-e的单词放入第1台计算机，若z的概率为0.2，则把所有以z开头的单词放入第5台计算机。缺点：不具有可扩展性，如果有100台计算机，那么可能就需要2个字母计算了，则程序就要改变。还有可能出现2台机器中有相同的单词。 Ans2（不是最优）：遍历文件，对于每一个单词，获得单词中各字母的ASCII码值，然后将ASCII值之和取余。则每台机器中的单词必定是不一样。 开放性问题 先不要考虑完善性或可实现性，调动你的一切知识储备和经验储备去设计，有多少说多少，想到什么说什么，方案都是在你和面试官讨论的过程里逐步完善的，不过面试官有两种风格：引导你思考考虑不周之处 or 指责你没有考虑到某些情况，遇到后者的话还请注意灵活调整答题策略; 和同学朋友开展讨论 用户流失率预测怎么做（游戏公司的数据挖掘都喜欢问这个）（涉及数据不平衡问题和时间序列分析） http://www.docin.com/p-1204742211.html 一个游戏的设计过程中该收集什么数据 如何从登陆日志中挖掘尽可能多的信息 http://www.docin.com/p-118297971.html 给你公司内部群组的聊天记录，怎样区分出主管和员工？ 如何评估网站内容的真实性（针对代刷、作弊类）？ 深度学习在推荐系统上可能有怎样的发挥？ 路段平均车速反映了路况，在道路上布控采集车辆速度，如何对路况做出合理估计？采集数据中的异常值如何处理？ 如何根据语料计算两个词词义的相似度？ 在百度贴吧里发布 APP 广告，问推荐策略？ 如何判断自己实现的 LR、Kmeans 算法是否正确？ 100亿数字，怎么统计前100大的？ 每个实体有不同属性，现在有很多实体的各种属性数据，如何判断两个实体是否是同一种东西 重写equals方法，对类里面的对象进行属性比较 学校食堂如何应用数据挖掘的知识 有用户的搜索历史数据，如何判断用户此时的各种状态（准备买车、已经买车等） 给定年龄、性别、学历等信息，对用户的信用进行评估 数学 如何判断函数凸或非凸？ 凸函数定义：对区间$[a,b]$上定义的函数$f$，若他对区间中任意两点$x_1,x_2$均有$f(\\frac{x_1+x_2}{2})\\leq \\frac{f(x_1)+f(x_2)}{2}$,则称$f$为区间$[a,b]$上的凸函数。 U形曲线的函数通常是凸函数 对实数集上的函数通常可以通过求二阶导数来判别，而二阶导数在区间上非负则称为凸函数，若二阶导数在区间上恒大于0则称严格凸函数 解释对偶的概念 一个概率题目： 6个LED灯管，找整体旋转180’后仍然是一个正常输入的情况（考虑全即可） 一个袋子里有很多种颜色的球，其中抽红球的概率为1/4，现在有放回地抽10个球，其中7个球为红球的概率是多少？（伯努利试验） 给定一个分类器p，它有0.5的概率输出1，0.5的概率输出0。 Q1：如何生成一个分类器使该分类器输出1的概率为0.25，输出0的概率为0.75？ Ans：连续进行两次分类，两次结果均为1则输出1，其余情况（10,01,00）均输出0。 Q2：如何生成一个分类器使该分类器输出1的概率为0.3，输出0的概率为0.7？ Tip：小明正在做一道选择题，问题只有A、B和C三个选项，通过抛一个硬币来使选择3个选项的概率相同。小明只需抛连续抛两次硬币，结果正正为A，正负为B，负正为C，负负则重新抛硬币。 Ans：连续进行4次分类（2^4=16 &gt; 10），结果前3种情况则输出1，结果接下来7种情况则输出0，其余情况重新进行分类。 概率题：一个游戏，升一级的概率为p1，等级保持不变的概率为p2，等级下降一级的概率为p3。一个用户经过n个回合，等级为m的概率 大数据相关 如何用hadoop实现k-means？ spark工作原理 spark运行原理 map-reduce mapreduce常用的接口 mapreduce的工作流程 MR优化方式 什么样的情况下不能用mapreduce mapreduce怎么实现join连接 如何解决mapreduce的数据倾斜 mapreduce怎么实现把移动数据到移动计算的 10亿个整数，1G内存，O(n)算法，统计只出现一次的数。 方案一：分拆然后分布式，方案二：对应每个数有三个状态，01代表出现一次，统计10亿以内数据，然后看最终哪些是01状态 海量数据排序； bit位操作 海量数据中求取出现次数最大的100个数 链接 分而治之/hash映射 + hash统计 + 堆/快速/归并排序； 双层桶划分 Bloom filter/Bitmap； Trie树/数据库/倒排索引； 外排序； 分布式处理之Hadoop/Mapreduce。 实现一个分布式的矩阵向量乘的算法 实现一个分布式的topN算法 深度学习 介绍卷积神经网络，和DBN有什么区别？ Deep CNN, Deep RNN, RBM的典型应用与局限，看Hinton讲义和Paper去吧 语言 你觉得Python和Java在使用起来，有什么区别？ Java 数组与链表的区别是什么？ 给你两张表，表A和表B，其中表A有3条数据，表B有5条数据，问：表A left join 表B后有几条？ ArryList、LinkedList、vector的区别 hashMap HashTable的区别 垃圾回收机制 JVM的工作原理 for循环LinkedList 遍历HashMap的并且把某一个值删除 线程 进程 Java中Runnable和Thread的区别Callable Callable与Future的介绍 sleep wait区别 java的数据类型 JAVA如何实现序列化 反序列化是什么？ 序列化是将（内存中的）结构化的数据数据，序列化成2进制 python读取文件，写代码 python计算一个文件中有N行，每行一列的数的平均值，方差，写代码 main(argc,argv[])里面两个参数什么意思 args是Java命令行参数，我们在DOS中执行Java程序的时候使用“java 文件名 args参数”。args这个数组可以接收到这些参数。当然也可以在一个类中main方法中直接调用另一个类里的main方法，因为main方法都是static修饰的静态方法，因此可以通过类名.main来调用，这时就可在调用处main方法中传入String[]类型的字符串数组，达到调用的目的，也可不传入参数 Python list有哪几种添加元素的方法，能否从表头插入元素？(append, extend和insert, insert能从表头插入元素, 但是时间复杂度为O(n).) 如何提高Python的运行效率 写一个简单的正则匹配表达式(将文本中的123.4匹配出来)（Python） a = [1, 2, 3, 4], b = a, b[0] = 100, 请问print(a)结果是什么 list是怎样实现的 常用的数据结构及应用场景（list，dict，tuple） Python装饰器、yield等 给定一个文件，包含了data, city, count等信息，写代码实现给定city和date1-date2日期内，count最高的日期 数据结构与算法 算法的时间复杂度和空间复杂度是什么？ 时间复杂度：在进行算法分析时，语句总的执行次数T(n)是关于问题规模n的函数，进而分析T(n)随n的变化情况并确定T(n)的数量级。算法的时间复杂度记作T(n)=O(f(n))。 空间复杂度：算法的空间复杂度通过计算算法所需的存储空间实现，算法空间复杂度的计算公式记作：S(n)=O(f(n))，其中n为问题的规模，n为问题的规模，f(n)为语句关于n所占存储空间的函数 两个数组，求差集 写程序实现二分查找算法，给出递归和非递归实现，并分析算法的时间复杂度。 实现单链表的反转 求两个一维数组的余弦相似度，写代码 字符串翻转 快排 非递归的二叉前/中/后序遍历 手写二叉树前/中/后序递归遍历算法（千万不要忘记异常处理！） 两个字符串的复制（除了字符串地址重叠的情况，也要注意判断字符串本身的空间足够不足够，对于异常情况要考虑全面） 一个数组，如果存在两个数之和等于第三个数，找出满足这一条件的最大的第三个数（设为x+y =c） 怎样将二叉排序树变成双向链表，且效率最高 从栈里找最小的元素，且时间复杂度为常数级 float转string 判断一棵树是否是另一棵的子树 在一个n*n的矩阵中填数的问题，那种转圈填数 链表存在环问题，环的第一个节点在哪里？ 几个排序算法，时间复杂度&amp;空间复杂度 数据结构当中的树，都有哪些？ 二叉查找树（二叉排序树）、平衡二叉树（AVL树）、红黑树、B-树、B+树、字典树（trie树）、后缀树、广义后缀树 链接 输出一个循环矩阵 翻转字符串（《剑指offer》原题） N个数找K大数那个题,堆解释了一遍,比较满意,问还能怎么优化O(Nlogk)的方法，并行方面想 一个班60个人怎么保证有两个人生日相同,听完后有点奇怪,①为什么是60个人?②为什么是保证?,反正没管这么多就是概率嘛,算就完了. 1减去50个人生日不同的概率≈100% 问一个字符串怎么判断是邮箱比如:vzcxn@sdf.gre.有限状态自动机,然后要我画状态转移图. 链接 给10^10个64位数,100M内存的空间排序,一个求中位数的方法.用文件操作来做了,像快排一样,二分选个数统计大于那个数的数量和小于那个数的数量,如果能用100M的空间排序就把那些数排了,如果不能继续.直到能排为止. 链接 kmp算法 求最大字段和，用动态规划和分治法两个方法，时间复杂度怎么算 统计字符串中出现的字符个数，忽略大小写，其中可能有其他字符。 寻找二叉树的公共父节点 b+ b-树、红黑树 判断两条链表是否交叉(360有问)最简单的方法是遍历到两个链表的尾节点然后判断两个尾节点是否相同即可 树的广度、深度遍历 稳定与不稳定排序 并行计算、压缩算法 最长上升子序列，两个大小相同的有序数组找公共中位数 介绍大顶堆和小顶堆 反转链表 单链表转二叉树，二叉树转单链表（要求原地） 手写sqrt()，不调用其它任何函数http://liuqing-2010-07.iteye.com/blog/1396859 参考 知乎-如何准备机器学习工程师的面试 ？ by 刘志权 机器学习及大数据相关面试的职责和面试问题(转) by 姚凯飞","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"offer","slug":"offer","permalink":"http://yoursite.com/tags/offer/"}]},{"title":"统计学习方法笔记（六）——Logistic Regression","date":"2017-07-08T05:49:08.000Z","path":"2017/07/08/lr/","text":"Logistic Regression与最大熵模型均属于对数线性模型 Logistic Regression的理解角度一：输入变量$X$服从Logistic分布的模型Logistic分布 定义：设$X$是连续随机变量，$X$服从Logistic分布是指$X$具有下列分布函数和密度函数 分布函数$$F(X)=P(X \\leq x)=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}$$ 密度函数$$f(x)=F’(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma(1+e^{-(x-\\mu)/\\gamma})^2}$$其中$\\mu$为位置参数，$\\gamma&gt;0$为形状参数 分布函数属于Logistic函数，对应的图形为一条S型曲线（sigmoid curve），以点$\\left(\\mu,\\frac{1}{2}\\right)$为中心对称，如图，曲线在中心附近增长较快 二项LR模型 二项LR模型是一种分类模型，由条件概率分布$P(Y|X)$表示，形式为参数化的Logistic分布，其中随机变量$X$的取值为实数，随机变量$Y$的取值为1或0 定义：二项LR模型是如下的条件概率分布：$$P(Y=1|x)=\\frac{exp(w \\cdot x+b)}{1+exp(w \\cdot x+b)}$$$$P(Y=0|x)=\\frac{1}{1+exp(w \\cdot x+b)}$$其中$w \\in R^n$称为权值向量，$b$称为偏置，两者为需要估计（学习）的参数,$w \\cdot x$为$w$和$x$的内积 对于给定的输入实例$x$，按照上式可求得对应的条件概率，比较两个条件概率的大小，将实例划分到较大的一类 有时为了方便，将偏置扩充到权值向量中，即$w=(w^{(1)},w^{(2)},…,w^{(n)},b)^T, x=(x^{(1)},x^{(2)},…,x^{(n)},1)^T$,此时模型表示如下$$P(Y=1|x)=\\frac{exp(w \\cdot x)}{1+exp(w \\cdot x)}$$$$P(Y=0|x)=\\frac{1}{1+exp(w \\cdot x)}$$ 角度二：LR模型将线性分类函数转换为（条件）概率 几率：一个事件的几率是指该事件发生的概率与该事件不发生的概率的比值：若事件发生的概率为$p$则该事件的几率为$\\frac{p}{1-p}$，进一步地该事件的对数几率为$$logit(p)=log\\frac{p}{1-p}$$ 而观察前面的LR模型可以发现，$$log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=log\\frac{P(Y=1|x)}{P(Y=0|x)}=w \\cdot x$$，也就是说，在LR模型中，输出$Y=1$的对数几率是输入$x$的线性函数，或者说输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型，即LR模型（所以说Logistic Regression属于对数线性模型） 因此，针对对输入$x$进行分类的线性函数，通过LR模型可以将其转换为（条件）概率：$$P(Y=1|x)=\\frac{exp(w \\cdot x+b)}{1+exp(w \\cdot x+b)}$$，其中线性函数的值越接近正无穷，概率值就越接近1，线性函数的值越接近负无穷，概率值就越接近0，这样的模型就是LR模型。 角度三：广义线性模型在分类问题上的推广（参考《机器学习》） 用线性回归模型的预测结果去逼近真实标记的对数几率 对于二分类任务：其输出标记$y \\in \\lbrace 0,1 \\rbrace$,而线性回归模型产生的预测值$f(x)=w\\cdot x+b$是实值 而根据广义线性模型的思想，可以通过寻找一个联系函数来将分类任务的真实标记与线性回归模型的预测值联系起来 最直接的想法是使用单位阶跃函数，即 但是单位阶跃函数不连续，所以我们希望找到一个替代函数来在一定程度上近似单位阶跃函数，并希望该函数单调可微，于是就引入了对数几率函数$$y=\\frac{1}{1+e^{-z}}$$ 将对数几率函数作为$g^-(\\cdot)$带入广义线性模型则有$$y=\\frac{1}{1+e^{-(w \\cdot x+b)}}$$ 其变形形式为$$ln\\frac{y}{1-y}=w\\cdot x+b$$,若将$y$视作样本$x$作为正例的可能性，则$1-y$是其作为反例的可能性，两者的比值$\\frac{y}{1-y}$称为几率（联系角度二），反映了$x$作为正例的相对可能性 LR模型的优点 LR模型直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题 LR模型不是仅预测出类别，而是可得到近似概率预测，这对于许多需利用概率辅助决策的任务很有用 对数几率函数是任意阶可导的凸函数，有很好的数学性质，现有很多数值优化算法均可直接用于求取最优解 模型参数估计损失函数推导 基本思想：应用极大似然估计法估计模型参数，从而将问题转化为以对数似然函数为目标函数的最优化问题，然后采用梯度下降法或者拟牛顿法进行求解 此处的目标函数为考点 为什么LR模型的损失函数是交叉熵？而线性回归模型的损失函数却是最小二乘呢？能否随意确定一个损失函数作为目标呢？ 答案是模型的损失函数由各自的响应变量$y$的概率分布决定，比如对于线性回归模型，针对的问题是预测某个结果，属于回归问题，其输出是连续值，所以我们对于该问题假设$y$服从正态分布；相对的，LR模型一般用来解决二分类问题，所以其输出是0/1，故而我们假设其输出服从伯努利分布；而进一步地，两者的损失函数都是通过极大似然估计推导的来的，所以模型的损失函数并非随意确定。 另外，对于线性回归问题，它的模型是 $p(y|x) = N(w^Tx, \\sigma^2)$,我们采用最大似然来构造一个目标函数，最后用梯度下降来找到目标函数的最值。当然，对于这个问题，我们也可以不用梯度下降，直接用向量的投影来直接算出最优解的表达式,即“最小二乘法”；而LR模型是$p(y|x)=Ber(y|sigmoid(w^Tx))$，Ber是伯努利分布，sigmoid是logistic sigmoid函数，我们采用最大似然来构造一个目标函数，与之前的问题不同，这个目标函数比较复杂，是无法像线性回归那样一步直接算出最终的解的，但是，这个目标函数是凸的，所以我们依然能用梯度下降或者牛顿法来来找到它的最优解。 在LR中，最大似然函数与最小化对数损失函数等价 梯度下降法推导 sigmoid由来及推导 最大熵的推导方式 指数族分布的推导方式 多项LR模型——多分类假设离散型随机变量$Y$的取值集合为$\\lbrace 1,2,…,K \\rbrace$，则多项LR模型是$$P(Y=k|x)=\\frac{exp(w_k \\cdot x)}{1+\\sum_{k=1}^{K-1}exp(w_k \\cdot x)}$$$$P(Y=K|x)=\\frac{1}{1+\\sum_{k=1}^{K-1}exp(w_k \\cdot x)}$$注意$k=1,2,…,K-1$,其中$x\\in R^{n+1}, w_k \\in R^{n+1}$ 面试考点LR中损失函数的意义是什么？在LR中，最大似然函数与最小化对数损失函数等价，证明如下 我们前面已经推导过极大对数似然估计为$$lnL(w)=\\sum_{i=1}^N[y_iln\\pi(x_i)+(1-y_i)ln(1-\\pi(x_i))]$$一般而言，对数似然函数在单个数据点的定义为$$-ylnp(y|x)-(1-y)ln[1-p(y|x)]=-[y_iln\\pi(x_i)+(1-y_i)ln(1-\\pi(x_i))]$$此时取整个数据集上的平均对数似然损失就可以得到$$J(w)=-\\frac{1}{N}lnL(w)$$可见最大似然函数与最小化对数损失函数是等价的 LR与线性回归的联系与区别逻辑回归和线性回归首先都可看做广义的线性回归，其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。 LR与最大熵模型逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应类别为二类时的特殊情况，也就是当逻辑回归类别扩展到多类别时，就是最大熵模型。 指数簇分布的最大熵等价于其指数形式的最大似然。 二项式分布的最大熵解等价于二项式指数形式(sigmoid)的最大似然； 多项式分布的最大熵等价于多项式分布指数形式(softmax)的最大似然。 LR与朴素贝叶斯LR与神经网络LR与感知机 相同点 都是线性分类模型，原始模型都只能处理二分类问题 模型的形式相似(都是在线性模型外通过联系函数映射到另外的空间，只不过感知机是符号函数，LR是sigmoid(或者说Logistic)函数) 训练方法都是使用梯度下降法 不同点 LR输出为仅是概率的值，而感知机输出为1/-1 LR与SVM多分类-softmax如果y不是在[0,1]中取值，而是在K个类别中取值，这时问题就变为一个多分类问题。有两种方式可以出处理该类问题：一种是我们对每个类别训练一个二元分类器（One-vs-all），当K个类别不是互斥的时候，比如用户会购买哪种品类，这种方法是合适的。如果K个类别是互斥的，即y=i的时候意味着y不能取其他的值，比如用户的年龄段，这种情况下 Softmax 回归更合适一些。Softmax 回归是直接对逻辑回归在多分类的推广，相应的模型也可以叫做多元逻辑回归（Multinomial Logistic Regression）。模型通过 softmax函数来对概率建模，具体形式如下：$$P(y=i|x, \\theta) = \\frac{e^{\\theta_i^T x}}{\\sum_j^K{e^{\\theta_j^T x}}}$$其中决策函数为$$y^* = argmax_i P(y=i|x,\\theta)$$损失函数为$$J(\\theta)=-\\frac{1}{N} \\sum_i^N \\sum_j^K {1[y_i=j] \\log{\\frac{e^{\\theta_i^T x}}{\\sum {e^{\\theta_k^T x}}}}}$$ LR优缺点 优点 实现简单 分类时计算量非常小，速度快，存储资源低 缺点 容易欠拟合，一般准确度不太高 原始的LR仅能处理二分类问题，且必须线性可分(衍生出的softmax可以用于多分类) LR模型在工业界的应用常见应用场景 预估问题场景（如推荐、广告系统中的点击率预估，转化率预估等） 分类场景（如用户画像中的标签预测，判断内容是否具有商业价值，判断点击作弊等 LR适用上述场景的原因LR模型自身的特点具备了应用广泛性 模型易用：LR模型建模思路清晰，容易理解与掌握； 概率结果：输出结果可以用概率解释（二项分布），天然的可用于结果预估问题上； 强解释性：特征（向量）和标签之间通过线性累加与Sigmoid函数建立关联，参数的取值直接反应特征的强弱，具有强解释性； 简单易用：有大量的机器学习开源工具包含LR模型，如sklearn、spark-mllib等，使用起来比较方便，能快速的搭建起一个learning task pipeline； 面临的问题 学习的过拟合问题； 学习的数据稀疏性问题； 模型自身的学习效率（收敛速度，稳定性）； 训练模型时数据、特征的扩展性问题，即学习算法可否在分布式环境下工作； 如何结合实际应用场景（比如多资源位／多广告位的点击预估问题），给出相应的解决方案. 可行的解决方法 loss function + $\\lambda |w|_2^2$：解决过拟合 loss function + $\\lambda |w|_1$：解决稀疏性，比如Google13年出的预估方法－FTRL模型，虽然是在线学习算法，但主要是为了解决预估时的稀疏性问题。 超大规模稀疏LR模型学习问题，LR模型自身是做不到的。这个时候需要我们为它选择一个学习算法和分布式系统。在分布式环境下，约束优化求解理想方案之一－ADMM算法（交叉方向乘子法），可用于求解形式为”loss function + 正则项”目标函数极值问题。 参考资料 李航《统计学习方法》 周志华《机器学习》 知乎-最小二乘、极大似然、梯度下降有何区别？ 美团点评技术团队 逻辑回归LR推导 Logistic Regression Theory LR推导原文 LR推导-指数族分布 Logistic Regression 的前世今生（理论篇） 深入浅出ML之Regression家族 线性回归、Logistic 回归和感知机","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"LR","slug":"LR","permalink":"http://yoursite.com/tags/LR/"}]},{"title":"GBDT","date":"2017-07-07T06:50:00.000Z","path":"2017/07/07/GBDT/","text":"参考资料:A Gentle Introduction to Gradient Boosting gradient boosting = gradient descent + boosting 附：回顾AdaBoost AdaBoost vs. Gradient Boosting AdaBoost Gradient Boosting 相同点 1、加性模型+前向分步算法 2、每一步训练一个弱学习器以弥补前面模型的不足 不同点 1、AdaBoost中当前学习器的“不足”由样本权重来决定 2、GBDT中当前学习器的“不足”由求梯度决定 Boosting历史 发明AdaBoost。第一个成功的提升算法 规整AdaBoost为使用特殊损失函数的梯度下降方法 将AdaBoost泛化为Gradient Boosting（能处理一般的损失函数） 简单例子理解给定数据集$(x_1,y_1),(x_2,y_2),…,(x_n,y_n)$，现在需要拟和模型$F(x)$来使平均误差最小化，假设你的朋友给你提供了一个模型$F$，你发现该模型还可以进一步提升，对于上述数据集，该模型存在少量错误，比如$F(x_1)=0.8$但$y_1=0.9; F(x_2)=1.4$但$y_2=1.3$，如何改进这个模型？ 一种可行的方法就是在原始的模型$F$的基础上添加一个模型，h（比如回归树），从而得到模型$F(x):=F(x)+h(x)$ 通过拟合来改进该模型$$F(x_1)+h(x_1)=y_1$$$$F(x_2)+h(x_2)=y_2$$$$…$$$$F(x_n)+h(x_n)=y_n$$于是我们可以得到$$h(x_i)=y_i-F(x_i)$$,也就是在$F$的基础上我们针对数据集$(x_1,y_1-F(x_1)),(x_2,y_2-F(x_2)),…,(x_n,y_n-F(x_n))$拟和一个模型$h$，其中$y_i-F(x_i)$被称做是残差 与梯度下降法的关联（回归树）梯度下降$$\\theta_i=\\theta_i-\\rho \\frac{\\partial J}{\\partial \\theta_i}$$通过朝负梯度方向移动来最小化函数 损失函数残差是如何与梯度联系起来的？那么我们为何还需要其他的损失函数？是因为平方损失函数不够好么？ 平方损失函数 优点：易于计算 缺点：对于离群值非常敏感（容易为了拟合离群值而导致整体性能下降） 而其他的一些损失函数比如绝对损失函数、huber loss（丘陵损失）等对于离群值相对没有那么敏感 一般过程（算法） 以$L$为损失函数的回归问题的一般过程 初始化模型如$F(x)=\\frac{\\sum_{i=1}^ny_i}{n}$ 循环直至收敛 计算负梯度$-g(x_i)=-\\frac{dL(y_i,F(x_i))}{dF(x_i)}$ 用一个回归树$h$来拟合负梯度 更新模型$F:=F+\\rho h$ Notes:相较之下，负梯度关注离群点较少。 GBDT用于分类——识别手写字母（多分类）基本思路 训练26个模型：$F_A ~ F_Z$ 针对每一个样本实例，分别使用每一个模型打分，计算样本属于某个类别的概率 将概率最高的类标记为该样本的预测标记 一般过程 将样本集的实际标记转换为概率分布的形式（0-1） 计算各个模型预测样本属于各自分类的概率 计算真实概率与预测概率的差值，目的是通过调整模型减少整体误差（使预测点概率分布尽可能接近真实概率分布） Notes:优化参数矩阵、计算梯度矩阵","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"GBDT","slug":"GBDT","permalink":"http://yoursite.com/tags/GBDT/"}]},{"title":"模型集成小记","date":"2017-07-03T12:08:51.000Z","path":"2017/07/03/ensemble/","text":"本文主要基于周志华《机器学习》一书第八章 集成学习内容做的整理笔记，此外查阅了网上的一些博客和问答网站。 概念 集成学习通过构建并结合多个学习器来完成学习任务。 如何使得集成学习性能比最好的单一学习器更好？ 准确性 多样性 好而不同 如何产生并结合“好而不同”的个体学习器？集成学习研究的核心当前按照个体学习器的生成方式划分 bagging（及其变体随机森林）——个体学习器间不存在强依赖关系，可同时生成的并行化方法 boosting——个体学习器间存在强依赖关系，必须串行生成的序列化方法 从偏差-方差分解的角度，bagging和boosting分别是关注降低方差和降低偏差的两个代表，也可以按照机器学习技法的两张图来理解第一幅图(对应boosting)可以看作进行了一个特征转换来防止欠拟合，第二幅图(对应bagging)则是为进行了一个正则化来防止过拟合 boosting工作机制 首先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T,最终将这T个基学习器进行加权组合。 AdaBoost参见提升方法 推导方式，基于“加性模型”，即基于基学习器的线性组合 如何在每一轮修改样本分布 重赋权法：在每一轮训练中，根据样本分布为每一个训练样本重新赋予一个权重（比如《机器学习实战》一书中就是利用这种方法构建提升树算法，通过修改权重来计算每一轮的损失） 重采样法：利用重采样的训练集来对基学习器进行训练–&gt;重启动：避免训练过程过早停止 Notes: 西瓜书上的算法还提到训练的每一轮开始都要检查当前学习器是否比随机猜测好，若条件不满足则抛弃当前学习器，这种情形可能会导致学习过程未达到T轮即停止，所以有重采样的方法来进行重启动避免出现此种情况；但是另一方面《统计学习方法》以及《机器学习实战》中的算法并未有这条判断语句； 《机器学习》一书中提到，从统计学的出发认为AdaBoost实质上是基于加性模型（后续指出这一视角阐释的是一个与AdaBoost很相似的过程而非其本身），以类似牛顿迭代法来优化指数损失函数，通过将迭代优化过程替换为其他优化方法产生了GradientBoosting、LPBoost等；而这里也提到每一种变体针对不同的问题（噪声、数据不平衡等）而拥有不同的权重更新规则。 特点 从偏差-方差分解的角度来看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成（与bagging不同）。 Notes: boosting对于噪音数据较为敏感 bagging Bootstrap AGGregatING的缩写 出发点依然是“好而不同” “不同”——不同基学习器基于不同的样本子集 “好”——使用相互有交叠的采样子集 工作机制基于自助采样法（bootstrap sampling）——“有放回地全抽” 给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样经过m次随机采样操作，我们得到含m个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现，照这样我们可采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合；在对预测输出进行结合时，通常对分类任务使用简单投票法，对回归任务使用简单平均法 优点（相对于boosting） 高效——训练一个bagging集成与直接使用基学习器算法训练一个学习器的复杂度同阶 baggign能不经修改地用于多分类、回归任务（标准AdaBoosting只能适用于二分类任务） 包外估计——自助采样过程中剩余的样本可以作为验证集来对泛化性能进行“包外估计” Notes:包外样本的其他好处（针对基学习器为决策树或神经网络时） 可使用包外样本来辅助剪枝 用于估计决策树中各结点的后验概率以辅助对零训练样本节点的处理 当基学习器为神经网络时，可使用包外样本来辅助早期停止以减小过拟和风险 特点 从偏差-方差角度看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。 随机森林概述 Bagging的一个扩展变体 以决策树为基学习器构建Bagging集成 在决策树的训练过程中引入了随机属性选择 传统决策树在选择划分属性时是在当前结点的属性集合中选择一个最优属性；而在随机森林中，对基决策树的每个节点，先从该节点属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分，其中k控制了随机性的引入程度 特点 简单、易于实现、计算开销小 样本扰动+属性扰动 bagging vs. 随机森林 两者收敛性相似，随机森林的起始性能往往相对较差，但会收敛到更低的泛化误差 随机森林的训练效率常优于Bagging，主要是决策树划分属性时，原始baggin需要对属性全集进行考虑，而rf是针对一个子集 结合策略学习器结合的好处 统计的角度：假设空间很大时，可能存在多个假设在训练集上达到同等性能，但学习其可能误选导致泛化性能不佳，结合多个学习器可以减小该风险 计算的角度：降低陷入糟糕局部极小点的风险 表示的角度：结合多个学习器可扩大假设空间，对于真实假设在假设空间之外的情形可能学得更好的近似 策略 平均法(Averaging) 简单平均法 加权平均法——BMA(贝叶斯模型平均：基于后验概率来为不同模型赋予权重) 投票法(Voting) 多数投票法 绝对多数投票法：若某标记得票过半数则预测为该标记，否则拒绝预测（可靠性） 相对多数投票法：预测为得票最多的标记 加权投票法 若按个体学习器输出值类型划分 硬投票：预测为0/1 软投票：相当于对后验概率的一个估计 学习法 Stacking:先从初始数据集训练出初级学习器，然后生成一个新的数据集用于训练元学习器，在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记；一般使用交叉验证法或留一法来用训练初级学习器未使用的样本来产生元学习器的训练样本 Notes:关于Stacking 《机器学习》作者也指出Stacking本身是一种著名的集成方法，且有不少变体和特例，但他这里是作为一种特殊的结合策略看待 关于Stacking的细节详述，个人觉得“如何在 Kaggle 首战中进入前 10%”一文中阐述的比较透彻，以一幅图来说说明5折Stacking的过程 推荐一个Python的实现了Stacking集成的库mlxtend 原作者举了一个5折stacking的例子，基本方法是， 每一折取训练集80%的数据训练一个基模型并对剩下的20%的数据进行预测，同时将该模型对测试集做出预测，保留训练子集的预测结果和测试集的预测结果 将5折的训练子集预测结果结合起来构成第二层元模型的输入特征进行训练得到元分类器 将前面每一折在测试集预测得到的结果取均值作为最终元分类器的预测输入(最终的测试数据)，并使用训练好的元分类器在该数据上作出最终预测 此外知乎上的一篇文章还提到 可以将K个模型对Test Data的预测结果求平均，也可以用所有的Train Data重新训练一个新模型来预测Test Data 多样性误差-分歧分解 集成学习“好而不同”的理论分析，见《机器学习》P185~186 寻找集成泛化误差、个体学习器泛化误差、个体学习器间的分歧三者之间的关系 多样性度量 多样性度量是用于度量集成中个体分类器的多样性，即估算个体学习器的多样化程度，典型做法是考虑个体分类器的两两相似/不相似性 度量方法 不合度量 相关系数 Q-统计量 k-统计量 多样性增强如何增强多样性？——在学习过程中引入随机性 类别 数据样本扰动 输入属性扰动 输出表示扰动 算法参数扰动 思路 给定初始数据集，可从中产生不同的数据子集，再利用不同的数据子集训练出不同的个体学习器，通常基于采样法 从初始属性集中抽取若干个属性子集、基于每个属性子集训练一个基学习器（如随机子空间算法），最后结合 对输出表示进行操纵，比如 1、对训练样本的类标记稍作变动（翻转法） 2、对输出表示进行转化，如输出调制法 3、将原任务拆解成多个可同时求解的子任务，如Ecoc法 随机设置不同的参数，比如1、负相关法 2、对参数较少的算法，可将其学习过程中某些环节用其他类似方式替代 适用 不稳定学习器：决策树、神经网络 包含大量冗余属性的数据 - - Notes: 数据样本扰动中相对的稳定基学习器包括：线性学习器、支持向量机、朴素贝叶斯、k近邻学习器等 对于算法参数扰动，与交叉验证做比较，交叉验证常常是在不同参数组合模型里选择最优参数组合模型，而集成则是将这些不同参数组合的模型结合起来，所以集成学习技术的实际计算开销并不比使用单一学习器大很多 参考 周志华《机器学习》 kaggle-ensembling-guide Bagging, Boosting &amp; Stacking stackexchange及评论区 如何在 Kaggle 首战中进入前 10% 分分钟带你杀入Kaggle Top 1% 机器学习技法","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"ensemble","slug":"ensemble","permalink":"http://yoursite.com/tags/ensemble/"}]},{"title":"统计学习方法笔记(八) —— 提升方法","date":"2017-06-29T05:45:11.000Z","path":"2017/06/29/boosting/","text":"在分类问题中，Boosting通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。 “三个臭皮匠，顶个诸葛亮” 关于boosting，机器学习技法一课中开头举的一个小学生认苹果的例子比较传神，假设给定一堆水果，让一群小学生来识别其中的苹果，逐个的询问小学生根据什么样的规则来认为一个水果是苹果，比如学生A认为形状为圆的是苹果，第二个学生认为颜色为红的为水果等等，逐一询问学生的过程中，每问一个学生之后，按照他的规则对当前水果分类，记录其中的错分的水果然后让下一个同学重点根据这些被错分的水果来找找规则，这样一步一步的最后全班同学得到一个较为完备的规则来判定出苹果，这个过程中单个学生扮演的较色就是基分类器，比如常用的决策树桩，类似某一维的的感知机，在该维度将空间一分为二的划分样本，而教师的角色则是在这一过程中不断引导学生关注被上一个学生错分的样本来做出判定规则，并最终得出一个较为完备的输出结果 关键词：强可学习、弱可学习、PAC(probably approximately correct) 由于发现弱学习算法通常比发现强学习算法容易，所以Boosting试图解决这么一个问题：“如果已经发现了‘弱学习算法’，那么能否将它提升为‘强学习算法’？” 提升方法就是从弱学习算法出发，反复学习(改变训练数据的权值分布)，得到一系列弱分类器，然后组合这些弱分类器，构成一个强分类器； 提升方法需要解决的两个问题： 在每一轮训练中，如何改变训练数据的权值/概率分布？ 在得到一系列弱分类器后，如何将弱分类器组合成一个强分类器？ AdaBoost——Adaptive Boosting基本思路针对提升方法要解决的两个问题，AdaBoost的做法是这样的:（两个‘权值’） 提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。（让被错误分类的样本得到当前弱分类器更多的‘关注’） AdaBoost采取加权多数表决的方法来组合弱分类器：加大分类误差率小的弱分类器的权值 Notes:由上述基本思路可以发现，对于AdaBoost算法而言，其中最核心的部分是两个权值的求解，一个是每一轮用于训练基分类器的训练数据的样本的权值分布，而另一个则是最终组合各个基分类器时代表各个基分类器的‘话语权’的分类器的权重，而这两者都跟每一轮的基分类器的分类错误、训练误差相关；样本的权值分布根据前一轮基分类器的错误来进行调整，是一种‘弥补前人错误’的体现，而基分类器的权重则是受到每一轮训练的基分类器在该轮的分类误差影响，分类误差越小的基分类器自然可信度越高，所以最终我们组合基分类器时给它赋予的‘话语权’越大。 算法-AdaBoost 输入： 训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $，其中$x_i \\in R^n, y_i \\in \\lbrace -1, +1 \\rbrace$; 弱学习算法 输出：最终分类器$G(x)$ (1) 初始化训练样本的权值分布为$$D_1=(w_{11}, w_{12},…,w_{1N}), w_{1i}=\\frac{1}{N}, i=1,2,…,N$$即各个训练样本的权重均等 (2) 对$m=1,2,…,M, m$表示训练轮数(亦即第$m$个基分类器，因为每一轮产生一个基分类器)： 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器$G_m(x)$ 计算$G_m(x)$在训练数据集上的分类错误率：$$e_m=P(G_m(x_i) \\neq y_i)=\\sum_{i=1}^Nw_{mi}I(G_m(x_i) \\neq y_i)$$即(分类错误的样本$\\times$该样本的权重)之和 计算最终基分类器在组合分类器中的权重值$$\\alpha_m=\\frac{1}{2}log\\frac{1-e_m}{e_m} $$为什么是这么个值，周志华的西瓜书上有对应推导，也可以参考维基百科的推导部分 更新训练数据集的权值分布$$D_{m+1}=(w_{m+1,1},…,w_{m+1,i},…,w_{m+1,N})$$如果$G_m(x_i)=y_i$那么$$w_{m+1,i}=\\frac{w_{m,i}}{Z_m}e^{-\\alpha_m}$$否则$$w_{m+1,i}=\\frac{w_{m,i}}{Z_m}e^{\\alpha_m}$$ 如果将权重更新简化表示就是(因为$y_i,G_m(x_i) \\in \\lbrace -1, +1 \\rbrace$,根据其异同可以决定指数部分的符号)$$w_{m+1,i}=\\frac{w_{m,i}}{Z_m}exp(-\\alpha_my_iG_m(x_i))$$其中$Z_m$是规范化因子，用来使权重分布成为一个概率分布(这里就可以看到错分样本的权重被调大)$$Z_m=\\sum_{i=1}^Nw_{m,i}exp(-\\alpha_my_iG_m(x_i))$$ (3) 构建基本分类器的线性组合$$f(x)=\\sum_{m=1}^M\\alpha_mG_m(x)$$得到最终分类器$$G(x)=sign(f(x))=sign(\\sum_{m=1}^M\\alpha_mG_m(x))$$ Notes: 关于权值分布更新的计算：由于$Z_m$是未规范化的权值分布更新结果之和，所以一般先通过$$\\acute{w_{m+1,i}}=w_{m,i}exp(-\\alpha_my_iG_m(x_i))$$进一步由$$Z_m=\\sum_{i=1}^N\\acute{w_{m+1,i}}$$求得$Z_m$，最后将$\\acute{w_{m+1,i}}$均除以$Z_m$即得到$w_{m+1,i}$的值，其实本质上就是一个规范化的过程 训练结束的条件：一般是基分类器的组合分类器分类错误率足够小或者达到指定的基分类器数目（亦即迭代次数m）时即停止迭代训练 由上述算法可知AdaBoost的两大特点 不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用（与bagging的区别） 利用基本分类器的线性组合构建最终的分类器（与GBDT的区别） $f(x)$的符号决定实例的类别，其绝对值表示分类的确信度 关于上述算法流程的熟悉，可以通过书上P140简单的例子或者《机器学习实战》一书中对应AdaBoost实现的部分来感受一下 从整个算法的流程结合维基百科的推导可以看出算法的核心部分，即一再强调的两个权重$\\alpha_m,w_m$，引用维基百科的话来说 每一轮，我们通过最小化权重损失和$\\sum_{y_i\\neq G_m(x_i)}w_{m,i}$来选择最佳的基分类器$G_m$,并得到该基分类器对应的错误率$e_m=\\sum_{i=1}^Nw_{mi}I(G_m(x_i) \\neq y_i)$，用计算得到的错误率可以计算基分类器在最终的模型里的权重$\\alpha_m=\\frac{1}{2}log\\frac{1-e_m}{e_m}$,最后根据该权重改善当前模型并更新下一轮的基分类器的权重分布$w_m$ 更行权重分布的计算过程也可以从样本的正反例比的角度来看，第$t+1$次调整权重分布的目的是为了学得一个与第$t$次不一样的模型，避免犯同样的错误，所以需要根据第$t$次的分类错误来重新调整样本分布，使正负样本权重和均衡(一般而言第$t$次训练后的分类错误的样本数要少于正确的样本数，其实这个过程也就是加大分错样本的权重)，具体而言就是按照正负样本比例调整，比如分类错误样本数为$p$,分类正确样本数为$q$错误率为$e_m=\\frac{p}{p+q}$，则在第$t+1$轮需要将负样本权重调整为$w_t\\times q$对应的正确样本权重调整为$w_t\\times p$使两者平衡，也就等价于对错误样本$w_{t+1}=w_t\\times (1-e_m)$,对正确样本$w_{t+1}=w_t\\times (e_m)$,进一步可以定义变量$\\lambda=\\sqrt{\\frac{1-e_m}{e_m}}$,于是对错误样本$w_{t+1}=w_t\\times \\lambda$,对正确样本$w_{t+1}=w_t \\div \\lambda$,(加根号的做法还是在机器学习技法上看到，另外的《统计学习方法》、《机器学习》上面都没有加根号)这么做使其具有更好的物理意义，具体的，如果分类错误率不超过$\\frac{1}{2}$则$\\lambda&gt;1$也就表明更加关注上一轮被错分样本，而减小正确分类的样本的关注度，同时也表明基分类器应该优于随机猜测 进一步地，对于$\\alpha$即各个基分类器最终在加和模型里的权重而言，其创作者对取值设置为$\\alpha_t=ln(\\lambda)$，其物理意义就是在极限情况下随机猜测的话，分类错误率为0.5而对应的$\\lambda=1$所以$\\alpha=0$也就表明对于乱猜的分类器的分类结果最后不计入整体分类器，而另外对于分类错误率为0的情形，对应的$\\alpha=\\infty$也就是说如果一个单一的分类器就足以分类了，那么就用这一个就够了；此外，按照林轩田的说法，算法中基分类器和$\\alpha_t$的计算是on the fly的做法，也就是每计算一个基分类器也就可以计算对应的$\\alpha_t$，最后就得到一系列基分类器和对应的权重值 简而言之就是每轮一方面通过最小化基分类器的分类错误率来选择该轮的基分类器以及最终的权重，另一方面据该轮表现来为修正当前模型的错误而更新样本权重分布 训练误差分析即证明 AdaBoost能在学习过程中不断减少训练误差，即在训练数据集上的分类误差率（关于这一点进一步引出了AdaBoost是否不会过拟合的讨论） AdaBoost的训练误差是以指数速率下降的 由上面两个定理，可以得到推论 如果存在$\\gamma&gt;0$,对所有$m$有$\\gamma_m \\leq \\gamma$【原书这里是$\\gamma_m \\geq \\gamma$，但个人感觉应该是$\\leq$才能推得$exp(-2\\sum_{m=1}^M \\gamma_m^2) \\leq exp(-2M\\gamma^2)$】,则$$\\frac{1}{N}\\sum_{i=1}^NI(G(x_i) \\neq y_i) \\leq exp(-2M\\gamma^2)$$ 因为$$\\frac{1}{N}\\sum_{i=1}^NI(G(x_i) \\neq y_i) \\leq \\prod_mZ_m \\leq exp(-2\\sum_{m=1}^M \\gamma_m^2) \\leq exp(-2M\\gamma^2)$$这也表明在此条件下AdaBoost的训练误差是以指数速率速率下降的 Notes: AdaBoost算法不需要知道下界$\\gamma$ AdaBoost具有适应性，即它能适应弱分类器各自的训练误差率，这也是它的名字Adaptive Boosting的由来 由上述推导以及结论可以发现，AdaBoost的分类误差率的上界恰好是一个指数函数，所以为后续使用指数损失函数作为另一种解读方式来推导(前向分步+指数损失函数)做了铺垫 前向分步算法 可以认为AdaBoost算法是模型为加法模型，损失函数为指数函数，学习算法为前向分步算法时的二类分类学习方法。 基本思想 假设现在在给定训练数据和损失函数$L(y, f(x))$的条件下，要学习一个加法模型$$f(x)=\\sum_{m=1}^{M}\\beta_mb(x;\\gamma_m)$$其中$b(x;\\gamma_m)$为基函数，$\\gamma_m$为基函数的参数，$\\beta_m$为基函数的系数 学习该加法模型的问题可以转化为经验风险极小化即损失函数极小化问题$$min_{\\beta_m, \\gamma_m}\\sum_{i=1}^{N}L(y_i,\\sum_{m=1}^{M}\\beta_mb(x_i;\\gamma_m))$$ 而上述问题是一个需要同时求解从$m=1$到$M$所有参数$\\beta_m, \\gamma_m$的优化问题，较为复杂； 而前向分步算法的思想就是，针对该加法模型，从前到后，每一步仅学习一个基函数及其参数，逐步逼近上述优化目标函数式，如此简化优化的复杂度，即每一步只需优化损失函数$$min_{\\beta, \\gamma}\\sum_{i=1}^{N}L(y_i,\\beta b(x_i;\\gamma))$$ 算法-加法模型+前向分步算法 输入： 训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $ 损失函数$L(y, f(x))$ 基函数集$b(x;\\gamma)$ 输出：加法模型$f(x)$ (1) 初始化$f_0(x)=0$ (2) 对$m=1,2,…,M$ 极小化损失函数$$(\\beta_m, \\gamma_m)=argmin_{\\beta, \\gamma}\\sum_{i=1}^{N}L(y_i,f_{m-1}(x_i)+\\beta b(x_i;\\gamma))$$得到参数$\\beta_m, \\gamma_m$ 更新$$f_m(x)=f_{m-1}(x)+\\beta_m b(x;\\gamma_m)$$ (3) 得到加法模型$$f(x)=f_M(x)=\\sum_{m=1}^M\\beta_mb(x;\\gamma_m)$$ Notes:由上述算法的第(2)部分可以看到，前向分布算法每次在上一个基函数得到的累加模型基础上通过极小化当前基函数与累加模型之和的损失函数值从而得到当前基函数及其参数，然后累加到前面得到的加法模型上，这样一步一步地逼近想要优化的目标函数，最终会得到一个近似的加法模型 前向分步算法与AdaBoost最后，来看一下每一轮样本权值的更新$$f_m(x)=f_{m-1}(x)+\\alpha_mG_m(x)$$因为$\\overline{w_{mi}}=exp(-y_if_{m-1}(x_i))$，对两边乘上$-y_i$并取指数得到$$\\overline{w_{m+1,i}}=\\overline{w_{mi}}exp(-y_i\\alpha_mG_m(x))$$对比AdaBoost中的权值更新步骤$$w_{m+1,i}=\\frac{w_{m,i}}{Z_m}exp(-\\alpha_my_iG_m(x_i))$$仅相差规范化因子，因而等价 Notes: 对于原书中证明求$\\alpha_m^*$的最后$$e_m=\\frac{\\sum_{i=1}^N\\overline{w_{mi}}I(y_i \\neq G_m(x_i))}{\\sum_{i=1}^N\\overline{w_{mi}}}=\\sum_{i=1}^Nw_{mi}I(y_i \\neq G_m(x_i))$$中的后半部分是怎么来的实在理解不了，猜测可能跟AdaBoost训练误差界中的定理以及后面的$\\overline{w_{m+1,i}}=\\overline{w_{mi}}exp(-y_i\\alpha_mG_m(x))$有关 推导的核心还是基分类器在最终分类器中的权重$\\alpha_m$以及样本权重分布这两项的推导 为什么是指数损失函数？参考周志华《机器学习》一书P174(不过这里也只是逆向从如果是指数损失函数的角度来分析为什么可以用指数损失函数，并没有说明起源，比如为什么不用合页损失函数？) 由最终得到的基分类器的线性组合$f(x)=\\sum_{m=1}^M\\alpha_mG_m(x)$ 若$f(x)$能令指数损失函数最小化，即最小化$L(y,f(x))=e^{-yf(x)}$ 对其求偏导得$$\\frac{d(L(y,f(x)))}{df(x)}=-e^{f(x)}P(y=1|x)+e^{f(x)}P(y=-1|x)$$ 令其为0得到$$f(x)=\\frac{1}{2}ln\\frac{P(y=1|x)}{P(y=-1|x)}$$ 于是最终分类器$$sign(f(x))=sign(\\frac{1}{2}ln\\frac{P(y=1|x)}{P(y=-1|x)})$$ 即$$sign(f(x))=1，如果P(y=1|x)&gt;P(y=-1|x)$$,而$$sign(f(x))=-1，如果P(y=-1|x)&gt;P(y=1|x)$$ 也就表示，$sign(f(x))$达到了贝叶斯最优错误率，即若指数损失函数最小化则分类错误率也将最小化，这说明 指数损失函数是分类任务原本0/1损失函数的一致的替代损失函数； 由于该替代函数有更好的数学性质（比如连续可微），因此我们用其替代0/1损失函数作为优化目标（类似的这种情况在SVM里用合页损失函数代替0-1损失函数也出现过） 更多讨论也可以看看知乎，似乎大部分观点认为AdaBoost=boosting+指数损失函数 AdaBoost也可以从优化权值分布之和的角度进行推导，最后看作是“steepest descent with approximate functional gradient”,参见机器学习技法11.4 提升树模型 提升树是以分类树或回归树为基本分类器的提升方法 以决策树为基函数的提升方法称为提升树（一般是用决策树桩(decision stump)即由一个根结点直接连接两个叶节点的简单决策树） 提升树模型可以表示为决策树的加法模型$$f_M(x)=\\sum_{m=1}^{M}T(x;\\varTheta_m)$$其中$T(x;\\varTheta_m)$表示决策树，$\\varTheta_m$为决策树的参数，$M$为树的个数 算法 不同问题的提升树学习算法区别在于使用的损失函数不同 回归树：平方误差（拟合残差） 分类树：指数损失函数 一般决策树：一般损失函数 回归问题的提升树算法前向分步算法 输入：训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace ,x_i \\in R^n, y_i \\in R $ 输出：提升树$f_M(x)$ (1) 初始化$f_0(x)=0$ (2) 对$m=1,2,…,M$ 计算残差$r_{mi}=y_i - f_{m-1}(x_i), i=1,2,…,N$ 拟合残差$r_{mi}$学习一个回归树，得到$T(x;\\varTheta_m)$ 更新$f_m(x)=f_{m-1}(x)+T(x;\\varTheta_m)$ (3) 得到回归问题提升树$$f_M(x)=\\sum_{m=1}^{M}T(x;\\varTheta_m)$$ Notes:对于上述算法的过程理解可以参照原书P149的例子来感受一下，需要回顾决策树（主要是CART算法）的知识点 除了一开始初始化以外，每一步都是拟合上一轮得到的加法模型的残差来寻找最佳切分点并构建决策树桩，然后将该决策树桩累加到加法模型上作为当前模型并计算平方损失误差 当当前模型的平方损失误差降到某一阈值时则当前模型就是最终输出的模型。 梯度提升由来 提升树利用加法模型与前向分步算法实现学习的优化过程，当损失函数为平方损失（回归）和指数损失函数（分类）时，每一步的优化较为简单，但是对一般损失函数而言，往往每一步的优化并不容易，而梯度提升算法则是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值$$-\\left[\\frac{\\partial L(y,f(x_i)}{\\partial f(x_i)} \\right]_{f(x)=f_{m-1}(x)}$$作为回归问题提升树算法中的残差的近似值，拟合一个回归树 算法（回归树） 输入： 训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace ,x_i \\in R^n, y_i \\in R $ 损失函数$L(y,f(x))$ 输出：回归树$\\widehat{f}(x)$ (1) 初始化$$f_0(x)=argmin_c\\sum_{i=1}^NL(y_i,c)$$来估计使损失函数极小化的常数值，它是只有一个根节点的树 (2) 对$m=1,2,…,M$ 对$i=1,2,…,N$,计算$$r_{mi}=-\\left[\\frac{\\partial L(y,f(x_i)}{\\partial f(x_i)} \\right]_{f(x)=f_{m-1}(x)}$$即损失函数的负梯度在当前模型的值，将它作为残差的估计（对于平方损失函数，它就是通常所说的残差；而对于一般损失函数，它就是残差的近似值） 对$r_{mi}$拟合一个回归树，得到第$m$棵树的叶节点区域$R_{mj}, j=1,2,…,J$ 对$j=1,2,…,J$,计算$$c_{mj}=argmin_c\\sum_{x_i \\in R_{mj}}L(y_i, f_{m-1}(x_i)+c)$$即利用线性搜索估计叶节点区域的值，使损失函数极小化 更新回归树$$f_m(x)=f_{m-1}(x)+\\sum_{j=1}^Jc_{mj}I(x \\in R_{mj})$$ (3) 得到最终的回归树$$\\widehat{f}(x)=f_M(x)=\\sum_{m=1}^M\\sum_{j=1}^Jc_{mj}I(x \\in R_{mj})$$ Notes:梯度提升树（回归树）的算法与原始的提升树（回归树）算法的核心区别主要在于残差计算这里，由于原始的回归树指定了平方损失函数所以可以直接计算残差，而梯度提升树针对一般损失函数，所以采用负梯度来近似求解残差。 实现基于《机器学习实战》一书的第7章内容用决策树桩实现了简单的AdaBoost算法 简要叙述了Boosting及其与Bagging的异同 基于决策树桩实现了简单的AdaBoost算法 应用所实现的AdaBoost分类器对马疝病数据集进行分类 此外简要探讨了非平衡数据集的一些问题，包括 精确率、召回率 过采样、欠采样 roc-auc的实现原始代码可以参考我的github 参考资料 《统计学习方法》 《机器学习》 《机器学习实战》 维基百科 知乎 林轩田《机器学习技法》","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"decision tree","slug":"decision-tree","permalink":"http://yoursite.com/tags/decision-tree/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"}]},{"title":"基础算法小记","date":"2017-06-27T02:56:11.000Z","path":"2017/06/27/basic-algorithm/","text":"本文大部分内容基于《算法》4th一书内容，部分补充来自于网络或者其他书籍 查找类二分查找【重点】基本思想在查找时，通过比较中位数与所查找值的大小关系，若找到则返回，若查找失败则相应的调整查找数据集的最高位/最低位的位置来使每次缩减查找范围为原始的一半 Java实现 循环 1234567891011121314151617181920212223242526 //二分查找适用于有序的数组，另外也有一些延伸用法，见剑指offer public static int binarySearch(int[] num, int key)&#123; if(num==null || num.length&lt;=0)&#123; return -1; &#125; int low = 0; //为什么是length-1,因为查找也是从第0位开始的 int high = num.length-1; int pos = -1; while(low &lt;= high)&#123;//注意这里的取等与数组的元素奇偶个数、取边界值、边界以外的值相关，小心 int mid = (low+high) &gt;&gt; 1; if(num[mid] == key)&#123; pos = mid; break; &#125; else if(num[mid] &gt; key)&#123; high = mid - 1; &#125; else&#123; low = mid + 1; &#125; &#125; return pos; //如果是需要在数组中不存在该键时要求返回插入的位置，则修改如下// return low; &#125; 递归实现 123456789101112131415161718192021 public static int binarySearchRecursive(int[] num, int key)&#123; return (num==null||num.length==0)?-1:binarySearchRecursive(num,key,0,num.length-1); &#125; public static int binarySearchRecursive(int[] num, int key, int begin, int end)&#123; if(begin&gt;end)&#123; return -1; //如果是需要在数组中不存在该键时要求返回插入的位置，则修改如下// return begin; &#125; int mid=(begin+end)&gt;&gt;1; if(num[mid]==key)&#123; return mid; &#125; else if(num[mid]&gt;key)&#123; return binarySearchRecursive(num,key,begin,mid-1); &#125; else&#123; return binarySearchRecursive(num,key,mid+1,end); &#125; &#125; 测试 123456789 public static void main(String[] args) &#123; int[] num = &#123;1, 3, 5, 7, 9, 10&#125;;// int[] num = &#123;&#125;; System.out.println(binarySearch(num, 9)); System.out.println(binarySearchRecursive(num, 9)); System.out.println(binarySearch(num, 11)); System.out.println(binarySearchRecursive(num, 0)); &#125; 特点在$N$个键的有序数组中进行二分查找最多需要$logN+1$次比较（无论是否成功），即二分查找所需时间在对数范围之内。 附：符号表（查找）的各种实现的优缺点 符号表的各种实现的优缺点 使用的数据结构 优点 缺点 链表（顺序查找） 适用于小型问题 对于大型数据集很慢 有序数组(二分查找) 最优的查找效率和空间需求，能够进行有序性相关的操作 插入操作很慢 二叉查找树 实现简单，能够进行有序性相关的操作 没有性能上界的保证，此外链接需要额外的空间 平衡二叉查找树（红黑树实现） 最优的查找和插入效率，能够进行有序性相关的操作 链接需要额外的空间 散列表 能够快速的查找和插入常见类型的数据 需要计算各种类型的数据的散列，无法进行有序性相关的操作，此外链接和空结点需要额外的空间 来源于《算法》一书中3.1.7 各种符号表实现的渐进性能总结 算法（数据结构） 查找（最坏情况） 插入（最坏情况） 查找命中（平均情况） 插入（平均情况） 关键接口 顺序查询（无序链表） N N N/2 N equals() 二分查找（有序数组） $lgN$ N $lgN$ N/2 compareTo() 二叉树查找（二叉查找树） N N 1.39$lgN$ 1.39$lgN$ compareTo() 2-3树查找（红黑树） 2$lgN$ 2$lgN$ 1.00$lgN$ 1.00$lgN$ compareTo() 拉链法（链表数组） &lt;$lgN$ &lt;$lgN$ N/2M N/M equals() hashCode() 线性探测法（并行数组） c$lgN$ c$lgN$ &lt;1.5 &lt;2.5 equals() hashCode() N表示N次插入之后，M表示哈希表中的键的个数 Java的java.util.TreeMap和java.util.HashMap分别是基于红黑树和拉链法的散列表的符号表实现 来源于《算法》一书中3.5.1 排序类快速排序【重点】基本思想分治+双指针：核心在于切分(partition)，即首先根据数组中的某一个元素(一般取当前首位元素)，将数组进行切分，最终目的是使该元素左侧元素均不大于该元素，该元素右侧元素均不小于该元素（通过双指针将左侧指针指向较大元素与右侧指针指向较小元素交换实现）；然后进一步递归地对左右子数组调用切分方法直到最终数组有序。 Notes:双指针i,j,保证i左侧元素均不超过pivot,而j右侧元素均不小于pivot，最终i,j相遇的地方即为pivot的位置，所以重点不在于pivot的选取，比如还可以取中间位置或者任意位置，只是需要选定一个pivot来开始进行划分，每次划分都保证了双指针左右的元素的大小符合要求，然后递归的对双指针左右的元素进行切分处理 Java实现 快速排序 1234567891011121314151617181920212223242526272829303132333435363738public static void quickSort(int[] nums)&#123; shuffle(nums); quickSort(nums,0,nums.length-1);&#125;public static void quickSort(int[] nums, int low, int high)&#123; if(low&gt;=high)&#123; return; &#125; int j=partition(nums,low,high); quickSort(nums,low,j-1); quickSort(nums,j+1,high);&#125;public static int partition(int[] nums, int low, int high)&#123; int pivot=nums[low]; //设置分割值为首位的元素值 int i=low+1; int j=high; while(i&lt;=j)&#123; while(i&lt;=j &amp;&amp; nums[i]&lt;pivot)&#123; //找到左侧超过pivot的第一个元素 i++; &#125; while(i&lt;=j &amp;&amp; nums[j]&gt;pivot)&#123; //找到右侧小于pivot的第一个元素 j--; &#125; if(i&lt;=j)&#123; //交换两边的元素 int temp=nums[i]; nums[i]=nums[j]; nums[j]=temp; i++; j--; &#125; &#125; //最后将pivot放置到“中间”位置达成a[low..j-1]&lt;=a[j]&lt;=a[j+1..high] nums[low]=nums[j]; nums[j]=pivot; return j;&#125; 测试及辅助函数 123456789101112131415161718192021public static void randArray(int[] nums) &#123; for(int i=0;i&lt;nums.length;i++)&#123; nums[i]=(int)(Math.random()*100); &#125;&#125;public static void printArray(int[] nums)&#123; for(int i=0;i&lt;nums.length;i++)&#123; System.out.print(nums[i]+\" \"); &#125; System.out.println();&#125;public static void shuffle(int[] nums)&#123; List&lt;Integer&gt; list=new ArrayList&lt;&gt;(); for(int i=0;i&lt;nums.length;i++)&#123; list.add(nums[i]); &#125; Collections.shuffle(list); for(int i=0;i&lt;nums.length;i++)&#123; nums[i]=list.get(i); &#125;&#125; 特点 优点： 实现简单，适用于各种不同的输入数据且在一般应用中比其他排序算法都要快得多； 原地排序（只需要一个很小的辅助栈） 将长度为N的数组排序所需时间和$NlgN$成正比 性能优势 内循环简洁，只涉及元素比较，而像希尔排序、归并排序还涉及元素的移动 比较次数很少 缺点：切分不平衡时程序可能会变得极为低效–&gt; 这也是在快排中一开始对数组进行随机化处理（shuffle()）的原因，目的是为了降低这种情况出现的概率 进一步改进 对于小数组切换到插入排序，修改123if(low&gt;=high)&#123; return;&#125; 为1234if(low+M&gt;=high)&#123; //M为转换参数，与系统相关，一般取5~15之间 insertSort(nums,low,high); return;&#125; 三取样切分：使用数组的一部分元素的中位数来切分数组，代价是需要计算中位数 熵最优的排序——三向切分的快排 针对存在大量重复元素的数组，可将性能提升到线性级别 将数组划分为三个部分，分别对应于小于、等于和大于切分元素的数组元素1234567891011121314151617181920public static void quick3waySort(int[] nums,int low, int high)&#123; if(high&lt;=low)&#123; return; &#125; int lt=low,i=low+1,gt=high; int pivot=nums[low]; while(i&lt;=gt)&#123; //保证nums[low..lt-1]&lt;pivot=nums[lt..gt]&lt;nums[gt+1..high] if(nums[i]&lt;pivot)&#123; //而i在lt与gt之间，循环的过程就是不断调整i与gt之间的元素 exchange(nums,lt++,i++); &#125; else if(nums[i]&gt;pivot)&#123; exchange(nums,i,gt--); &#125; else&#123; i++; &#125; &#125; quick3waySort(nums,low,lt-1); quick3waySort(nums,gt+1,high);&#125; 归并排序【重点】归并 额外数组空间+双指针 先将原始数组复制一份 分别遍历两个子数组，遍历原始数组，逐个比较指针指向的元素和移动指针，将较小的元素复制到原始的数组，若一个子数组指向最后，则将另一个子数组元素逐个复制到原数组中 Java实现123456789101112131415161718192021public static void merge(int[] nums, int low, int mid, int high)&#123; for(int k=low;k&lt;=high;k++)&#123; copy[k]=nums[k]; &#125; int i=low; int j=mid+1; for(int k=low;k&lt;=high;k++)&#123; if(i&gt;mid)&#123;//左子数组元素用尽 nums[k]=copy[j++]; &#125; else if(j&gt;high)&#123;//左子数组元素用尽 nums[k]=copy[i++]; &#125; else if(copy[i]&lt;copy[j])&#123;//否则谁小就填充谁 nums[k]=copy[i++]; &#125; else&#123; nums[k]=copy[j++]; &#125; &#125;&#125; 这里的copy数组在类中一开始显式声明，然后在sort初始部分一次性分配空间初始化1private static int[] copy; //事先声明一个辅助数组 Notes:这里不一开始就复制所有元素到copy数组的原因在于，后续归并时是一个回退的过程，每次都是将前一阶段已经排好序的子数组复制到对应copy部分，然后进一步归并排序，所以如果一开始就复制而后面不复制，后面中间的排序过程就失效了 递归方法（自顶向下） 递归+分治的思想对某一子数组排序，先将其对半分，然后对各部分分别递归调用排序，最后将有序的子数组归并为最终的排序结果 Java实现12345678910111213141516public static void mergeSort(int[] nums)&#123; copy=new int[nums.length]; mergeSort(nums,0,nums.length-1);&#125;private static void mergeSort(int[] nums, int low, int high) &#123; if(low&gt;=high)&#123; return; &#125; int mid=(low+high)&gt;&gt;1; mergeSort(nums,low,mid); mergeSort(nums,mid+1,high); if(nums[mid]&gt;nums[mid+1])&#123; merge(nums,low,mid,high); &#125;&#125; Notes:sort方法的作用在于安排多次merge方法调用的正确顺序，只有子数组有序了，merge方法才是有效的 归并排序的时间复杂度分析 对于长度为N的任意数组，自顶向下的归并排序需要$\\frac{1}{2}NlgN$至$NlgN$次比较 如图，以$N$表示数组元素数目，n表示树的层级则有$n=lgN$,每个节点都表示一个mergeSort()方法通过merge()方法归并而成的子数组，第$k$层有$2^k$个子数组，每个数组的长度为$2^{n-k}$，所以归并最多需要$2^{n-k}$次比较，因此每层的比较次数为$2^k\\times 2^{n-k}=2^n$,$n$层总共是$n2^n=NlgN$ 自底向上（非递归） 先归并微型数组，然后再成对归并得到的子数组，直到将整个数组归并在一起 Java实现12345678public static void mergeSort2(int[] nums)&#123; copy=new int[nums.length]; for(int sz=1;sz&lt;nums.length;sz*=2)&#123; //每轮遍历倍增归并的子数组大小 for(int low=0;low&lt;nums.length;low+=(2*sz))&#123; //成对归并 merge(nums,low,low+sz-1,Math.min(low+2*sz-1, nums.length-1));//最后一组子数组大小可能小于对半分的子数组 &#125; &#125;&#125; 自底向上的归并排序比较适合用链表组织的数据，这种方法只需要重新组织链表链接就能将链表原地排序 特点 优点：能保证将任意长度为$N$的数组排序所需时间和$NlgN$成正比，所以能使用归并排序处理数百万乃至更大规模的数组 缺点：他所需的额外空间与$N$成正比 归并排序是一种渐近最优的基于比较排序的算法：归并排序在最坏情况下的比较次数和任意基于比较的排序算法所需的最少比较次数都是$~NlgN$ 进一步改进的方向 对小规模子数组用插入排序 不将元素复制到辅助数组，而是在递归调用的每个层次交换输入数组和辅助数组的角色 堆排序【重点】基本思想根据《算法》一书的实现方法，堆排序是使用了一种特殊的数据结构——优先队列从而实现快速获取数组中的最大或者最小值来完成排序。 优先队列：两个核心操作：插入元素&amp;获取最大元素，在书中是使用了二叉堆（数组实现完全二叉树的数据结构）来实现的 基本思路是先将传入的数组处理成有序堆，然后逐个取最大元素（根节点）并恢复对有序的结构直到所有元素均被处理完 堆有序：当一棵二叉树的每个节点都大于或等于它的两个子节点时，被称为堆有序 二叉堆是一组能够用堆有序的完全二叉树排序的元素，并在数组中按照层级存储 Notes: 简言之，主要操作是堆的构建和每次取最大元素以后（堆结构局部破坏）调整和恢复堆结构的操作，也就是sink()方法 从下标0开始存储在数组中的完全二叉树有如下特点，对于二叉树的某一内部结点，假设其下标为k，则其父节点下标为(k-1)/2,两个子节点（若存在）的下标节点分别为2k+1和2k+2,所以我们可以根据下标很方便的交换元素来构建堆以及恢复堆 《算法》一书中给出的是从下标1开始的数组，把第0位作为哨兵在其他应用里会有用，这样的话父节点，子节点相应的需要调整为k,2k和2k+1，这里主要考虑到一般传进来的数组是从0开始的，为了避免需要开辟额外的空间建堆所以进行了修改，使其原地排序 Java实现 堆排序12345678910111213141516171819202122232425262728293031323334public static void heapSort(int[] nums)&#123; int N=nums.length-1; //构建堆 for(int k=(N-1)/2;k&gt;=0;k--)&#123; sink(nums,k,N); &#125; //重复取最大元素并恢复堆结构 while(N&gt;0)&#123; exchange(nums, 0, N--); sink(nums,0,N); &#125;&#125;private static void sink(int[] nums, int p, int N) &#123; //自上而下调整 while(p*2+1&lt;=N)&#123; int temp=p*2+1; if(temp+1&lt;=N &amp;&amp; nums[temp]&lt;nums[temp+1])&#123;//取子节点中较大的一个元素 temp+=1; &#125; if(nums[p]&gt;=nums[temp])&#123; //此时堆有序，无需调整，结束循环 break; &#125; //否则元素下沉 exchange(nums,p,temp); p=temp; &#125; &#125;public static void exchange(int[] nums, int i, int j)&#123; int temp=nums[i]; nums[i]=nums[j]; nums[j]=temp;&#125; 特点 堆排序能同时最优地利用空间和时间，即使在最坏的情形下也能保证使用$~2NlgN$次比较和恒定的额外空间 在空间比较紧张的开发中应用广泛(比如嵌入式系统)，但是现代操作系统中较少使用，主要是他无法利用缓存，数组元素很少与相邻的其他元素进行比较，所以缓存未命中的次数要远远高于大多数比较都在相邻元素间进行的算法，比如快排、归并排序甚至希尔排序 但另一方面，用堆实现的优先队列的应用却越来越重要，因为它能在插入操作和删除最大元素操作混合的动态场景中保证对数级别的运行时间，另外常考算法比如topM和MultiWay(特别针对大数据、数据流) 进一步优化可以通过先下沉后上浮来来免去检查元素是否到达正确位置的时间，但是这种方法需要额外的空间 优先队列的应用延伸TopM【考点】问题描述，参照geeksforgeeks 打印一个无序数组中最大的M个元素 思路概述 部分排序，比如修改冒泡排序，执行M次，然后将数组末端的M个元素打印出来（也可以使用选择排序等，思路类似），时间复杂度为$O(NM)$ 使用一个小型的缓存数组，将原数组中的M个元素先保存到一个大小为M的小数组中，然后查找其中的最小值记为min，接下来遍历原始数组剩余部分，遇到比min大的元素则从小数组中去掉min元素，插入新元素并重新查找到当前最小元素，时间复杂度为$O((N-M)*M+MlgM)$,前一项为遍历以及插入元素以后更新（查找）当前最小元素的时间消耗，后一项则是最后要按顺序打印的小数组排序时间复杂度 直接对原始数组进行排序，时间复杂度一般为$O(NlgN)$，但是这种方法对于数据量较大的情况是不合适的 使用大顶堆存储原始数组，这样构建大顶堆的时间为$O(N)$(参见《算法》P206命题R用下沉法构建堆)，构建好堆以后重复取M次堆顶元素的时间复杂度为$O(MlgN)$，因为涉及到每次取最大元素以后调整堆的操作，所以总体时间复杂度为$O(N+MlgN)$ 有序统计【有待进一步分析】 使用有序统计算法（order statistic algorithm）来找到第k大的元素，时间复杂度为$O(N)$ 使用快速排序的切分算法来将前一步找到的第k大元素作为pivot切分点对数组进行切分，时间复杂度为O(N) 最后对与小于切分点的元素排序输出$O(MlgM)$ 总体时间复杂度为$O(N+MlgM)$ 使用小顶堆【重点掌握】，该方法是对第二种方法的一个改进，即不使用暂存数组，而是使用一个大小为M的小顶堆来暂存数据，这样节省查找时间 使用前M个元素构建一个小顶堆，时间复杂度$O(M)$ 对后续每一个元素，将元素与堆顶元素比较，若比堆顶元素大，则交换并下沉调整堆，时间复杂度为$O((N-M)lgM)$ 最后对小顶堆排序输出$O(MlgM)$总体时间复杂度$O(M+(N-M)lgM+MlgM)$ Java实现 针对小顶堆的方法，不过这里进行了微调，在后续插入数据时采取的方法是移除当前最小值然后插入元素调整堆的方法，可以修改delMin()方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135import java.util.Scanner;//用小顶堆实现从数组中找出其中最大的M个元素public class TopM &#123; public static void main(String[] args) &#123; Scanner in=new Scanner(System.in); if(in.hasNext())&#123; int cases=in.nextInt(); while(cases--&gt;=0 &amp;&amp; in.hasNext())&#123; int N=in.nextInt(); int M=in.nextInt(); int[] total=new int[N]; int[] heap=new int[M]; for(int i=0;i&lt;total.length;i++)&#123; total[i]=in.nextInt(); &#125; topM(total,heap); printArray(heap); &#125; &#125; in.close(); &#125; public static void topM(int[] total, int[] top)&#123; int N=total.length; int M=top.length; for(int i=0;i&lt;N;i++)&#123; if(i&lt;M)&#123; insert(top,i,total[i]); &#125; else&#123; if(total[i]&gt;getMin(top))&#123; delMin(top); insert(top,i,total[i]); &#125; &#125; &#125; heapSort(top); &#125; private static int getMin(int[] top) &#123; if(top.length==0)&#123; return -1; &#125; return top[0]; &#125; private static void delMin(int[] top) &#123; int M=top.length; exchange(top,0,M-1); sink(top); &#125; private static void sink(int[] top) &#123; int M=top.length; int i=0; while(2*i+2&lt;=M-1)&#123; int left=2*i+1; if(left+1!=M-1&amp;&amp;top[left+1]&lt;top[left])&#123; left++; &#125; if(top[i]&lt;=top[left])&#123; break; &#125; exchange(top,i,left); i=left; &#125; &#125; private static void insert(int[] top,int index, int key) &#123; int M=top.length; if(index&gt;=M-1)&#123; index=M-1; &#125; top[index]=key; swim(top,index); &#125; private static void swim(int[] top, int index) &#123; while(index&gt;0)&#123; int parent=(index-1)/2; if(top[parent]&gt;top[index])&#123; exchange(top,index,parent); index=parent; &#125; else&#123; break; &#125; &#125; &#125; private static void exchange(int[] nums, int i, int j) &#123; int temp=nums[i]; nums[i]=nums[j]; nums[j]=temp; &#125; //构建小顶堆排序 public static void heapSort(int[] nums)&#123; int N=nums.length-1; //构建堆 for(int k=(N-1)/2;k&gt;=0;k--)&#123; sink(nums,k,N); &#125; //重复取最大元素并恢复堆结构 while(N&gt;0)&#123; exchange(nums, 0, N--); sink(nums,0,N); &#125; &#125; private static void sink(int[] nums, int p, int N) &#123; //自上而下调整 while(p*2+1&lt;=N)&#123; int temp=p*2+1; if(temp+1&lt;=N &amp;&amp; nums[temp]&gt;nums[temp+1])&#123;//取子节点中较小的一个元素 temp+=1; &#125; if(nums[p]&lt;=nums[temp])&#123; //此时堆有序，无需调整，结束循环 break; &#125; //否则元素下沉 exchange(nums,p,temp); p=temp; &#125; &#125; public static void printArray(int[] nums)&#123; for(int i=0;i&lt;nums.length;i++)&#123; System.out.print(nums[i]+\" \"); &#125; System.out.println(); &#125;&#125; 从数据流中得到当前的k个最大的元素（默认数据流为无限） 这样的话上述不少方法就不能使用了，常见思路如下 维持一个大小为k的有序数组，这样和前面一样，针对后续读取的每一个元素，比当前数组中的首位元素（最小元素）小的元素忽略，大的则删除当前数组最小元素并插入该元素 使用大小为k的平衡二叉查找树来存储元素，当然要求是每次插入元素后得调整树结构保持树的平衡 小顶堆的做法，与前一个问题一致 《剑指offer》第二版一书P209面试题40也有关于这个问题的讨论 Multiway（多向归并） 将M个有序输入流归并为一个有序的输出流(《算法》中文版2.4.4.7 P205) 基本思想 维持一个大小为M的小顶(索引)堆，存储来自不同输入流的索引 先逐个遍历输入流将每个输入流的首位元素（最小元素）插入到堆中，并与对应的索引关联 此时输出的最小元素即为所有输入流的最小元素，每输出一个最小元素，即在输出元素索引对应的输入流中读入下一个元素（如果存在）并插入堆中，这样就能保证每次输出的都是所有输入流中的最小元素 PrimDijkstra霍夫曼压缩选择排序基本思想 不断地选择剩余元素中的最小者并与当前剩余元素第一位交换首先，找到数组中最小的那个元素，其次将它和数组的第一个元素交换位置，再次，在剩下的元素中找到最小的元素，将它与数组的第二个元素交换位置，如此往复，直到将整个数组排序。 Java实现 选择排序 12345678910111213public static void selectSort(int[] nums)&#123; for(int i=0;i&lt;nums.length-1;i++)&#123; int minPos=i; for(int j=i+1;j&lt;nums.length;j++)&#123; if(nums[minPos]&gt;nums[j])&#123; minPos=j; &#125; &#125; int temp=nums[i]; nums[i]=nums[minPos]; nums[minPos]=temp; &#125;&#125; 测试 12345678910111213public static void main(String[] args) &#123; int[] nums=&#123;7,5,1,9,3&#125;; printArray(nums); selectSort(nums); printArray(nums);&#125;public static void printArray(int[] nums)&#123; for(int i=0;i&lt;nums.length;i++)&#123; System.out.print(nums[i]+\" \"); &#125; System.out.println();&#125; 特点 对于长度为$N$的数组，选择排序需要大约$N^2/2$次比较和$N$次交换 运行时间和输入无关（缺点），比如元素相等的数组/已经有序的数组与随机数组的排序时间一样长 数据移动是最少的，即元素交换次数是最少的，每次交换只改变两个数组元素的值，即交换次数和数组的大小呈线性关系，而其他的大部分排序算法是线性对数或者平方级别 插入排序基本思想 整理桥牌 保证数组当前遍历的元素位置左方元素已经有序，然后将当前元素前向逐个比较，一边比较一边将前面的元素向后覆盖，直到找到第一个不大于该元素的位置，将该元素插入（《算法》一书中的基本实现采用的方法是将目标元素与前面比他大的元素逐个交换的做法，相较之下，我的这种直接移动所有较大元素的做法速度更快（《算法》一书作为练习题）） Java实现 插入排序1234567891011public static void insertSort(int[] nums)&#123; for(int i=0;i&lt;nums.length-1;i++)&#123; int temp=nums[i+1]; int j=i+1; while(j&gt;0&amp;&amp;temp&lt;nums[j-1])&#123; nums[j]=nums[j-1]; j--; &#125; nums[j]=temp; &#125;&#125; 特点 插入排序所需时间取决于输入中元素的初始顺序，因此插入排序对于部分有序的数组十分高效，也很适合小规模数组（当数组中倒置（倒置指的是数组中两个顺序颠倒的元素）数量很少时，插入排序可能是最快的排序算法） 希尔排序基本思想 考虑到插入排序不适用于大规模乱序数组，所以进行改进得到的算法 原则是保证数组中任意间隔为h的元素有序，即交换间隔为h的元素以对数组的局部进行排序，并最终使用插入排序将局部有序的数组排序 权衡了子数组的规模和有序性：排序之初，各个子数组都很短，排序之后子数组都是部分有序的，这两种情况都很适合插入排序 Java实现 希尔排序 123456789101112131415161718public static void shellSort(int[] nums)&#123; int h=1; while(h&lt;nums.length/3)&#123; h=h*3+1; &#125; while(h&gt;=1)&#123; for(int i=0;i&lt;nums.length-h;i++)&#123; int temp=nums[i+h]; int j=i+h; while(j&gt;=h&amp;&amp;temp&lt;nums[j-h])&#123; nums[j]=nums[j-h]; j-=h; &#125; nums[j]=temp; &#125; h/=3; &#125;&#125; 测试 12345678910111213141516171819public static void printArray(int[] nums)&#123; for(int i=0;i&lt;nums.length;i++)&#123; System.out.print(nums[i]+\" \"); &#125; System.out.println();&#125;public static void randArray(int[] nums) &#123; for(int i=0;i&lt;nums.length;i++)&#123; nums[i]=(int)(Math.random()*100); &#125;&#125;public static void main(String[] args) &#123; int[] nums=new int[20]; randArray(nums); printArray(nums); shellSort(nums); printArray(nums);&#125; 特点 算法性能取决于h值的选择，这里使用的是序列$\\frac{3^k-1}{2}$，关于怎样的h值最好，尚无定论，实际应用中相差不大，但一般其时间复杂度不会到达$O(n^2)$ 希尔排序也可用于大型数组，其优点在于代码量小而且不需要额外的存储空间，在手头没有现成的排序算法可以调用时，不妨实现希尔排序来辅助工作 冒泡排序基本思想 用一个‘指针’遍历数组并比较与该指针指向元素相邻的元素，如果相邻元素呈逆序状态则交换元素，直到移动到数组末端即完成一轮冒泡排序，然后重复上述操作，一般会设置一个flag来在数组已经有序时（即该轮排序没有发生交换）结束排序 另外一种思路，由于每一次冒泡排序可以保证最大的元素‘沉到’最后一位，所以也可以不设置flag，而使用双指针，后一个指针的遍历范围逐轮缩小其中第一个设置flag的方案更优，因为第二种方法即使在数组有序时也会循环遍历两次 Java实现 flag方法 1234567891011121314151617181920public static void bubbleSort(int[] nums)&#123; while(true)&#123; boolean flag=true; //标记该轮排序是否没有发生交换操作（若是则表明当前数组已有序） for(int i=0;i&lt;nums.length-1;i++)&#123; if(nums[i+1]&lt;nums[i])&#123; flag=false; exchange(nums,i,i+1); &#125; &#125; if(flag)&#123; break; &#125; &#125;&#125;private static void exchange(int[] nums, int i, int j) &#123; int temp=nums[i]; nums[i]=nums[j]; nums[j]=temp;&#125; 双指针 123456789public static void bubbleSort2(int[] nums)&#123; for(int i=0;i&lt;nums.length-1;i++)&#123; for(int j=0;j&lt;nums.length-i-1;j++)&#123; if(nums[j]&gt;nums[j+1])&#123; exchange(nums,j,j+1); &#125; &#125; &#125;&#125; 特点 数组有序时冒泡排序只需遍历一次数组，但是若当数组完全逆序时，则时间复杂度达到$O(n^2)$，这点和插入排序类似 排序算法小结 算法 是否稳定 是否为原地排序 时间复杂度 空间复杂度 备注 选择排序 否 是 $N^2$ 1 - 插入排序 是 是 介于$N$和$N^2$之间 1 取决于输入元素的排列情况 希尔排序 否 是 $NlgN?$,$N^{\\frac{6}{5}}$ 1 - 快速排序 否 是 $NlgN$ lgN 运行效率由概率提供保证 三向快速排序 否 是 介于$N$和$NlgN$之间 $lgN$ 运行效率由概率保证，同时也取决于输入元素的分布情况 归并排序 是 否 $NlgN$ $N$ - 堆排序 否 是 $NlgN$ 1 - 冒泡排序 是 是 介于$N$和$N^2$之间 1 取决于输入元素的排列情况 来源于《算法》一书中2.5.2 稳定性 如果一个排序算法能够保留数组中重复元素的相对位置则可以被称为是稳定的 稳定排序算法：插入排序、归并排序、冒泡排序 另外有很多办法将任意排序算法变成稳定的，不过比较复杂 原地排序 除了函数调用所需的栈和固定数目的实例变量之外无需额外内存的原地排序算法 原地排序算法：上述算法中除了归并排序以外都是 对于Java而言，java.util.Arrays.sort实际上代表了一系列排序方法 每种原始数据类型都有一个不同的排序方法 一个适用于所有实现了Comparable接口的数据类型的排序方法 一个适用于所有实现了比较器Comparator的数据类型的排序方法 一般而言，Java系统程序员选择对原始数据类型使用（三向切分的）快速排序，对引用类型使用归并排序 Notes:关于排序算法稳定性以及其他更多的排序算法，可以参考这里 排序的应用延伸在线性对数时间内计算两组排列之间的Kendall tau距离关键字：逆序对、归并排序 题目描述 数组中的逆序对在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007题目保证输入的数组中没有的相同的数字数据范围：对于%50的数据,size&lt;=10^4对于%75的数据,size&lt;=10^5对于%100的数据,size&lt;=2*10^5例如，输入：1,2,3,4,5,6,7,0; 输出：7 基本思路归并排序的变体，主要变化在于修改归并操作的复制数组部分，原始的是顺序比较和复制，而此处需要从末尾开始比较两个子数组元素值，记录逆序情况并复制到原始数组 Java实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public class InversePairs &#123; static int[] copy; public static void main(String[] args) &#123; int[] a=&#123;7,5,1,9,3&#125;; int[] a2=&#123;7,5,6,4&#125;; int[] a3=&#123;1,2,3,4,5,6,7,0&#125;; System.out.println(InversePairs(a)); System.out.println(InversePairs(a2)); System.out.println(InversePairs(a3)); &#125; public static int InversePairs(int [] array) &#123; if(array==null || array.length==0)&#123; return 0; &#125; return mergeSort(array)%1000000007; &#125; public static int mergeSort(int[] array)&#123; if(array==null || array.length==0)&#123; return 0; &#125; copy=new int[array.length]; int low=0,high=array.length-1; int[] cnt=new int[1]; cnt[0]=0; mergeSort(array,low,high,cnt); return cnt[0]; &#125; private static void mergeSort(int[] array, int low, int high, int[] cnt) &#123; if(low&gt;=high)&#123; return; &#125; int mid=low+(high-low)/2; cnt[0]%=1000000007; mergeSort(array,low,mid,cnt); cnt[0]%=1000000007; mergeSort(array,mid+1,high,cnt); cnt[0]%=1000000007; if(array[mid]&gt;array[mid+1])&#123; cnt[0]+=mergeAndCnt(array,low,mid,high); &#125; &#125; //修改过的merge方法，除了进行归并操作外还会统计范围内的逆序对数目 public static int mergeAndCnt(int[] array,int low,int mid,int high)&#123; if(low&gt;=high)&#123; return 0; &#125; for(int k=low;k&lt;=high;k++)&#123; copy[k]=array[k]; &#125; int i=mid,j=high; int cnt=0; for(int k=high;k&gt;=low;k--)&#123; if(i&lt;low)&#123; // 左子数组耗尽 array[k]=copy[j--]; &#125; else if(j&lt;mid+1)&#123; //若右子数组耗尽 array[k]=copy[i--]; &#125; else&#123; if(copy[i]&gt;copy[j])&#123; cnt+=(j-mid); //只有左子数组元素比右子数组元素大时才计算逆序对 if(cnt&gt;=1000000007)&#123; cnt%=1000000007; &#125; array[k]=copy[i--]; &#125; else&#123; array[k]=copy[j--]; &#125; &#125; &#125; return cnt; &#125;&#125; 至于为什么是对1000000007取模？参考 找出一组元素的中位数快速排序的partition的进一步应用 基本思想partition+二分查找：由于对原始数组进行一次切分可以得到数组中的一个索引k，而且此时的数组中该位置左边的元素都不超过该元素，右边的都不小于该元素，即k是第k+1大的元素（从1开始计）如果此时k恰为数组大小的一半，那么这个位置的元素就恰好是数组的中位数，如果不是，则采取二分查找的思想，在该索引的某一边进行进一步切分（比如k小于len(array)/2就在k的右侧进行切分），经过有限次切分就可以找到中位数对应的元素。 Notes:本题的变体是寻找数组中出现次数超过一半的数字 最长重复子字符串特殊的字符串排序算法 字符串问题atoi(Str2Int)将字符串转换为整形这道题，LeetCode上的要求要更严一点，不过有些地方确实不一定合理了，主要需要注意以下的一些细节问题 字符串是否合法 开头是否为符号位以及相应的符号处理(以及仅有一位符号位) 字符串中是否存在非数字字符以及相应处理 字符串转化后的数溢出，整型范围为$-2^31~2^31-1$，特别要注意整数比负数又要少一位，所以还需要考虑符号位 字符串开头的0 字符串前后的空格 附leetcode上的一个相对比较好的解答 按照leetcode的要求这里是将合法数字后续的部分忽略，前后空格忽略，前面的0忽略 溢出的值分别输出对应的整型最大值或者最小值 其余非法情况输出0 12345678910111213141516171819202122232425public static int myAtoi(String str) &#123; int i = 0; str = str.trim(); char[] c = str.toCharArray(); int sign = 1; if (i &lt; c.length &amp;&amp; (c[i] == '-' || c[i] == '+')) &#123; if (c[i] == '-') &#123; sign = -1; &#125; i++; &#125; int num = 0; int bound = Integer.MAX_VALUE / 10; while (i &lt; c.length &amp;&amp; c[i] &gt;= '0' &amp;&amp; c[i] &lt;= '9') &#123; int digit = c[i] - '0'; if (num &gt; bound || (num == bound &amp;&amp; digit &gt; 7)) &#123; return sign == 1 ? Integer.MAX_VALUE : Integer.MIN_VALUE; &#125; num = num * 10 + digit; i++; &#125; return sign * num;&#125; 动态规划贪心回溯法位运算参考资料 Robert Sedgewick《算法》4th http://www.algolist.net/Algorithms/Sorting/Quicksort 排序算法稳定性 geeksforgeeks","tags":[{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"algorithm","slug":"algorithm","permalink":"http://yoursite.com/tags/algorithm/"},{"name":"校招","slug":"校招","permalink":"http://yoursite.com/tags/校招/"},{"name":"面试","slug":"面试","permalink":"http://yoursite.com/tags/面试/"},{"name":"Coding","slug":"Coding","permalink":"http://yoursite.com/tags/Coding/"}]},{"title":"京东JData算法大赛","date":"2017-06-25T06:04:31.000Z","path":"2017/06/25/JData/","text":"比赛简介 本次大赛以京东商城真实的用户、商品和行为数据（脱敏后）为基础，参赛队伍需要通过数据挖掘的技术和机器学习的算法，构建用户购买商品的预测模型，输出高潜用户和目标商品的匹配结果，为精准营销提供高质量的目标群体。同时，希望参赛队伍能通过本次比赛，挖掘数据背后潜在的意义，为电商用户提供更简单、快捷、省心的购物体验。 这是2017年京东举办的一场数据挖掘比赛，比赛地址，吸引了不少人参加，和实验室的两个同学也一起参加进来体验一下真实比赛的氛围，总的来说整个过程确实比较辛苦，不过也算是痛并快乐着。看过赛题的朋友可能大多会和往年的阿里移动推荐大赛作比较，甚至更早一点的天猫推荐大赛比较，总的来说，确实很相像，目的都是以用户的历史行为记录以及商品、用户的一些特征来构建模型，预测用户未来的购买状况。 我们团队的最终比赛成绩如下，A榜排名139/4240，B榜排名97/4240 关于比赛的一点思考推荐算法？首先针对题目的目的需要明确一点，这并不是如比赛标题所言的那种构建推荐系统类型的比赛，关于这一点，我想引用网上大牛的原话来说明 首先引用阿里移动推荐比赛内部赛的冠军算者最后获胜时的文章中的内容 1、代价不同。你推荐给我一首歌，不好听，大不了换一首，不好听，再换，最后总能找到自己喜欢的歌。可是购物呢，你推荐我一个服装品牌，我花了半个月的伙食费买了，等待了几天，到货后发现不满意，怎么办？扔掉？太可惜了！穿在身上？天天别扭！退掉？还要再花点邮费！不管结局怎们样，总之不爽。如果是买个家居什么的，不喜欢的话，可能后悔一辈子。所以购物的用户体验周期要远长于音乐、影视推荐。推荐的试错成本很大。2、需求的单一性与喜好的相似性。喜欢看动作片，那么相似的动作片我都能看一遍。但是购物就不同了，购物更多的是刚需，喜欢可爱的衣服，一般人也不会把所有喜欢的可爱的衣服都买下来，更多的是每个季节只买一件。家居类的频次就低了，可能一辈子就买一次。不同的类目的需求频率是不一样的。 另一个是天猫推荐算法大赛的top9的获奖感言里提到的 和很多拿到这个问题的同学一样，一开始都觉得是推荐问题，那么通过查阅相关资料自然而然的都是协同过滤等相关推荐算法，实际效果可想而知。具体原因我认为是品牌的转移成本，具体是什么意思呢？打个比方，一个普通的消费者，前三个月某一天购买了苹果的一款手机，根据商品相似等算法我们可能得到苹果和三星很相似，然后在第四个月系统向他推荐了三星，从一个消费者的正常心理来看，这种推荐很难促成交易的发生。也许这个比方不太恰当，但是这个例子反映的信息包括品牌的实际周期，用户对品牌的黏度，消费能力，还有时间季节等因素，我们仅仅通过赛题给出的字段很难分析出这些因素，而这些都是反映品牌的转移成本的关键因素，因为购买行为的发生不是一个单步，而是涉及到很多前因后果以及最终的整体相关性的问题。基于以上考虑，实际上留给我们的就是挖掘买买相关，看买相关的这部分信息，从这点出发这次的问题实际上就是一个有监督的二分类问题。 通过以上的观点可以看到本题并非协同过滤算法那种构建推荐系统的比赛，而是一种根据离线历史行为记录预测用户未来某段时间买了又买的可能性 与以往比赛的区别抛开数据量等的区别，本次比赛与天猫、阿里推荐算法大赛的最主要的区别在于本次比赛并非预测未来某一天的可能发生购买行为的用户商品对，而是预测未来5天可能发生购买行为的用户商品对，如果说预测未来一天可能发生购买行为的用户商品对难以捕捉到当天购买（冲动消费等）的用户行为，那么把时间跨度加大到5天的话其实我们就间接地损失了不少可能预测出的用户商品对： 首先根据我们的统计结果，大部分发生购买行为的用户商品对的交互在前三~五天，这也是在分析以前的一些比赛的ppt里提到的时间衰减规律的类似结论 此外，取某一个时间区间发生购买行为的用户商品对分析用户对该商品的交互行为，发现在这五天再往前取三天左右的时间交互的用户商品对仅占所有发生购买行为的用户商品对的约四分之一，仅用户的话能占到约一半，但是其中有四分之一的用户在这三天交互的对象并不是后五天购买的了的目标对象，而剩下的一半用户在这三天根本没有交互行为；而究其原因就是预测后五天的用户商品对购买情况的话，这五天对于我们而言就是一个黑匣子，而统计结果却显示我们需要目标时间点最近三天的交互状况，所以造成了预测的困难，后期也曾想过使用类似关联分析之类的方法，然而受精力时间限制也没尝试，所以这也是一个遗憾，不知道那些大牛是怎么处理的。 接下来我就简单的从三个步骤来介绍下我们团队在这场比赛所做的一些尝试 数据分析和预处理 特征工程 模型设计和评估 数据分析和预处理官方信息 符号定义： S：提供的商品全集； P：候选的商品子集，P是S的子集； U：用户集合； A：用户对S的行为数据集合； C：S的评价数据。 数据 按用途：* 训练数据（2016-2-1~2016-4-15）：用户集合U中的用户，对商品集合S中部分商品的行为、评价、用户数据；提供部分候选商品的数据P。 * 预测数据（2016-4-16~2016-4-20）：用户是否下单P(P为候选商品集)中的商品，**每个用户只会下单一个商品** 按类别 用户：ID,年龄，性别，用户等级，用户注册日期 商品：商品编号，属性1，属性2，属性3，品类ID，品牌ID 评价：商品编号，累计评论数（分段），是否有差评，差评率 行为：用户编号，商品编号，行为时间，点击模块编号，类型（浏览，加入购物车，购物车删除，下单，关注，点击），品类ID，品牌ID 注： 自行组成特征和数据格式，自由组合训练测试数据比例 预测数据分A,B榜计算分数 数据均采样和脱敏，存在空值 预测用户在未来5天内，对某个目标品类下商品的购买意向。对于训练集中出现的每一个用户，参赛者的模型需要预测该用户在未来5天内是否购买目标品类下的商品以及所购买商品的SKU_ID。 评分标准：所有用户在2016-04-16到2016-04-20是否下单P中的商品 是否下单： F11=6*Recall*Precise/(5*Recall+Precise) 若下单，预测下单的商品ID是否正确： F12=5*Recall*Precise/(2*Recall+3*Precise) Score=0.4*F11 + 0.6*F12 Precise为精确率，Recall为召回率 FAQ中的一些信息 提供的数据文件编码格式为GBK 一个用户（user_id）只购买一个所提供的候选商品集合P中的商品（sku_id）；用户购买候选商品集合P之外的商品无需提交 评论数据为截止到当日的累计数据 model_id表明用户在页面上点击了哪一个位置；数据中可能存在一些空值，有可能是异常值，也有可能是在页面上点击了一个空白的位置产生的数据,自行理解并处理 集合P为候选商品集合，即参赛者预测的结果中的sku需要在集合P中；集合S为行为数据中的商品全集； 日期格式统一为”yyyy-mm-dd”，时间格式统一为”yyyy-mm-dd hh:mi:ss” 关于用户表中存在10条记录，用户注册时间晚于2016年4月15日情况为真实生产环境中的异常数据,自行理解并处理 数据集解析这里涉及到的数据集是京东最新的数据集： JData_User.csv 用户数据集 105,321个用户 JData_Comment.csv 商品评论 558,552条记录 JData_Product.csv 预测商品集合 24,187条记录 JData_Action_201602.csv 2月份行为交互记录 11,485,424条记录 JData_Action_201603.csv 3月份行为交互记录 25,916,378条记录 JData_Action_201604.csv 4月份行为交互记录 13,199,934条记录 异常值判断12345%matplotlib inlineimport matplotlibimport matplotlib.pyplot as pltimport numpy as npimport pandas as pd 123456789#定义文件名ACTION_201602_FILE = \"data/JData_Action_201602.csv\"ACTION_201603_FILE = \"data/JData_Action_201603.csv\"ACTION_201604_FILE = \"data/JData_Action_201604.csv\"COMMENT_FILE = \"data/JData_Comment.csv\"PRODUCT_FILE = \"data/JData_Product.csv\"USER_FILE = \"data/JData_User.csv\"USER_TABLE_FILE = \"data/User_table.csv\"ITEM_TABLE_FILE = \"data/Item_table.csv\" 数据背景信息根据官方给出的数据介绍里，可以知道数据可能存在哪些异常信息 用户文件 用户的age存在未知的情况，标记为-1 用户的sex存在保密情况，标记为2 后续分析发现，用户注册日期存在系统异常导致在预测日之后的情况，不过目前针对该特征没有想法，所以不作处理 商品文件 属性a1,a2,a3均存在未知情形，标记为-1 行为文件 model_id为点击模块编号，针对用户的行为类型为6时，可能存在空值 空值判断12345678910def check_empty(file_path, file_name): df_file = pd.read_csv(file_path) print 'Is there any missing value in &#123;0&#125;? &#123;1&#125;'.format(file_name, df_file.isnull().any().any()) check_empty(USER_FILE, 'User')check_empty(ACTION_201602_FILE, 'Action 2')check_empty(ACTION_201603_FILE, 'Action 3')check_empty(ACTION_201604_FILE, 'Action 4')check_empty(COMMENT_FILE, 'Comment')check_empty(PRODUCT_FILE, 'Product') Is there any missing value in User? True Is there any missing value in Action 2? True Is there any missing value in Action 3? True Is there any missing value in Action 4? True Is there any missing value in Comment? False Is there any missing value in Product? False 由上述简单的分析可知，用户表及行为表中均存在空值记录，而评论表和商品表则不存在，但是结合之前的数据背景分析商品表中存在属性未知的情况，后续也需要针对分析，进一步的我们看看用户表和行为表中的空值情况 123456789def empty_detail(f_path, f_name): df_file = pd.read_csv(f_path) print 'empty info in detail of &#123;0&#125;:'.format(f_name) print pd.isnull(df_file).any()empty_detail(USER_FILE, 'User')empty_detail(ACTION_201602_FILE, 'Action 2')empty_detail(ACTION_201603_FILE, 'Action 3')empty_detail(ACTION_201604_FILE, 'Action 4') empty info in detail of User: user_id False age True sex True user_lv_cd False user_reg_tm True dtype: bool empty info in detail of Action 2: user_id False sku_id False time False model_id True type False cate False brand False dtype: bool empty info in detail of Action 3: user_id False sku_id False time False model_id True type False cate False brand False dtype: bool empty info in detail of Action 4: user_id False sku_id False time False model_id True type False cate False brand False dtype: bool 上面简单的输出了下存在空值的文件中具体哪些列存在空值(True)，结果如下 User age sex user_reg_tm Action model_id 接下来具体看看各文件中的空值情况： 123456789101112def empty_records(f_path, f_name, col_name): df_file = pd.read_csv(f_path) missing = df_file[col_name].isnull().sum().sum() print 'No. of missing &#123;0&#125; in &#123;1&#125; is &#123;2&#125;'.format(col_name, f_name, missing) print 'percent: ', missing * 1.0 / df_file.shape[0]empty_records(USER_FILE, 'User', 'age')empty_records(USER_FILE, 'User', 'sex')empty_records(USER_FILE, 'User', 'user_reg_tm')empty_records(ACTION_201602_FILE, 'Action 2', 'model_id')empty_records(ACTION_201603_FILE, 'Action 3', 'model_id')empty_records(ACTION_201604_FILE, 'Action 4', 'model_id') No. of missing age in User is 3 percent: 2.84843478509e-05 No. of missing sex in User is 3 percent: 2.84843478509e-05 No. of missing user_reg_tm in User is 3 percent: 2.84843478509e-05 No. of missing model_id in Action 2 is 4959617 percent: 0.431818363867 No. of missing model_id in Action 3 is 10553261 percent: 0.4072043169 No. of missing model_id in Action 4 is 5143018 percent: 0.38962452388 对比下数据集的记录数： 文件 文件说明 记录数 1. JData_User.csv 用户数据集 105,321个用户 2. JData_Comment.csv 商品评论 558,552条记录 3. JData_Product.csv 预测商品集合 24,187条记录 4. JData_Action_201602.csv 2月份行为交互记录 11,485,424条记录 5. JData_Action_201603.csv 3月份行为交互记录 25,916,378条记录 6. JData_Action_201604.csv 4月份行为交互记录 13,199,934条记录 两相对比结合前面输出的情况，针对不同数据进行不同处理 用户文件 age,sex:先填充为对应的未知状态（-1|2），后续作为未知状态的值进一步分析和处理 user_reg_tm:暂时不做处理 行为文件 model_id涉及数目接近一半，而且当前针对该特征没有很好的处理方法，待定 123user = pd.read_csv(USER_FILE)user['age'].fillna('-1', inplace=True)user['sex'].fillna(2, inplace=True) 1print pd.isnull(user).any() user_id False age False sex False user_lv_cd False user_reg_tm True dtype: bool 12nan_reg_tm = user[user['user_reg_tm'].isnull()]print nan_reg_tm user_id age sex user_lv_cd user_reg_tm 34072 234073 -1 2.0 1 NaN 38905 238906 -1 2.0 1 NaN 67704 267705 -1 2.0 1 NaN 123print len(user['age'].unique())print len(user['sex'].unique())print len(user['user_lv_cd'].unique()) 7 3 5 1prod = pd.read_csv(PRODUCT_FILE) 12345print len(prod['a1'].unique())print len(prod['a2'].unique())print len(prod['a3'].unique())# print len(prod['a2'].unique())print len(prod['brand'].unique()) 4 3 3 102 未知记录接下来看看各个文件中的未知记录占的比重 1234print 'No. of unknown age user: &#123;0&#125; and the percent: &#123;1&#125; '.format(user[user['age']=='-1'].shape[0], user[user['age']=='-1'].shape[0]*1.0/user.shape[0])print 'No. of unknown sex user: &#123;0&#125; and the percent: &#123;1&#125; '.format(user[user['sex']==2].shape[0], user[user['sex']==2].shape[0]*1.0/user.shape[0]) No. of unknown age user: 14415 and the percent: 0.136867291423 No. of unknown sex user: 54738 and the percent: 0.519725410887 123456789def unknown_records(f_path, f_name, col_name): df_file = pd.read_csv(f_path) missing = df_file[df_file[col_name]==-1].shape[0] print 'No. of unknown &#123;0&#125; in &#123;1&#125; is &#123;2&#125;'.format(col_name, f_name, missing) print 'percent: ', missing * 1.0 / df_file.shape[0] unknown_records(PRODUCT_FILE, 'Product', 'a1')unknown_records(PRODUCT_FILE, 'Product', 'a2')unknown_records(PRODUCT_FILE, 'Product', 'a3') No. of unknown a1 in Product is 1701 percent: 0.0703270351842 No. of unknown a2 in Product is 4050 percent: 0.167445321867 No. of unknown a3 in Product is 3815 percent: 0.157729358746 小结一下 空值部分对3条用户的sex,age填充为未知值,注册时间不作处理，此外行为数据部分model_id待定: 43.2%,40.7%,39.0% 未知值部分，用户age存在部分未知值: 13.7%，sex存在大量保密情况(超过一半) 52.0% 商品中各个属性存在部分未知的情况，a1&lt;a3&lt;a2，分别为： 7.0%,16.7%,15.8% 数据一致性验证首先检查JData_User中的用户和JData_Action中的用户是否一致，保证行为数据中的所产生的行为均由用户数据中的用户产生（但是可能存在用户在行为数据中无行为） 思路：利用pd.Merge连接sku 和 Action中的sku, 观察Action中的数据是否减少 1234567891011def user_action_check(): df_user = pd.read_csv('data/JData_User.csv') df_sku = df_user.ix[:,'user_id'].to_frame() df_month2 = pd.read_csv('data/JData_Action_201602.csv') print 'Is action of Feb. from User file? ', len(df_month2) == len(pd.merge(df_sku,df_month2)) df_month3 = pd.read_csv('data/JData_Action_201603.csv') print 'Is action of Mar. from User file? ', len(df_month3) == len(pd.merge(df_sku,df_month3)) df_month4 = pd.read_csv('data/JData_Action_201604.csv') print 'Is action of Apr. from User file? ', len(df_month4) == len(pd.merge(df_sku,df_month4))user_action_check() Is action of Feb. from User file? True Is action of Mar. from User file? True Is action of Apr. from User file? True 结论： User数据集中的用户和交互行为数据集中的用户完全一致 根据merge前后的数据量比对，能保证Action中的用户ID是User中的ID的子集 重复记录分析除去各个数据文件中完全重复的记录,结果证明线上成绩反而大幅下降，可能解释是重复数据是有意义的，比如用户同时购买多件商品，同时添加多个数量的商品到购物车等… 1234567891011def deduplicate(filepath, filename, newpath): df_file = pd.read_csv(filepath) before = df_file.shape[0] df_file.drop_duplicates(inplace=True) after = df_file.shape[0] n_dup = before-after print 'No. of duplicate records for ' + filename + ' is: ' + str(n_dup) if n_dup != 0: df_file.to_csv(newpath, index=None) else: print 'no duplicate records in ' + filename 123456# deduplicate('data/JData_Action_201602.csv', 'Feb. action', 'data/JData_Action_201602_dedup.csv')deduplicate('data/JData_Action_201603.csv', 'Mar. action', 'data/JData_Action_201603_dedup.csv')deduplicate('data/JData_Action_201604.csv', 'Feb. action', 'data/JData_Action_201604_dedup.csv')deduplicate('data/JData_Comment.csv', 'Comment', 'data/JData_Comment_dedup.csv')deduplicate('data/JData_Product.csv', 'Product', 'data/JData_Product_dedup.csv')deduplicate('data/JData_User.csv', 'User', 'data/JData_User_dedup.csv') No. of duplicate records for Mar. action is: 7085038 No. of duplicate records for Feb. action is: 3672710 No. of duplicate records for Comment is: 0 no duplicate records in Comment No. of duplicate records for Product is: 0 no duplicate records in Product No. of duplicate records for User is: 0 no duplicate records in User 123IsDuplicated = df_month.duplicated() df_d=df_month[IsDuplicated]df_d.groupby('type').count() #发现重复数据大多数都是由于浏览（1），或者点击(6)产生 type user_id sku_id time model_id cate brand 1 2176378 2176378 2176378 0 2176378 2176378 2 636 636 636 0 636 636 3 1464 1464 1464 0 1464 1464 4 37 37 37 0 37 37 5 1981 1981 1981 0 1981 1981 6 575597 575597 575597 545054 575597 575597 检查是否存在注册时间在2016年-4月-15号之后的用户1234import pandas as pddf_user = pd.read_csv('data\\JData_User.csv',encoding='gbk')df_user['user_reg_tm']=pd.to_datetime(df_user['user_reg_tm'])df_user.ix[df_user.user_reg_tm &gt;= '2016-4-15'] - user_id age sex user_lv_cd user_reg_tm 7457 207458 -1 2.0 1 2016-04-15 7463 207464 26-35岁 2.0 2 2016-04-15 7467 207468 36-45岁 2.0 3 2016-04-15 7472 207473 -1 2.0 1 2016-04-15 7482 207483 26-35岁 2.0 3 2016-04-15 7492 207493 16-25岁 2.0 3 2016-04-15 7493 207494 16-25岁 2.0 3 2016-04-15 7503 207504 16-25岁 2.0 4 2016-04-15 7510 207511 46-55岁 2.0 5 2016-04-15 7512 207513 -1 2.0 1 2016-04-15 7518 207519 26-35岁 2.0 2 2016-04-15 7521 207522 26-35岁 0.0 3 2016-04-15 7525 207526 -1 2.0 3 2016-04-15 7533 207534 -1 2.0 1 2016-04-15 7543 207544 26-35岁 2.0 3 2016-04-15 7544 207545 -1 2.0 1 2016-04-15 7551 207552 26-35岁 2.0 3 2016-04-15 7553 207554 16-25岁 2.0 4 2016-04-15 8545 208546 16-25岁 0.0 2 2016-04-29 9394 209395 16-25岁 1.0 2 2016-05-11 10362 210363 56岁以上 2.0 2 2016-05-24 10367 210368 -1 2.0 1 2016-05-24 11019 211020 36-45岁 2.0 3 2016-06-06 12014 212015 36-45岁 2.0 2 2016-07-05 13850 213851 26-35岁 2.0 3 2016-09-11 14542 214543 -1 2.0 1 2016-10-05 16746 216747 16-25岁 2.0 1 2016-11-25 由于注册时间是京东系统错误造成，如果行为数据中没有在4月15号之后的数据的话，那么说明这些用户还是正常用户，并不需要删除。 123df_month = pd.read_csv('data\\JData_Action_201604.csv')df_month['time'] = pd.to_datetime(df_month['time'])df_month.ix[df_month.time &gt;= '2016-4-16'] - user_id sku_id time model_id type cate brand 结论：说明用户没有异常操作数据，所以这一批用户不删除 行为数据中的user_id为浮点型，进行INT类型转换12345678910111213import pandas as pddf_month = pd.read_csv('data\\JData_Action_201602.csv')df_month['user_id'] = df_month['user_id'].apply(lambda x:int(x))print df_month['user_id'].dtypedf_month.to_csv('data\\JData_Action_201602.csv',index=None)df_month = pd.read_csv('data\\JData_Action_201603.csv')df_month['user_id'] = df_month['user_id'].apply(lambda x:int(x))print df_month['user_id'].dtypedf_month.to_csv('data\\JData_Action_201603.csv',index=None)df_month = pd.read_csv('data\\JData_Action_201604.csv')df_month['user_id'] = df_month['user_id'].apply(lambda x:int(x))print df_month['user_id'].dtypedf_month.to_csv('data\\JData_Action_201604.csv',index=None) int64 int64 int64 按照星期对用户购买行为进行分析12345678910111213141516171819# 提取购买(type=4)的行为数据def get_from_action_data(fname, chunk_size=100000): reader = pd.read_csv(fname, header=0, iterator=True) chunks = [] loop = True while loop: try: chunk = reader.get_chunk(chunk_size)[ [\"user_id\", \"sku_id\", \"type\", \"time\"]] chunks.append(chunk) except StopIteration: loop = False print(\"Iteration is stopped\") df_ac = pd.concat(chunks, ignore_index=True) # type=4,为购买 df_ac = df_ac[df_ac['type'] == 4] return df_ac[[\"user_id\", \"sku_id\", \"time\"]] 12345df_ac = []df_ac.append(get_from_action_data(fname=ACTION_201602_FILE))df_ac.append(get_from_action_data(fname=ACTION_201603_FILE))df_ac.append(get_from_action_data(fname=ACTION_201604_FILE))df_ac = pd.concat(df_ac, ignore_index=True) Iteration is stopped Iteration is stopped Iteration is stopped 1print(df_ac.dtypes) user_id int64 sku_id int64 time object dtype: object 12345# 将time字段转换为datetime类型df_ac['time'] = pd.to_datetime(df_ac['time'])# 使用lambda匿名函数将时间time转换为星期(周一为1, 周日为７)df_ac['time'] = df_ac['time'].apply(lambda x: x.weekday() + 1) 1234# 周一到周日每天购买用户个数df_user = df_ac.groupby('time')['user_id'].nunique()df_user = df_user.to_frame().reset_index()df_user.columns = ['weekday', 'user_num'] 1234# 周一到周日每天购买商品个数df_item = df_ac.groupby('time')['sku_id'].nunique()df_item = df_item.to_frame().reset_index()df_item.columns = ['weekday', 'item_num'] 1234# 周一到周日每天购买记录个数df_ui = df_ac.groupby('time', as_index=False).size()df_ui = df_ui.to_frame().reset_index()df_ui.columns = ['weekday', 'user_item_num'] 123456789101112131415161718# 条形宽度bar_width = 0.2# 透明度opacity = 0.4plt.bar(df_user['weekday'], df_user['user_num'], bar_width, alpha=opacity, color='c', label='user')plt.bar(df_item['weekday']+bar_width, df_item['item_num'], bar_width, alpha=opacity, color='g', label='item')plt.bar(df_ui['weekday']+bar_width*2, df_ui['user_item_num'], bar_width, alpha=opacity, color='m', label='user_item')plt.xlabel('weekday')plt.ylabel('number')plt.title('A Week Purchase Table')plt.xticks(df_user['weekday'] + bar_width * 3 / 2., (1,2,3,4,5,6,7))plt.tight_layout() plt.legend(prop=&#123;'size':10&#125;) 分析：周六，周日购买量较少 一个月中各天购买量2016年2月1234df_ac = get_from_action_data(fname=ACTION_201602_FILE)# 将time字段转换为datetime类型并使用lambda匿名函数将时间time转换为天df_ac['time'] = pd.to_datetime(df_ac['time']).apply(lambda x: x.day) Iteration is stopped 1234567891011df_user = df_ac.groupby('time')['user_id'].nunique()df_user = df_user.to_frame().reset_index()df_user.columns = ['day', 'user_num']df_item = df_ac.groupby('time')['sku_id'].nunique()df_item = df_item.to_frame().reset_index()df_item.columns = ['day', 'item_num']df_ui = df_ac.groupby('time', as_index=False).size()df_ui = df_ui.to_frame().reset_index()df_ui.columns = ['day', 'user_item_num'] 1234567891011121314151617181920212223# 条形宽度bar_width = 0.2# 透明度opacity = 0.4# 天数day_range = range(1,len(df_user['day']) + 1, 1)# 设置图片大小plt.figure(figsize=(14,10))plt.bar(df_user['day'], df_user['user_num'], bar_width, alpha=opacity, color='c', label='user')plt.bar(df_item['day']+bar_width, df_item['item_num'], bar_width, alpha=opacity, color='g', label='item')plt.bar(df_ui['day']+bar_width*2, df_ui['user_item_num'], bar_width, alpha=opacity, color='m', label='user_item')plt.xlabel('day')plt.ylabel('number')plt.title('February Purchase Table')plt.xticks(df_user['day'] + bar_width * 3 / 2., day_range)# plt.ylim(0, 80)plt.tight_layout() plt.legend(prop=&#123;'size':9&#125;) 分析： 2月份5,6,7,8,9,10 这几天购买量非常少，原因可能是中国农历春节，快递不营业 2016年3月1234df_ac = get_from_action_data(fname=ACTION_201603_FILE)# 将time字段转换为datetime类型并使用lambda匿名函数将时间time转换为天df_ac['time'] = pd.to_datetime(df_ac['time']).apply(lambda x: x.day) Iteration is stopped 1234567891011df_user = df_ac.groupby('time')['user_id'].nunique()df_user = df_user.to_frame().reset_index()df_user.columns = ['day', 'user_num']df_item = df_ac.groupby('time')['sku_id'].nunique()df_item = df_item.to_frame().reset_index()df_item.columns = ['day', 'item_num']df_ui = df_ac.groupby('time', as_index=False).size()df_ui = df_ui.to_frame().reset_index()df_ui.columns = ['day', 'user_item_num'] 1234567891011121314151617181920212223# 条形宽度bar_width = 0.2# 透明度opacity = 0.4# 天数day_range = range(1,len(df_user['day']) + 1, 1)# 设置图片大小plt.figure(figsize=(14,10))plt.bar(df_user['day'], df_user['user_num'], bar_width, alpha=opacity, color='c', label='user')plt.bar(df_item['day']+bar_width, df_item['item_num'], bar_width, alpha=opacity, color='g', label='item')plt.bar(df_ui['day']+bar_width*2, df_ui['user_item_num'], bar_width, alpha=opacity, color='m', label='user_item')plt.xlabel('day')plt.ylabel('number')plt.title('March Purchase Table')plt.xticks(df_user['day'] + bar_width * 3 / 2., day_range)# plt.ylim(0, 80)plt.tight_layout() plt.legend(prop=&#123;'size':9&#125;) 分析：3月份14,15,16不知名节日，造成购物量剧增，总体来看，购物记录多于2月份 2016年4月1234df_ac = get_from_action_data(fname=ACTION_201604_FILE)# 将time字段转换为datetime类型并使用lambda匿名函数将时间time转换为天df_ac['time'] = pd.to_datetime(df_ac['time']).apply(lambda x: x.day) Iteration is stopped 1234567891011df_user = df_ac.groupby('time')['user_id'].nunique()df_user = df_user.to_frame().reset_index()df_user.columns = ['day', 'user_num']df_item = df_ac.groupby('time')['sku_id'].nunique()df_item = df_item.to_frame().reset_index()df_item.columns = ['day', 'item_num']df_ui = df_ac.groupby('time', as_index=False).size()df_ui = df_ui.to_frame().reset_index()df_ui.columns = ['day', 'user_item_num'] 1234567891011121314151617181920212223# 条形宽度bar_width = 0.2# 透明度opacity = 0.4# 天数day_range = range(1,len(df_user['day']) + 1, 1)# 设置图片大小plt.figure(figsize=(14,10))plt.bar(df_user['day'], df_user['user_num'], bar_width, alpha=opacity, color='c', label='user')plt.bar(df_item['day']+bar_width, df_item['item_num'], bar_width, alpha=opacity, color='g', label='item')plt.bar(df_ui['day']+bar_width*2, df_ui['user_item_num'], bar_width, alpha=opacity, color='m', label='user_item')plt.xlabel('day')plt.ylabel('number')plt.title('April Purchase Table')plt.xticks(df_user['day'] + bar_width * 3 / 2., day_range)# plt.ylim(0, 80)plt.tight_layout() plt.legend(prop=&#123;'size':9&#125;) 分析：似乎每个月中旬都有较强的购物欲望？ 商品分类别销售统计周一到周日各商品类别销售情况12345678910111213141516171819# 从行为记录中提取商品类别数据def get_from_action_data(fname, chunk_size=100000): reader = pd.read_csv(fname, header=0, iterator=True) chunks = [] loop = True while loop: try: chunk = reader.get_chunk(chunk_size)[ [\"cate\", \"brand\", \"type\", \"time\"]] chunks.append(chunk) except StopIteration: loop = False print(\"Iteration is stopped\") df_ac = pd.concat(chunks, ignore_index=True) # type=4,为购买 df_ac = df_ac[df_ac['type'] == 4] return df_ac[[\"cate\", \"brand\", \"type\", \"time\"]] 12345df_ac = []df_ac.append(get_from_action_data(fname=ACTION_201602_FILE))df_ac.append(get_from_action_data(fname=ACTION_201603_FILE))df_ac.append(get_from_action_data(fname=ACTION_201604_FILE))df_ac = pd.concat(df_ac, ignore_index=True) Iteration is stopped Iteration is stopped Iteration is stopped 12345# 将time字段转换为datetime类型df_ac['time'] = pd.to_datetime(df_ac['time'])# 使用lambda匿名函数将时间time转换为星期(周一为1, 周日为７)df_ac['time'] = df_ac['time'].apply(lambda x: x.weekday() + 1) 12# 观察有几个类别商品df_ac.groupby(df_ac['cate']).count() cate cate brand type time 4 9326 9326 9326 9326 5 8138 8138 8138 8138 6 6982 6982 6982 6982 7 6214 6214 6214 6214 8 13281 13281 13281 13281 9 4104 4104 4104 4104 10 189 189 189 189 11 18 18 18 18 1234# 周一到周日每天购买商品类别4的数量统计df_product = df_ac['brand'].groupby([df_ac['time'],df_ac['cate']]).count()df_product=df_product.unstack()df_product.plot(kind='bar',title='Cate Purchase Table in a Week',figsize=(14,10)) 分析：星期二买类别8的最多，星期天最少。 每月各类商品销售情况（只关注商品8）2016年2，3，4月123456789101112df_ac2 = get_from_action_data(fname=ACTION_201602_FILE)# 将time字段转换为datetime类型并使用lambda匿名函数将时间time转换为天df_ac2['time'] = pd.to_datetime(df_ac2['time']).apply(lambda x: x.day)df_ac3 = get_from_action_data(fname=ACTION_201603_FILE)# 将time字段转换为datetime类型并使用lambda匿名函数将时间time转换为天df_ac3['time'] = pd.to_datetime(df_ac3['time']).apply(lambda x: x.day)df_ac4 = get_from_action_data(fname=ACTION_201604_FILE)# 将time字段转换为datetime类型并使用lambda匿名函数将时间time转换为天df_ac4['time'] = pd.to_datetime(df_ac4['time']).apply(lambda x: x.day) 1234567891011121314dc_cate2 = df_ac2[df_ac2['cate']==8]dc_cate2 = dc_cate2['brand'].groupby(dc_cate2['time']).count()dc_cate2 = dc_cate2.to_frame().reset_index()dc_cate2.columns = ['day', 'product_num']dc_cate3 = df_ac3[df_ac3['cate']==8]dc_cate3 = dc_cate3['brand'].groupby(dc_cate3['time']).count()dc_cate3 = dc_cate3.to_frame().reset_index()dc_cate3.columns = ['day', 'product_num']dc_cate4 = df_ac4[df_ac4['cate']==8]dc_cate4 = dc_cate4['brand'].groupby(dc_cate4['time']).count()dc_cate4 = dc_cate4.to_frame().reset_index()dc_cate4.columns = ['day', 'product_num'] 1234567891011121314151617181920212223# 条形宽度bar_width = 0.2# 透明度opacity = 0.4# 天数day_range = range(1,len(dc_cate3['day']) + 1, 1)# 设置图片大小plt.figure(figsize=(14,10))plt.bar(dc_cate2['day'], dc_cate2['product_num'], bar_width, alpha=opacity, color='c', label='February')plt.bar(dc_cate3['day']+bar_width, dc_cate3['product_num'], bar_width, alpha=opacity, color='g', label='March')plt.bar(dc_cate4['day']+bar_width*2, dc_cate4['product_num'], bar_width, alpha=opacity, color='m', label='April')plt.xlabel('day')plt.ylabel('number')plt.title('Cate-8 Purchase Table')plt.xticks(dc_cate3['day'] + bar_width * 3 / 2., day_range)# plt.ylim(0, 80)plt.tight_layout() plt.legend(prop=&#123;'size':9&#125;) 分析：2月份对类别8商品的购买普遍偏低，3，4月份普遍偏高，3月15日购买极其多！可以对比3月份的销售记录，发现类别8将近占了3月15日总销售的一半！同时发现，3,4月份类别8销售记录在前半个月特别相似，除了4月8号，9号和3月15号。 特征工程1234567891011121314151617181920212223242526272829303132333435comment_date = [ \"2016-02-01\", \"2016-02-08\", \"2016-02-15\", \"2016-02-22\", \"2016-02-29\", \"2016-03-07\", \"2016-03-14\", \"2016-03-21\", \"2016-03-28\", \"2016-04-04\", \"2016-04-11\", \"2016-04-15\"]def get_actions_1(): action = pd.read_csv(action_1_path) return actiondef get_actions_2(): action2 = pd.read_csv(action_2_path) return action2def get_actions_3(): action3 = pd.read_csv(action_3_path) return action3# 读取并拼接所有行为记录文件def get_all_action(): action_1 = get_actions_1() action_2 = get_actions_2() action_3 = get_actions_3() actions = pd.concat([action_1, action_2, action_3]) # type: pd.DataFrame# actions = pd.read_csv(action_path) return actions# 获取某个时间段的行为记录def get_actions(start_date, end_date, all_actions): \"\"\" :param start_date: :param end_date: :return: actions: pd.Dataframe \"\"\" actions = all_actions[(all_actions.time &gt;= start_date) &amp; (all_actions.time &lt; end_date)].copy() return actions 用户特征用户基本特征获取基本的用户特征，基于用户本身属性多为类别特征的特点，对age,sex,usr_lv_cd进行独热编码操作，对于用户注册时间暂时不处理 123456789101112131415161718from sklearn import preprocessingdef get_basic_user_feat(): # 针对年龄的中文字符问题处理，首先是读入的时候编码，填充空值，然后将其数值化，最后独热编码，此外对于sex也进行了数值类型转换 user = pd.read_csv(user_path, encoding='gbk')# user['age'].fillna('-1', inplace=True)# user['sex'].fillna(2, inplace=True) user['sex'] = user['sex'].astype(int) user['age'] = user['age'].astype(unicode) le = preprocessing.LabelEncoder() age_df = le.fit_transform(user['age'])# print list(le.classes_) age_df = pd.get_dummies(age_df, prefix='age') sex_df = pd.get_dummies(user['sex'], prefix='sex') user_lv_df = pd.get_dummies(user['user_lv_cd'], prefix='user_lv_cd') user = pd.concat([user['user_id'], age_df, sex_df, user_lv_df], axis=1) return user 商品特征商品基本特征根据商品文件获取基本的特征，针对属性a1,a2,a3进行独热编码，商品类别和品牌直接作为特征 1234567def get_basic_product_feat(): product = pd.read_csv(product_path) attr1_df = pd.get_dummies(product[\"a1\"], prefix=\"a1\") attr2_df = pd.get_dummies(product[\"a2\"], prefix=\"a2\") attr3_df = pd.get_dummies(product[\"a3\"], prefix=\"a3\") product = pd.concat([product[['sku_id', 'cate', 'brand']], attr1_df, attr2_df, attr3_df], axis=1) return product 评论特征 分时间段 对评论数进行独热编码 12345678910111213141516171819202122def get_comments_product_feat(end_date): comments = pd.read_csv(comment_path) comment_date_end = end_date comment_date_begin = comment_date[0] for date in reversed(comment_date): if date &lt; comment_date_end: comment_date_begin = date break comments = comments[comments.dt==comment_date_begin] df = pd.get_dummies(comments['comment_num'], prefix='comment_num') # 为了防止某个时间段不具备评论数为0的情况（测试集出现过这种情况） for i in range(0, 5): if 'comment_num_' + str(i) not in df.columns: df['comment_num_' + str(i)] = 0 df = df[['comment_num_0', 'comment_num_1', 'comment_num_2', 'comment_num_3', 'comment_num_4']] comments = pd.concat([comments, df], axis=1) # type: pd.DataFrame #del comments['dt'] #del comments['comment_num'] comments = comments[['sku_id', 'has_bad_comment', 'bad_comment_rate','comment_num_0', 'comment_num_1', 'comment_num_2', 'comment_num_3', 'comment_num_4']] return comments 行为特征 分时间段 对行为类别进行独热编码 分别按照用户-类别行为分组和用户-类别-商品行为分组统计，然后计算 用户对同类别下其他商品的行为计数 针对用户对同类别下目标商品的行为计数与该时间段的行为均值作差 12345678910111213141516171819202122232425262728293031323334def get_action_feat(start_date, end_date, all_actions, i): actions = get_actions(start_date, end_date, all_actions) actions = actions[['user_id', 'sku_id', 'cate','type']] # 不同时间累积的行为计数（3,5,7,10,15,21,30） df = pd.get_dummies(actions['type'], prefix='action_before_%s' %i) before_date = 'action_before_%s' %i actions = pd.concat([actions, df], axis=1) # type: pd.DataFrame # 分组统计，用户-类别-商品,不同用户对不同类别下商品的行为计数 actions = actions.groupby(['user_id', 'sku_id','cate'], as_index=False).sum() # 分组统计，用户-类别，不同用户对不同商品类别的行为计数 user_cate = actions.groupby(['user_id','cate'], as_index=False).sum() del user_cate['sku_id'] del user_cate['type'] actions = pd.merge(actions, user_cate, how='left', on=['user_id','cate']) #本类别下其他商品点击量 # 前述两种分组含有相同名称的不同行为的计数，系统会自动针对名称调整添加后缀,x,y，所以这里作差统计的是同一类别下其他商品的行为计数 actions[before_date+'_1_y'] = actions[before_date+'_1_y'] - actions[before_date+'_1_x'] actions[before_date+'_2_y'] = actions[before_date+'_2_y'] - actions[before_date+'_2_x'] actions[before_date+'_3_y'] = actions[before_date+'_3_y'] - actions[before_date+'_3_x'] actions[before_date+'_4_y'] = actions[before_date+'_4_y'] - actions[before_date+'_4_x'] actions[before_date+'_5_y'] = actions[before_date+'_5_y'] - actions[before_date+'_5_x'] actions[before_date+'_6_y'] = actions[before_date+'_6_y'] - actions[before_date+'_6_x'] # 统计用户对不同类别下商品计数与该类别下商品行为计数均值（对时间）的差值 actions[before_date+'minus_mean_1'] = actions[before_date+'_1_x'] - (actions[before_date+'_1_x']/i) actions[before_date+'minus_mean_2'] = actions[before_date+'_2_x'] - (actions[before_date+'_2_x']/i) actions[before_date+'minus_mean_3'] = actions[before_date+'_3_x'] - (actions[before_date+'_3_x']/i) actions[before_date+'minus_mean_4'] = actions[before_date+'_4_x'] - (actions[before_date+'_4_x']/i) actions[before_date+'minus_mean_5'] = actions[before_date+'_5_x'] - (actions[before_date+'_5_x']/i) actions[before_date+'minus_mean_6'] = actions[before_date+'_6_x'] - (actions[before_date+'_6_x']/i) del actions['type'] # 保留cate特征# del actions['cate'] return actions 用户-行为累积用户特征 分时间段 用户不同行为的 购买转化率 均值 标准差 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384def get_accumulate_user_feat(end_date, all_actions, day): start_date = datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=day) start_date = start_date.strftime('%Y-%m-%d') before_date = 'user_action_%s' % day feature = [ 'user_id', before_date + '_1', before_date + '_2', before_date + '_3', before_date + '_4', before_date + '_5', before_date + '_6', before_date + '_1_ratio', before_date + '_2_ratio', before_date + '_3_ratio', before_date + '_5_ratio', before_date + '_6_ratio', before_date + '_1_mean', before_date + '_2_mean', before_date + '_3_mean', before_date + '_4_mean', before_date + '_5_mean', before_date + '_6_mean', before_date + '_1_std', before_date + '_2_std', before_date + '_3_std', before_date + '_4_std', before_date + '_5_std', before_date + '_6_std' ] actions = get_actions(start_date, end_date, all_actions) df = pd.get_dummies(actions['type'], prefix=before_date)# actions['date'] = pd.to_datetime(actions['time']).apply(lambda x: x.date()) actions = pd.concat([actions[['user_id', 'date']], df], axis=1) # 分组统计，用户不同日期的行为计算标准差 actions_date = actions.groupby(['user_id', 'date']).sum() actions_date = actions_date.unstack() actions_date.fillna(0, inplace=True) action_1 = np.std(actions_date[before_date + '_1'], axis=1) action_1 = action_1.to_frame() action_1.columns = [before_date + '_1_std'] action_2 = np.std(actions_date[before_date + '_2'], axis=1) action_2 = action_2.to_frame() action_2.columns = [before_date + '_2_std'] action_3 = np.std(actions_date[before_date + '_3'], axis=1) action_3 = action_3.to_frame() action_3.columns = [before_date + '_3_std'] action_4 = np.std(actions_date[before_date + '_4'], axis=1) action_4 = action_4.to_frame() action_4.columns = [before_date + '_4_std'] action_5 = np.std(actions_date[before_date + '_5'], axis=1) action_5 = action_5.to_frame() action_5.columns = [before_date + '_5_std'] action_6 = np.std(actions_date[before_date + '_6'], axis=1) action_6 = action_6.to_frame() action_6.columns = [before_date + '_6_std'] actions_date = pd.concat( [action_1, action_2, action_3, action_4, action_5, action_6], axis=1) actions_date['user_id'] = actions_date.index # 分组统计，按用户分组，统计用户各项行为的转化率、均值 actions = actions.groupby(['user_id'], as_index=False).sum()# days_interal = (datetime.strptime(end_date, '%Y-%m-%d') -# datetime.strptime(start_date, '%Y-%m-%d')).days # 转化率# actions[before_date + '_1_ratio'] = actions[before_date +# '_4'] / actions[before_date +# '_1']# actions[before_date + '_2_ratio'] = actions[before_date +# '_4'] / actions[before_date +# '_2']# actions[before_date + '_3_ratio'] = actions[before_date +# '_4'] / actions[before_date +# '_3']# actions[before_date + '_5_ratio'] = actions[before_date +# '_4'] / actions[before_date +# '_5']# actions[before_date + '_6_ratio'] = actions[before_date +# '_4'] / actions[before_date +# '_6'] actions[before_date + '_1_ratio'] = np.log(1 + actions[before_date + '_4']) - np.log(1 + actions[before_date +'_1']) actions[before_date + '_2_ratio'] = np.log(1 + actions[before_date + '_4']) - np.log(1 + actions[before_date +'_2']) actions[before_date + '_3_ratio'] = np.log(1 + actions[before_date + '_4']) - np.log(1 + actions[before_date +'_3']) actions[before_date + '_5_ratio'] = np.log(1 + actions[before_date + '_4']) - np.log(1 + actions[before_date +'_5']) actions[before_date + '_6_ratio'] = np.log(1 + actions[before_date + '_4']) - np.log(1 + actions[before_date +'_6']) # 均值 actions[before_date + '_1_mean'] = actions[before_date + '_1'] / day actions[before_date + '_2_mean'] = actions[before_date + '_2'] / day actions[before_date + '_3_mean'] = actions[before_date + '_3'] / day actions[before_date + '_4_mean'] = actions[before_date + '_4'] / day actions[before_date + '_5_mean'] = actions[before_date + '_5'] / day actions[before_date + '_6_mean'] = actions[before_date + '_6'] / day actions = pd.merge(actions, actions_date, how='left', on='user_id') actions = actions[feature] return actions 用户近期行为特征在上面针对用户进行累积特征提取的基础上，分别提取用户近一个月、近三天的特征，然后提取一个月内用户除去最近三天的行为占据一个月的行为的比重 12345678910111213141516171819202122def get_recent_user_feat(end_date, all_actions): actions_3 = get_accumulate_user_feat(end_date, all_actions, 3) actions_30 = get_accumulate_user_feat(end_date, all_actions, 30) actions = pd.merge(actions_3, actions_30, how ='left', on='user_id') del actions_3 del actions_30 actions['recent_action1'] = np.log(1 + actions['user_action_30_1']-actions['user_action_3_1']) - np.log(1 + actions['user_action_30_1']) actions['recent_action2'] = np.log(1 + actions['user_action_30_2']-actions['user_action_3_2']) - np.log(1 + actions['user_action_30_2']) actions['recent_action3'] = np.log(1 + actions['user_action_30_3']-actions['user_action_3_3']) - np.log(1 + actions['user_action_30_3']) actions['recent_action4'] = np.log(1 + actions['user_action_30_4']-actions['user_action_3_4']) - np.log(1 + actions['user_action_30_4']) actions['recent_action5'] = np.log(1 + actions['user_action_30_5']-actions['user_action_3_5']) - np.log(1 + actions['user_action_30_5']) actions['recent_action6'] = np.log(1 + actions['user_action_30_6']-actions['user_action_3_6']) - np.log(1 + actions['user_action_30_6']) # actions['recent_action1'] = (actions['user_action_30_1']-actions['user_action_3_1'])/actions['user_action_30_1']# actions['recent_action2'] = (actions['user_action_30_2']-actions['user_action_3_2'])/actions['user_action_30_2']# actions['recent_action3'] = (actions['user_action_30_3']-actions['user_action_3_3'])/actions['user_action_30_3']# actions['recent_action4'] = (actions['user_action_30_4']-actions['user_action_3_4'])/actions['user_action_30_4']# actions['recent_action5'] = (actions['user_action_30_5']-actions['user_action_3_5'])/actions['user_action_30_5']# actions['recent_action6'] = (actions['user_action_30_6']-actions['user_action_3_6'])/actions['user_action_30_6'] return actions 用户对同类别下各种商品的行为 用户对各个类别的各项行为操作统计 用户对各个类别操作行为统计占对所有类别操作行为统计的比重 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114#增加了用户对不同类别的交互特征def get_user_cate_feature(start_date, end_date, all_actions): actions = get_actions(start_date, end_date, all_actions) actions = actions[['user_id', 'cate', 'type']] df = pd.get_dummies(actions['type'], prefix='type') actions = pd.concat([actions[['user_id', 'cate']], df], axis=1) actions = actions.groupby(['user_id', 'cate']).sum() actions = actions.unstack() actions.columns = actions.columns.swaplevel(0, 1) actions.columns = actions.columns.droplevel() actions.columns = [ 'cate_4_type1', 'cate_5_type1', 'cate_6_type1', 'cate_7_type1', 'cate_8_type1', 'cate_9_type1', 'cate_10_type1', 'cate_11_type1', 'cate_4_type2', 'cate_5_type2', 'cate_6_type2', 'cate_7_type2', 'cate_8_type2', 'cate_9_type2', 'cate_10_type2', 'cate_11_type2', 'cate_4_type3', 'cate_5_type3', 'cate_6_type3', 'cate_7_type3', 'cate_8_type3', 'cate_9_type3', 'cate_10_type3', 'cate_11_type3', 'cate_4_type4', 'cate_5_type4', 'cate_6_type4', 'cate_7_type4', 'cate_8_type4', 'cate_9_type4', 'cate_10_type4', 'cate_11_type4', 'cate_4_type5', 'cate_5_type5', 'cate_6_type5', 'cate_7_type5', 'cate_8_type5', 'cate_9_type5', 'cate_10_type5', 'cate_11_type5', 'cate_4_type6', 'cate_5_type6', 'cate_6_type6', 'cate_7_type6', 'cate_8_type6', 'cate_9_type6', 'cate_10_type6', 'cate_11_type6' ] actions = actions.fillna(0) actions['cate_action_sum'] = actions.sum(axis=1) actions['cate8_percentage'] = ( actions['cate_8_type1'] + actions['cate_8_type2'] + actions['cate_8_type3'] + actions['cate_8_type4'] + actions['cate_8_type5'] + actions['cate_8_type6'] ) / actions['cate_action_sum'] actions['cate4_percentage'] = ( actions['cate_4_type1'] + actions['cate_4_type2'] + actions['cate_4_type3'] + actions['cate_4_type4'] + actions['cate_4_type5'] + actions['cate_4_type6'] ) / actions['cate_action_sum'] actions['cate5_percentage'] = ( actions['cate_5_type1'] + actions['cate_5_type2'] + actions['cate_5_type3'] + actions['cate_5_type4'] + actions['cate_5_type5'] + actions['cate_5_type6'] ) / actions['cate_action_sum'] actions['cate6_percentage'] = ( actions['cate_6_type1'] + actions['cate_6_type2'] + actions['cate_6_type3'] + actions['cate_6_type4'] + actions['cate_6_type5'] + actions['cate_6_type6'] ) / actions['cate_action_sum'] actions['cate7_percentage'] = ( actions['cate_7_type1'] + actions['cate_7_type2'] + actions['cate_7_type3'] + actions['cate_7_type4'] + actions['cate_7_type5'] + actions['cate_7_type6'] ) / actions['cate_action_sum'] actions['cate9_percentage'] = ( actions['cate_9_type1'] + actions['cate_9_type2'] + actions['cate_9_type3'] + actions['cate_9_type4'] + actions['cate_9_type5'] + actions['cate_9_type6'] ) / actions['cate_action_sum'] actions['cate10_percentage'] = ( actions['cate_10_type1'] + actions['cate_10_type2'] + actions['cate_10_type3'] + actions['cate_10_type4'] + actions['cate_10_type5'] + actions['cate_10_type6'] ) / actions['cate_action_sum'] actions['cate11_percentage'] = ( actions['cate_11_type1'] + actions['cate_11_type2'] + actions['cate_11_type3'] + actions['cate_11_type4'] + actions['cate_11_type5'] + actions['cate_11_type6'] ) / actions['cate_action_sum'] actions['cate8_type1_percentage'] = np.log( 1 + actions['cate_8_type1']) - np.log( 1 + actions['cate_8_type1'] + actions['cate_4_type1'] + actions['cate_5_type1'] + actions['cate_6_type1'] + actions['cate_7_type1'] + actions['cate_9_type1'] + actions['cate_10_type1'] + actions['cate_11_type1']) actions['cate8_type2_percentage'] = np.log( 1 + actions['cate_8_type2']) - np.log( 1 + actions['cate_8_type2'] + actions['cate_4_type2'] + actions['cate_5_type2'] + actions['cate_6_type2'] + actions['cate_7_type2'] + actions['cate_9_type2'] + actions['cate_10_type2'] + actions['cate_11_type2']) actions['cate8_type3_percentage'] = np.log( 1 + actions['cate_8_type3']) - np.log( 1 + actions['cate_8_type3'] + actions['cate_4_type3'] + actions['cate_5_type3'] + actions['cate_6_type3'] + actions['cate_7_type3'] + actions['cate_9_type3'] + actions['cate_10_type3'] + actions['cate_11_type3']) actions['cate8_type4_percentage'] = np.log( 1 + actions['cate_8_type4']) - np.log( 1 + actions['cate_8_type4'] + actions['cate_4_type4'] + actions['cate_5_type4'] + actions['cate_6_type4'] + actions['cate_7_type4'] + actions['cate_9_type4'] + actions['cate_10_type4'] + actions['cate_11_type4']) actions['cate8_type5_percentage'] = np.log( 1 + actions['cate_8_type5']) - np.log( 1 + actions['cate_8_type5'] + actions['cate_4_type5'] + actions['cate_5_type5'] + actions['cate_6_type5'] + actions['cate_7_type5'] + actions['cate_9_type5'] + actions['cate_10_type5'] + actions['cate_11_type5']) actions['cate8_type6_percentage'] = np.log( 1 + actions['cate_8_type6']) - np.log( 1 + actions['cate_8_type6'] + actions['cate_4_type6'] + actions['cate_5_type6'] + actions['cate_6_type6'] + actions['cate_7_type6'] + actions['cate_9_type6'] + actions['cate_10_type6'] + actions['cate_11_type6']) actions['user_id'] = actions.index actions = actions[[ 'user_id', 'cate8_percentage', 'cate4_percentage', 'cate5_percentage', 'cate6_percentage', 'cate7_percentage', 'cate9_percentage', 'cate10_percentage', 'cate11_percentage', 'cate8_type1_percentage', 'cate8_type2_percentage', 'cate8_type3_percentage', 'cate8_type4_percentage', 'cate8_type5_percentage', 'cate8_type6_percentage' ]] return actions 商品-行为累积商品特征 分时间段 针对商品的不同行为的 购买转化率 均值 标准差 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879def get_accumulate_product_feat(start_date, end_date, all_actions): feature = [ 'sku_id', 'product_action_1', 'product_action_2', 'product_action_3', 'product_action_4', 'product_action_5', 'product_action_6', 'product_action_1_ratio', 'product_action_2_ratio', 'product_action_3_ratio', 'product_action_5_ratio', 'product_action_6_ratio', 'product_action_1_mean', 'product_action_2_mean', 'product_action_3_mean', 'product_action_4_mean', 'product_action_5_mean', 'product_action_6_mean', 'product_action_1_std', 'product_action_2_std', 'product_action_3_std', 'product_action_4_std', 'product_action_5_std', 'product_action_6_std' ] actions = get_actions(start_date, end_date, all_actions) df = pd.get_dummies(actions['type'], prefix='product_action') # 按照商品-日期分组，计算某个时间段该商品的各项行为的标准差# actions['date'] = pd.to_datetime(actions['time']).apply(lambda x: x.date()) actions = pd.concat([actions[['sku_id', 'date']], df], axis=1) actions_date = actions.groupby(['sku_id', 'date']).sum() actions_date = actions_date.unstack() actions_date.fillna(0, inplace=True) action_1 = np.std(actions_date['product_action_1'], axis=1) action_1 = action_1.to_frame() action_1.columns = ['product_action_1_std'] action_2 = np.std(actions_date['product_action_2'], axis=1) action_2 = action_2.to_frame() action_2.columns = ['product_action_2_std'] action_3 = np.std(actions_date['product_action_3'], axis=1) action_3 = action_3.to_frame() action_3.columns = ['product_action_3_std'] action_4 = np.std(actions_date['product_action_4'], axis=1) action_4 = action_4.to_frame() action_4.columns = ['product_action_4_std'] action_5 = np.std(actions_date['product_action_5'], axis=1) action_5 = action_5.to_frame() action_5.columns = ['product_action_5_std'] action_6 = np.std(actions_date['product_action_6'], axis=1) action_6 = action_6.to_frame() action_6.columns = ['product_action_6_std'] actions_date = pd.concat( [action_1, action_2, action_3, action_4, action_5, action_6], axis=1) actions_date['sku_id'] = actions_date.index actions = actions.groupby(['sku_id'], as_index=False).sum() days_interal = (datetime.strptime(end_date, '%Y-%m-%d') - datetime.strptime(start_date, '%Y-%m-%d')).days # 针对商品分组，计算购买转化率# actions['product_action_1_ratio'] = actions['product_action_4'] / actions[# 'product_action_1']# actions['product_action_2_ratio'] = actions['product_action_4'] / actions[# 'product_action_2']# actions['product_action_3_ratio'] = actions['product_action_4'] / actions[# 'product_action_3']# actions['product_action_5_ratio'] = actions['product_action_4'] / actions[# 'product_action_5']# actions['product_action_6_ratio'] = actions['product_action_4'] / actions[# 'product_action_6'] actions['product_action_1_ratio'] = np.log(1 + actions['product_action_4']) - np.log(1 + actions['product_action_1']) actions['product_action_2_ratio'] = np.log(1 + actions['product_action_4']) - np.log(1 + actions['product_action_2']) actions['product_action_3_ratio'] = np.log(1 + actions['product_action_4']) - np.log(1 + actions['product_action_3']) actions['product_action_5_ratio'] = np.log(1 + actions['product_action_4']) - np.log(1 + actions['product_action_5']) actions['product_action_6_ratio'] = np.log(1 + actions['product_action_4']) - np.log(1 + actions['product_action_6']) # 计算各种行为的均值 actions['product_action_1_mean'] = actions[ 'product_action_1'] / days_interal actions['product_action_2_mean'] = actions[ 'product_action_2'] / days_interal actions['product_action_3_mean'] = actions[ 'product_action_3'] / days_interal actions['product_action_4_mean'] = actions[ 'product_action_4'] / days_interal actions['product_action_5_mean'] = actions[ 'product_action_5'] / days_interal actions['product_action_6_mean'] = actions[ 'product_action_6'] / days_interal actions = pd.merge(actions, actions_date, how='left', on='sku_id') actions = actions[feature] return actions 类别特征分时间段下各个商品类别的 购买转化率 标准差 均值 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859def get_accumulate_cate_feat(start_date, end_date, all_actions): feature = ['cate','cate_action_1', 'cate_action_2', 'cate_action_3', 'cate_action_4', 'cate_action_5', 'cate_action_6', 'cate_action_1_ratio', 'cate_action_2_ratio', 'cate_action_3_ratio', 'cate_action_5_ratio', 'cate_action_6_ratio', 'cate_action_1_mean', 'cate_action_2_mean', 'cate_action_3_mean', 'cate_action_4_mean', 'cate_action_5_mean', 'cate_action_6_mean', 'cate_action_1_std', 'cate_action_2_std', 'cate_action_3_std', 'cate_action_4_std', 'cate_action_5_std', 'cate_action_6_std'] actions = get_actions(start_date, end_date, all_actions)# actions['date'] = pd.to_datetime(actions['time']).apply(lambda x: x.date()) df = pd.get_dummies(actions['type'], prefix='cate_action') actions = pd.concat([actions[['cate','date']], df], axis=1) # 按照类别-日期分组计算针对不同类别的各种行为某段时间的标准差 actions_date = actions.groupby(['cate','date']).sum() actions_date = actions_date.unstack() actions_date.fillna(0, inplace=True) action_1 = np.std(actions_date['cate_action_1'], axis=1) action_1 = action_1.to_frame() action_1.columns = ['cate_action_1_std'] action_2 = np.std(actions_date['cate_action_2'], axis=1) action_2 = action_2.to_frame() action_2.columns = ['cate_action_2_std'] action_3 = np.std(actions_date['cate_action_3'], axis=1) action_3 = action_3.to_frame() action_3.columns = ['cate_action_3_std'] action_4 = np.std(actions_date['cate_action_4'], axis=1) action_4 = action_4.to_frame() action_4.columns = ['cate_action_4_std'] action_5 = np.std(actions_date['cate_action_5'], axis=1) action_5 = action_5.to_frame() action_5.columns = ['cate_action_5_std'] action_6 = np.std(actions_date['cate_action_6'], axis=1) action_6 = action_6.to_frame() action_6.columns = ['cate_action_6_std'] actions_date = pd.concat([action_1, action_2, action_3, action_4, action_5, action_6], axis=1) actions_date['cate'] = actions_date.index # 按照类别分组，统计各个商品类别下行为的转化率 actions = actions.groupby(['cate'], as_index=False).sum() days_interal = (datetime.strptime(end_date, '%Y-%m-%d')-datetime.strptime(start_date, '%Y-%m-%d')).days # actions['cate_action_1_ratio'] = actions['cate_action_4'] / actions['cate_action_1']# actions['cate_action_2_ratio'] = actions['cate_action_4'] / actions['cate_action_2']# actions['cate_action_3_ratio'] = actions['cate_action_4'] / actions['cate_action_3']# actions['cate_action_5_ratio'] = actions['cate_action_4'] / actions['cate_action_5']# actions['cate_action_6_ratio'] = actions['cate_action_4'] / actions['cate_action_6'] actions['cate_action_1_ratio'] =(np.log(1 + actions['cate_action_4']) - np.log(1 + actions['cate_action_1'])) actions['cate_action_2_ratio'] =(np.log(1 + actions['cate_action_4']) - np.log(1 + actions['cate_action_2'])) actions['cate_action_3_ratio'] =(np.log(1 + actions['cate_action_4']) - np.log(1 + actions['cate_action_3'])) actions['cate_action_5_ratio'] =(np.log(1 + actions['cate_action_4']) - np.log(1 + actions['cate_action_5'])) actions['cate_action_6_ratio'] =(np.log(1 + actions['cate_action_4']) - np.log(1 + actions['cate_action_6'])) # 按照类别分组，统计各个商品类别下行为在一段时间的均值 actions['cate_action_1_mean'] = actions['cate_action_1'] / days_interal actions['cate_action_2_mean'] = actions['cate_action_2'] / days_interal actions['cate_action_3_mean'] = actions['cate_action_3'] / days_interal actions['cate_action_4_mean'] = actions['cate_action_4'] / days_interal actions['cate_action_5_mean'] = actions['cate_action_5'] / days_interal actions['cate_action_6_mean'] = actions['cate_action_6'] / days_interal actions = pd.merge(actions, actions_date, how ='left',on='cate') actions = actions[feature] return actions 构造训练集/验证集 标签,采用滑动窗口的方式，构造训练集的时候针对产生购买的行为标记为1 整合特征 12345678910def get_labels(start_date, end_date, all_actions): actions = get_actions(start_date, end_date, all_actions)# actions = actions[actions['type'] == 4] # 修改为预测购买了商品8的用户预测 actions = actions[(actions['type'] == 4) &amp; (actions['cate']==8)] actions = actions.groupby(['user_id', 'sku_id'], as_index=False).sum() actions['label'] = 1 actions = actions[['user_id', 'sku_id', 'label']] return actions 构造训练集 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def make_actions(user, product, all_actions, train_start_date): train_end_date = datetime.strptime(train_start_date, '%Y-%m-%d') + timedelta(days=3) train_end_date = train_end_date.strftime('%Y-%m-%d') # 修正prod_acc,cate_acc的时间跨度 start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=30) start_days = start_days.strftime('%Y-%m-%d') print train_end_date user_acc = get_recent_user_feat(train_end_date, all_actions) print 'get_recent_user_feat finsihed' user_cate = get_user_cate_feature(train_start_date, train_end_date, all_actions) print 'get_user_cate_feature finished' product_acc = get_accumulate_product_feat(start_days, train_end_date, all_actions) print 'get_accumulate_product_feat finsihed' cate_acc = get_accumulate_cate_feat(start_days, train_end_date, all_actions) print 'get_accumulate_cate_feat finsihed' comment_acc = get_comments_product_feat(train_end_date) print 'get_comments_product_feat finished' # 标记 test_start_date = train_end_date test_end_date = datetime.strptime(test_start_date, '%Y-%m-%d') + timedelta(days=5) test_end_date = test_end_date.strftime('%Y-%m-%d') labels = get_labels(test_start_date, test_end_date, all_actions) print \"get labels\" actions = None for i in (3, 5, 7, 10, 15, 21, 30): start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=i) start_days = start_days.strftime('%Y-%m-%d') if actions is None: actions = get_action_feat(start_days, train_end_date, all_actions, i) else: # 注意这里的拼接key actions = pd.merge(actions, get_action_feat(start_days, train_end_date, all_actions, i), how='left', on=['user_id', 'sku_id', 'cate']) actions = pd.merge(actions, user, how='left', on='user_id') actions = pd.merge(actions, user_acc, how='left', on='user_id') actions = pd.merge(actions, user_cate, how='left', on='user_id') # 注意这里的拼接key actions = pd.merge(actions, product, how='left', on=['sku_id', 'cate']) actions = pd.merge(actions, product_acc, how='left', on='sku_id') actions = pd.merge(actions, cate_acc, how='left', on='cate') actions = pd.merge(actions, comment_acc, how='left', on='sku_id') actions = pd.merge(actions, labels, how='left', on=['user_id', 'sku_id']) # 主要是填充拼接商品基本特征、评论特征、标签之后的空值 actions = actions.fillna(0)# return actions # 采样 action_postive = actions[actions['label'] == 1] action_negative = actions[actions['label'] == 0] del actions neg_len = len(action_postive) * 10 action_negative = action_negative.sample(n=neg_len) action_sample = pd.concat([action_postive, action_negative], ignore_index=True) return action_sample 12345678910111213141516171819202122def make_train_set(train_start_date, setNums ,f_path): train_actions = None all_actions = get_all_action() print \"get all actions!\" user = get_basic_user_feat() print 'get_basic_user_feat finsihed' product = get_basic_product_feat() print 'get_basic_product_feat finsihed' # 滑窗,构造多组训练集/验证集 for i in range(setNums): print train_start_date if train_actions is None: train_actions = make_actions(user, product, all_actions, train_start_date) else: train_actions = pd.concat([train_actions, make_actions(user, product, all_actions, train_start_date)], ignore_index=True) # 接下来每次移动一天 train_start_date = datetime.strptime(train_start_date, '%Y-%m-%d') + timedelta(days=1) train_start_date = train_start_date.strftime('%Y-%m-%d') print \"round &#123;0&#125;/&#123;1&#125; over!\".format(i+1, setNums) train_actions.to_csv(f_path, index=False) 123# 训练集train_start_date = '2016-03-01'make_train_set(train_start_date, 34, 'train_set.csv') 构造线下测试集 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061def make_val_answer(val_start_date, val_end_date, all_actions, label_val_s1_path): actions = get_actions(val_start_date, val_end_date,all_actions) actions = actions[(actions['type'] == 4) &amp; (actions['cate'] == 8)] actions = actions[['user_id', 'sku_id']] actions = actions.drop_duplicates() actions.to_csv(label_val_s1_path, index=False)def make_val_set(train_start_date, train_end_date, val_s1_path): # 修改时间跨度 start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=30) start_days = start_days.strftime('%Y-%m-%d') all_actions = get_all_action() print \"get all actions!\" user = get_basic_user_feat() print 'get_basic_user_feat finsihed' product = get_basic_product_feat() print 'get_basic_product_feat finsihed'# user_acc = get_accumulate_user_feat(train_end_date,all_actions,30)# print 'get_accumulate_user_feat finished' user_acc = get_recent_user_feat(train_end_date, all_actions) print 'get_recent_user_feat finsihed' user_cate = get_user_cate_feature(train_start_date, train_end_date, all_actions) print 'get_user_cate_feature finished' product_acc = get_accumulate_product_feat(start_days, train_end_date, all_actions) print 'get_accumulate_product_feat finsihed' cate_acc = get_accumulate_cate_feat(start_days, train_end_date, all_actions) print 'get_accumulate_cate_feat finsihed' comment_acc = get_comments_product_feat(train_end_date) print 'get_comments_product_feat finished' actions = None for i in (3, 5, 7, 10, 15, 21, 30): start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=i) start_days = start_days.strftime('%Y-%m-%d') if actions is None: actions = get_action_feat(start_days, train_end_date, all_actions,i) else: actions = pd.merge(actions, get_action_feat(start_days, train_end_date,all_actions,i), how='left', on=['user_id', 'sku_id', 'cate']) actions = pd.merge(actions, user, how='left', on='user_id') actions = pd.merge(actions, user_acc, how='left', on='user_id') actions = pd.merge(actions, user_cate, how='left', on='user_id') # 注意这里的拼接key actions = pd.merge(actions, product, how='left', on=['sku_id', 'cate']) actions = pd.merge(actions, product_acc, how='left', on='sku_id') actions = pd.merge(actions, cate_acc, how='left', on='cate') actions = pd.merge(actions, comment_acc, how='left', on='sku_id') actions = actions.fillna(0) # print actions # 构造真实用户购买情况作为后续验证 val_start_date = train_end_date val_end_date = datetime.strptime(val_start_date, '%Y-%m-%d') + timedelta(days=5) val_end_date = val_end_date.strftime('%Y-%m-%d') make_val_answer(val_start_date, val_end_date, all_actions, 'label_'+val_s1_path) actions.to_csv(val_s1_path, index=False) 123456# 验证集# train_start_date = '2016-04-06'# make_train_set(train_start_date, 3, 'val_set.csv')make_val_set('2016-04-06', '2016-04-09', 'val_1.csv')make_val_set('2016-04-07', '2016-04-10', 'val_2.csv')make_val_set('2016-04-08', '2016-04-11', 'val_3.csv') 构造测试集123456789101112131415161718192021222324252627282930313233343536373839404142434445def make_test_set(train_start_date, train_end_date): start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=30) start_days = start_days.strftime('%Y-%m-%d') all_actions = get_all_action() print \"get all actions!\" user = get_basic_user_feat() print 'get_basic_user_feat finsihed' product = get_basic_product_feat() print 'get_basic_product_feat finsihed' user_acc = get_recent_user_feat(train_end_date, all_actions) print 'get_accumulate_user_feat finsihed' user_cate = get_user_cate_feature(train_start_date, train_end_date, all_actions) print 'get_user_cate_feature finished' product_acc = get_accumulate_product_feat(start_days, train_end_date, all_actions) print 'get_accumulate_product_feat finsihed' cate_acc = get_accumulate_cate_feat(start_days, train_end_date, all_actions) print 'get_accumulate_cate_feat finsihed' comment_acc = get_comments_product_feat(train_end_date) actions = None for i in (3, 5, 7, 10, 15, 21, 30): start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=i) start_days = start_days.strftime('%Y-%m-%d') if actions is None: actions = get_action_feat(start_days, train_end_date, all_actions,i) else: actions = pd.merge(actions, get_action_feat(start_days, train_end_date,all_actions,i), how='left', on=['user_id', 'sku_id', 'cate']) actions = pd.merge(actions, user, how='left', on='user_id') actions = pd.merge(actions, user_acc, how='left', on='user_id') actions = pd.merge(actions, user_cate, how='left', on='user_id') # 注意这里的拼接key actions = pd.merge(actions, product, how='left', on=['sku_id', 'cate']) actions = pd.merge(actions, product_acc, how='left', on='sku_id') actions = pd.merge(actions, cate_acc, how='left', on='cate') actions = pd.merge(actions, comment_acc, how='left', on='sku_id') actions = actions.fillna(0) actions.to_csv(\"test_set.csv\", index=False) 4.13~4.16这三天的评论记录似乎并不存在为0的情况，导致构建测试集时出错 KeyError: &quot;[&#39;comment_num_0&#39;] not in index&quot; 1234# 预测结果sub_start_date = '2016-04-13'sub_end_date = '2016-04-16'make_test_set(sub_start_date, sub_end_date) 模型设计和评估123456789101112#!/usr/bin/env python# -*- coding: UTF-8 -*-import sysimport pandas as pdimport numpy as npimport xgboost as xgbfrom sklearn.model_selection import train_test_splitimport operatorfrom matplotlib import pylab as pltfrom datetime import datetimeimport timefrom sklearn.model_selection import GridSearchCV 1234567891011121314151617181920# import gcdef show_record(): train = pd.read_csv('train_set.csv')# valid = pd.read_csv('val_set.csv')# label_val = pd.read_csv('label_val_set.csv') valid1 = pd.read_csv('val_1.csv') valid2 = pd.read_csv('val_2.csv') valid3 = pd.read_csv('val_3.csv')# test = pd.read_csv('test_set.csv') print train.shape# print valid.shape# print label_val.shape# print test.shape print valid1.shape print valid2.shape print valid3.shape# show_record()# del train, valid, test# gc.collect() 训练模型 返回训练后的模型 生成特征map文件作为后续特征重要性之用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667def create_feature_map(features): outfile = open(r'xgb.fmap', 'w') i = 0 for feat in features: outfile.write('&#123;0&#125;\\t&#123;1&#125;\\tq\\n'.format(i, feat)) i = i + 1 outfile.close()def xgb_model(train_set): actions = pd.read_csv(train_set) #read train_set # 单纯的删掉模型前一遍训练认为无用的特征（根据特征重要性中不存在的特征） lst_useless = ['brand'] actions.drop(lst_useless, inplace=True, axis=1) users = actions[['user_id', 'sku_id']].copy() labels = actions['label'].copy() del actions['user_id'] del actions['sku_id'] del actions['label'] # 尝试通过设置scale_pos_weight来调整政府比例不均的问题，但是经过采样的正负比为1:10，训练结果反而不如设置为1# ratio = float(np.sum(labels==0)) / np.sum(labels==1)# print ratio # write to feature map features = list(actions.columns[:]) print 'total features: ', len(features) create_feature_map(features) # 训练时即传入特征名# features = list(actions.columns.values) user_index=users training_data=actions label=labels X_train, X_valid, y_train, y_valid = train_test_split(training_data.values, label.values, test_size=0.2, random_state=0) # 尝试通过提前设置传入训练的正负例的权重来改善正负比例不均的问题# weights = np.zeros(len(y_train))# weights[y_train==0] = 1# weights[y_train==1] = 10 # dtrain = xgb.DMatrix(X_train, label=y_train, weight=weights) dtrain = xgb.DMatrix(X_train, label=y_train) dvalid = xgb.DMatrix(X_valid, label=y_valid)# dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=features)# dvalid = xgb.DMatrix(X_valid, label=y_valid, feature_names=features)# dtrain = xgb.DMatrix(training_data.values, label.values) param = &#123;'n_estimators': 4000, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight':10, 'eta': 0.1, 'silent': 1, 'objective': 'binary:logistic', 'eval_metric':'auc'&#125;# param = &#123;'n_estimators': 4000, 'max_depth': 6, 'seed': 7, 'min_child_weight': 5, 'gamma': 0, 'subsample': 1.0, # 'colsample_bytree': 0.8, 'scale_pos_weight': 1, 'eta': 0.09, 'silent': 1, 'objective': 'binary:logistic',# 'eval_metric':'auc'&#125; num_round = param['n_estimators']# param['nthread'] = 4 # param['eval_metric'] = \"auc\" plst = param.items() evallist = [(dtrain, 'train'), (dvalid, 'eval')]# evallist = [(dvalid, 'eval'), (dtrain, 'train')]# evallist = [(dtrain, 'train')] bst = xgb.train(plst, dtrain, num_round, evallist, early_stopping_rounds=10) bst.save_model('bst.model') return bst, featuresbst_xgb, features = xgb_model('train_set.csv') total features: 301 [0] train-auc:0.913108 eval-auc:0.911621 Multiple eval metrics have been passed: &apos;eval-auc&apos; will be used for early stopping. Will train until eval-auc hasn&apos;t improved in 10 rounds. [1] train-auc:0.932872 eval-auc:0.930423 [2] train-auc:0.936241 eval-auc:0.93338 ... [416] train-auc:0.982069 eval-auc:0.976755 [417] train-auc:0.982076 eval-auc:0.976751 [418] train-auc:0.982087 eval-auc:0.976753 Stopping. Best iteration: [408] train-auc:0.981964 eval-auc:0.976777 1print bst_xgb.attributes() {&apos;best_iteration&apos;: &apos;408&apos;, &apos;best_msg&apos;: &apos;[408]\\ttrain-auc:0.981964\\teval-auc:0.976777&apos;, &apos;best_score&apos;: &apos;0.976777&apos;} 对验证集进行线下测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546def report(pred, label): actions = label result = pred # 所有实际用户商品对 all_user_item_pair = actions['user_id'].map(str) + '-' + actions['sku_id'].map(str) all_user_item_pair = np.array(all_user_item_pair) # 所有购买用户 all_user_set = actions['user_id'].unique() # 所有预测购买的用户 all_user_test_set = result['user_id'].unique() all_user_test_item_pair = result['user_id'].map(str) + '-' + result['sku_id'].map(str) all_user_test_item_pair = np.array(all_user_test_item_pair) # 计算所有用户购买评价指标 pos, neg = 0,0 for user_id in all_user_test_set: if user_id in all_user_set: pos += 1 else: neg += 1 all_user_acc = 1.0 * pos / ( pos + neg) all_user_recall = 1.0 * pos / len(all_user_set) print '所有用户中预测购买用户的准确率为 ' + str(all_user_acc) print '所有用户中预测购买用户的召回率' + str(all_user_recall) pos, neg = 0, 0 for user_item_pair in all_user_test_item_pair: if user_item_pair in all_user_item_pair: pos += 1 else: neg += 1 all_item_acc = 1.0 * pos / ( pos + neg) all_item_recall = 1.0 * pos / len(all_user_item_pair) print '所有用户中预测购买商品的准确率为 ' + str(all_item_acc) print '所有用户中预测购买商品的召回率' + str(all_item_recall) F11 = 6.0 * all_user_recall * all_user_acc / (5.0 * all_user_recall + all_user_acc) F12 = 5.0 * all_item_acc * all_item_recall / (2.0 * all_item_recall + 3 * all_item_acc) score = 0.4 * F11 + 0.6 * F12 print 'F11=' + str(F11) print 'F12=' + str(F12) print 'score=' + str(score) return all_user_acc, all_user_recall, F11, all_item_acc, all_item_recall, F12, score 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465def validate(valid_set, val_label, model): actions = pd.read_csv(valid_set) #read test_set # users = actions[['user_id', 'sku_id']].copy() # 避免预测到非8类商品，所以最后还是再筛一遍的好 users = actions[['user_id', 'sku_id', 'cate']].copy() actions['user_id'] = actions['user_id'].astype(np.int64)# test_label= actions[actions['label'] == 1]# test_label= actions[(actions['label']==1) &amp; (actions['cate']==8)] test_label = pd.read_csv(val_label) lst_useless = ['brand'] actions.drop(lst_useless, inplace=True, axis=1) # test_label = test_label[['user_id','sku_id','label']] del actions['user_id'] del actions['sku_id'] # features = list(actions.columns.values) # del actions['label'] sub_user_index = users# sub_trainning_data = xgb.DMatrix(actions.values, feature_names=features) sub_trainning_data = xgb.DMatrix(actions.values)# y = model.predict(sub_trainning_data,ntree_limit=model.best_iteration) y = model.predict(sub_trainning_data, ntree_limit=model.best_ntree_limit) sub_user_index['label'] = y sub_user_index.to_csv('result_' + valid_set, index=False) # sub_user_index = sub_user_index[sub_user_index['cate']==8]# del sub_user_index['cate'] rank = 1000 pred = sub_user_index.sort_values(by='label', ascending=False)[:rank]# pred = sub_user_index[sub_user_index['label'] &gt;= 0.05] print 'No. of raw pred users: ', len(pred['user_id'].unique()) pred = pred[pred['cate']==8] print 'No. of pred users bought cate 8: ', len(pred['user_id'].unique()) # pred = pred[['user_id', 'sku_id']] pred = pred[['user_id', 'sku_id', 'label']] pred = pred.groupby('user_id').first().reset_index() pred['user_id'] = pred['user_id'].astype(int) pred['sku_id'] = pred['sku_id'].astype(int) # print 'No. of pred users after deduplicates: ', len(pred['user_id'].unique()) true_user = len(test_label['user_id']) pred_ui = len(pred['user_id'].unique()) print 'pred item: ', len(pred['sku_id'].unique()) print 'true users: ', true_user print 'pred users: ', pred_ui test_label['user_id'] = test_label['user_id'].astype(int) test_label['sku_id'] = test_label['sku_id'].astype(int) all_user_acc, all_user_recall, F11, all_item_acc, all_item_recall, F12, score = report(pred, test_label) f_name = 'pred_' + str(rank) + '_' + valid_set pred.to_csv(f_name, index=False) return rank, true_user, pred_ui, all_user_acc, all_user_recall, F11, all_item_acc, all_item_recall, F12, score# validate('val_set.csv', bst_xgb) 评分文件 1234567891011121314151617181920212223242526272829303132def avg_score(): rank1, true_user1, pred_ui1, user_acc1, user_recall1, F11_1, item_acc1, item_recall1, F12_1, score1 = validate('val_1.csv', 'label_val_1.csv', bst_xgb) print '-------------------------------------------' rank2, true_user2, pred_ui2, user_acc2, user_recall2, F11_2, item_acc2, item_recall2, F12_2, score2 = validate('val_2.csv', 'label_val_2.csv', bst_xgb) print '-------------------------------------------' rank3, true_user3, pred_ui3, user_acc3, user_recall3, F11_3, item_acc3, item_recall3, F12_3, score3 = validate('val_3.csv', 'label_val_3.csv', bst_xgb) print '===========================================' print 'avg user acc: ', (user_acc1+user_acc2+user_acc3)/3 print 'avg user recall: ', (user_recall1+user_recall2+user_recall3)/3 print 'avg item acc: ', (item_acc1+item_acc2+item_acc3)/3 print 'avg item recall: ', (item_recall1+item_recall2+item_recall3)/3 print 'avg F11: ', (F11_1+F11_2+F11_3)/3 print 'avg F12: ', (F12_1+F12_2+F12_3)/3 print 'avg score: ', (score1+score2+score3)/3 # make the csv file dct_score = &#123;&#125; dct_score['rank'] = [rank1, rank2, rank3] dct_score['true_user'] = [true_user1, true_user2, true_user3] dct_score['pred_ui'] = [pred_ui1, pred_ui2, pred_ui3] dct_score['user_acc'] = [user_acc1, user_acc2, user_acc3] dct_score['user_recall'] = [user_recall1, user_recall2, user_recall3] dct_score['F11'] = [F11_1, F11_2, F11_3] dct_score['item_acc'] = [item_acc1, item_acc2, item_acc3] dct_score['item_recall'] = [item_recall1, item_recall2, item_recall3] dct_score['F12'] = [F12_1, F12_2, F12_3] dct_score['score'] = [score1, score2, score3] column_order = ['rank', 'true_user', 'pred_ui', 'user_acc', 'user_recall', 'item_acc', 'item_recall', 'F11', 'F12', 'score'] df_score = pd.DataFrame(dct_score) file_name = 'score_' + str(datetime.now().date())[5:] +'_'+ str(rank1) + '.csv' df_score[column_order].to_csv(file_name, index=False)avg_score() No. of raw pred users: 950 No. of pred users bought cate 8: 950 pred item: 220 true users: 1211 pred users: 950 所有用户中预测购买用户的准确率为 0.147368421053 所有用户中预测购买用户的召回率0.116569525396 所有用户中预测购买商品的准确率为 0.108421052632 所有用户中预测购买商品的召回率0.0850536746491 F11=0.141152747437 F12=0.0930778962588 score=0.11230783673 ------------------------------------------- No. of raw pred users: 950 No. of pred users bought cate 8: 950 pred item: 203 true users: 1259 pred users: 950 所有用户中预测购买用户的准确率为 0.163157894737 所有用户中预测购买用户的召回率0.12360446571 所有用户中预测购买商品的准确率为 0.116842105263 所有用户中预测购买商品的召回率0.0881652104845 F11=0.15489673551 F12=0.0977629029417 score=0.120616435969 ------------------------------------------- No. of raw pred users: 960 No. of pred users bought cate 8: 960 pred item: 219 true users: 1385 pred users: 960 所有用户中预测购买用户的准确率为 0.161458333333 所有用户中预测购买用户的召回率0.11231884058 所有用户中预测购买商品的准确率为 0.120833333333 所有用户中预测购买商品的召回率0.0837545126354 F11=0.150485436893 F12=0.0954732510288 score=0.117478125375 =========================================== avg user acc: 0.157328216374 avg user recall: 0.117497610562 avg item acc: 0.115365497076 avg item recall: 0.0856577992563 avg F11: 0.14884497328 avg F12: 0.0954380167431 avg score: 0.116800799358 输出特征重要性12345678910def feature_importance(bst_xgb): importance = bst_xgb.get_fscore(fmap=r'xgb.fmap') importance = sorted(importance.items(), key=operator.itemgetter(1), reverse=True) df = pd.DataFrame(importance, columns=['feature', 'fscore']) df['fscore'] = df['fscore'] / df['fscore'].sum() file_name = 'feature_importance_' + str(datetime.now().date())[5:] + '.csv' df.to_csv(file_name)feature_importance(bst_xgb) 生成提交结果12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# sub_filedef submit(pred_set, model): actions = pd.read_csv(pred_set) #read test_set # print 'total user before: ', len(actions['user_id'].unique())# potential = pd.read_csv('potential_user_04-28.csv')# lst_user = potential['user_id'].unique().tolist()# actions = actions[actions['user_id'].isin(lst_user)]# print 'total user after: ', len(actions['user_id'].unique()) # 提前去掉部分特征 lst_useless = ['brand'] actions.drop(lst_useless, inplace=True, axis=1) users = actions[['user_id', 'sku_id', 'cate']].copy()# users = actions[['user_id', 'sku_id']].copy() actions['user_id'] = actions['user_id'].astype(np.int64) del actions['user_id'] del actions['sku_id'] sub_user_index = users sub_trainning_data = xgb.DMatrix(actions.values) y = model.predict(sub_trainning_data, ntree_limit=model.best_ntree_limit) sub_user_index['label'] = y # sub_user_index = sub_user_index[sub_user_index['cate']==8]# del sub_user_index['cate'] rank = 1200 pred = sub_user_index.sort_values(by='label', ascending=False)[:rank]# pred = sub_user_index[sub_user_index['label'] &gt;= 0.05]# pred = pred[['user_id', 'sku_id', 'label']]# pred = pred[pred['label']&gt;0.45] print 'No. of raw pred users: ', len(pred['user_id'].unique()) pred = pred[pred['cate']==8] print 'No. of pred users bought cate 8: ', len(pred['user_id'].unique()) pred = pred[['user_id', 'sku_id']]# print pred = pred.groupby('user_id').first().reset_index() pred['user_id'] = pred['user_id'].astype(int) pred['sku_id'] = pred['sku_id'].astype(int) sub_file = 'submission_' + str(rank) + '_' + str(datetime.now().date())[5:] + '.csv'# sub_file = 'submission_detail_' + str(datetime.now().date())[5:] + '.csv' pred.to_csv(sub_file, index=False, index_label=False) submit('test_set.csv', bst_xgb) No. of raw pred users: 1142 No. of pred users bought cate 8: 1142 提交结果除重（可选）除去最后临近三天的发生购买行为的用户商品对 123456789101112131415161718192021222324252627282930from datetime import datetimedef sub_improv(action, sub_file): # 获取4月最近三天的目标用户商品对 action_4 = pd.read_csv(action) action_4['time'] = pd.to_datetime(action_4['time']).apply(lambda x: x.date()) aim_date = [datetime.strptime(s, '%Y-%m-%d').date() for s in ['2016-04-09', '2016-04-10', '2016-04-11' , '2016-04-12', '2016-04-13', '2016-04-14', '2016-04-15']] aim_action = action_4[(action_4['type']==4) &amp; (action_4['cate']==8) &amp; (action_4['time'].isin(aim_date))] aim_ui = aim_action['user_id'].map(int).map(str) + '-' + aim_action['sku_id'].map(str) # 拼接提交数据的用户商品 sub = pd.read_csv(sub_file) before = sub.shape[0] sub_ui = sub['user_id'].map(str) + '-' + sub['sku_id'].map(str) # 交集 lst_aim = aim_ui.unique().tolist() lst_sub = sub_ui.unique().tolist() lst_common = [i for i in lst_aim if i in lst_sub] dct_ui = &#123;i.split('-')[0]: i.split('-')[1] for i in lst_common&#125; # 从提交结果除掉交集部分 for k in dct_ui: sub.drop(sub[(sub['user_id']==int(k)) &amp; (sub['sku_id']==int(dct_ui[k]))].index, inplace=True) print 'No. of records after remove dup: ', sub.shape[0] print 'No. of dup: ', before - sub.shape[0] if (before - sub.shape[0])!=0: file_name = 'submission_' + str(datetime.now().date())[5:] + '_improv.csv' sub.to_csv(file_name, index=False, index_label=False) sub_improv('data/JData_Action_201604.csv', 'submission_1200_05-25.csv') No. of records after remove dup: 1142 No. of dup: 0 比赛过程中遇到和解决的问题线上线下测试的分相差悬殊问题传入report的真实预测用户商品对有问题，之前使用滑动窗口时仅用前三天预测后五天，对后五天发生了购买(类别8)行为的用户打上标签，生成训练集、验证集，这里隐式的划定了前三天有交互的用户群体，但是事实在后五天发生购买行为的用户应该理论上不小于这个数目(存在用户前三天并无交互但是在后五天发生购买行为的情况)，所以在计算report中的callback等值时传入的应该是这5天实际购买的用户行为记录，修改后，线上线下测试误差控制在0.01左右 线下测试集组数开始只设定了一组，导致有时候线下线上的变化不一致，后期设定了三组并结合三组的输出以及均值判断，基本能保证线上线下成绩同步 编码问题文件本身是gbk编码，其中的年龄属性就是中文的，导致转换时经常出现编码错误，后来修改了独热编码部分的代码，一开始对年龄这类含有中文的特征进行独热编码前，先将文本转化为数值（用数字表示类别）LabelEncoder + get_dummies()来源 feature_importance列未对应，导致一段时间分析的特征重要性文件存在问题，产生误导合并数据集时产生空值，由数据不一致造成手动添加一些特征列并填充缺失值，此外注意拼接的键 除零出现无限值平滑部分除零特征 优化时间计算效率开始时，构建时间差的地方用到了日期和字符串转换等操作，apply等比较费时，后面不使用apply而且一次性全部计算所有时间差并优先存储到数据中直接读入，提升了特征构建的速度 失败的尝试以及未尝试的 是否自定义衡量标准，而不使用roc-auc? 参照 正负例比例不均的问题是否使用权重来矫正？参照 参数：scale_pos_weight DMatrix中的weight 尝试设置正负比例的参数，特别分阶段而且没有采样的条件下，不同阶段模型里的正负例是不一样的，尝试过，可能是提前随机欠抽样导致本身数据的随机性而再修改该属性反而没有效果；但是考虑到本身使用滑窗会产生大量数据，若不采样则机器内存不足以承受，而且后面尝试另一种滑窗的方法并不采样使用scale_pos_weight却也并无改善，可能需要参考《机器学习》使用EasyEnsemble的方法 分阶段预测第一阶段预测用户，第二阶段预测前一阶段预测出的用户可能购买的商品设计特征如何分割和构建的问题以及标签的问题没有解决，导致失败 打标签方式的尝试差别不大 时间窗的修改最后是要预测五天内的购买，我们可以先随机取几个五天发生购买的用户，看看这些用户与所购买商品的交互时间周期覆盖率较大的一个值，然后反过来取另外的时间周期的交互用户商品对，看看这些里面有多少在时间周期后五天发生了购买行为，主要是交互周期的一个覆盖率问题将对这三个月购买过的类别8商品后五天之前14天有过交互的所有用户商品对提取出来，而这里面根据前面所分析的，可以涵盖所有存在交互的用户商品对的80%的用户但实际上虽然这14天能覆盖到80%的用户商品对，但是其实这部分用户商品对在不少交互其实只是在这14天的前面的某几天出现过，而后续并未进一步交互，虽然最后发生了购买行为，但是对于我们通过统计交互行为来构建特征的方法并没有什么帮助，反而扩大的时间窗囊括进了大量的噪声数据致使结果变糟 平滑特征 去重 填充空值 优化问题，如何加速训练 时间窗口构造上避开统计结果中的高峰、低谷时间区间效果反而变差 其他模型以及模型融合尝试过lightgbm,randomforest,gbdt，以及averaging但是效果不佳 后记以上大概是本次比赛的一个简要的流程，完整的代码可以到我的github查看，下面简要回顾下本次比赛的收获和有待进一步改进的地方 团队合作很重要，特别是队员之间工作协调好，此外code review非常重要 对数据的分析是很关键的，只有通过统计分析才能说明你对数据的了解，你以为的常识是靠不住的 有想法在分析过后的基础上最好马上实践来验证，并做好相应的记录 做好分析记录很重要，无论是过程中的数据分析、特征尝试还是模型的参数调整；另外后面线下验证的过程也有必要做记录，毕竟线上测试机会有限，线下测试做的好、记录完备可以提高成绩 如开头所述，如何选取恰当的时间区间来预测用户商品对以及最近一段时间没有交互记录的用户商品对该如何挖掘出来是个难题 本次最终只提交了xgboost单模型的预测，后期也有尝试lightgbm、RandomForest、GBDT等模型，但是效果不是很好，而且时间仓促，模型集成这块也来不及做，尝试了bagging但是效果不是很好 数据本身存在正负比例不均的问题，查阅过一些资料，最后使用的是随机欠采样负例按与正例1：10的比例构建训练集，但是这种方法随机性较大，造成结果不稳定，对训练集敏感性较强，应该还有更好的方法 特征筛选问题，中间为了快速提高成绩，一味的添加特征，对于特征却没有进行适当的筛选，后期聚集了上百个特征却难以筛选，尝试过方差法、通过xgboost模型本身输出的特征重要性来筛选，但是效果不佳，这一块应该还有很多提升空间 关于分步预测的问题，也有尝试过按照群里大神提到的分阶段预测，即第一阶段仅预测未来可能会购买的用户，第二阶段预测这部分用户可能购买的商品，然而中间尝试很长一段时间效果不好，主要是不同阶段之间特征的取舍和冲突，最终效果不好而作罢，但是不失为一种思路","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"JData","slug":"JData","permalink":"http://yoursite.com/tags/JData/"},{"name":"比赛","slug":"比赛","permalink":"http://yoursite.com/tags/比赛/"}]},{"title":"概率统计小记","date":"2017-06-22T02:53:49.000Z","path":"2017/06/22/Statistics/","text":"似然函数知乎上的一些理解wiki 最大似然估计知乎wiki 最大后验概率最大似然估计 vs 最大后验概率","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"统计学","slug":"统计学","permalink":"http://yoursite.com/tags/统计学/"}]},{"title":"统计学习方法笔记(五) —— 决策树","date":"2017-06-19T08:41:25.000Z","path":"2017/06/19/Decision-Tree/","text":"决策树是一种基本的分类与回归方法，其模型呈树形结构，在分类问题中表示基于特征对实例进行分类的过程，可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布； 其主要优点是模型具有可读性，分类速度快； 学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型，预测时，对新的数据，利用决策树模型进行分类； 决策树学习的三个主要步骤 特征选择 决策树的生成 决策树的剪枝 常用决策树算法 ID3 C4.5 CART Notes:关于损失函数最小化可以回顾第一章节的模型选择部分的内容。 决策树模型定义分类决策树模型是一种描述对实例进行分类的树形结构，其中包含两种类型的节点 内部节点：表示一个特征（属性） 叶节点：表示一个类 Notes:一般决策树可以根据以下四个方面划分 分支数 划分策略 终止策略 基分类器 if-then规则集合 一条由根节点到叶节点的路径 –&gt; 一条规则 路径上内部节点的特征 –&gt; 规则的条件 叶节点的类 –&gt; 规则的结论 性质：互斥且完备 条件概率分布给定特征条件下类的条件概率分布 决策树的学习 决策树学习本质上是从训练数据集中归纳出一组分类规则，另一个角度，学习是由训练数据集估计条件概率模型 目的：得到一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力 策略：以损失函数（通常为正则化的极大似然函数）为目标函数的最小化，并在损失函数确定后，选择最优决策树 学习算法： 理论上：从所有可能的决策树中选取最优决策树，NP完全问题 实际中：采用启发式方法，近似求解（得到次最优决策树）–&gt; 递归的选择最优特征，并根据该最优特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。 主要步骤： 特征选择 决策树的生成 决策树的剪枝 Notes: 决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。 特征选择 实质：选取对于训练数据具有分类能力的特征（决定用哪个特征来划分特征空间） 常用准则 信息增益 –&gt; ID3 信息增益比 –&gt; C4.5 基尼指数 –&gt; CART Notes: 分类能力强即表示给定一个特征使得实例能较准确地被分类，及减少了实例的不确定性，掌握了更多的信息（信息增益角度的理解） 每次挑一个特征（列），根据该特征下各记录的不同取值来划分实例点（过程让我联想起k-nn中kd树的构建）–&gt; 所以一般要求特征值为离散值，或者事先对特征进行离散化处理 信息增益定义$$g(D, A)=H(D)-H(D|A)$$ $g(D, A)$即信息增益，表示得知特征$A$的信息而使得类$D$的信息的不确定性减少的程度 $H(D)$为集合$D$的经验熵 其中假设$D$是一个取有限个值的离散随机变量，概率分布为$P(X=x_i)=p_i, i=1, 2,…,n$ 熵是表示随机变量不确定性的度量，定义$H(D)=- \\sum_{i=1}^n p_ilogp_i$，熵越大，随机变量的不确定性就越大，$0 \\leq H(D) \\leq logn$ $H(D|A)$即经验条件熵表示在已知随机变量$A$（特征）的条件下随机变量$D$的不确定性$H(D|A)= \\sum_{i=1}^{n}p_iH(D|A=a_i)$ 一般将熵$H(D)$与条件熵$H(D|A)$之差称为互信息，决策树学习中的信息增益等价于训练数据集中类与特征的互信息。 小结 给定训练数据集$D$和特征$A$，经验熵$H(D)$表示对数据集$D$进行分类的不确定性，而经验条件熵$H(D|A)$表示在特征$A$给定的条件下对数据集进行分类的不确定性，因此两者之差即信息增益表示由于特征$A$而使得数据集$D$的分类的不确定性减少的程度。 对于数据集$D$而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益，信息增益大的特征具有更强的分类能力（也就是我们需要挑选的目标） 算法——特征选择 输入：训练数据集$D$和特征$A$； 输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$ (1) 计算数据集$D$的经验熵$H(D)$，$$H(D)=- \\sum_{k=1}^{K}\\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|}$$ (2) 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$,$$H(D|A)= \\sum_{i=1}^{n} \\frac{|D_i|}{|D|}H(D_i)=- \\sum_{i=1}^{n} \\frac{|D_i|}{|D|} \\sum_{k=1}^{K} \\frac{|D_{ik}|}{|D_i|}log_2 \\frac{|D_{ik}|}{|D_i|}$$ (3) 计算信息增益$g(D, A)$,$$g(D|A)=H(D)-H(D|A)$$其中假设训练数据集$D$，$|D|$表示样本个数，设有$K$个类$C_k$, $|C_k|$表示属于类$C_k$的样本个数，根据特征$A$的$n$个不同的取值将训练数据集$D$划分为$n$个子集$D_1, D_2,…,D_n$，其中$|D_i|$表示子集$D_i$中的样本数，记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$ Notes:上述过程是一次特征挑选的过程，首先计算原始数据的经验熵，然后对每一个特征（轮流判断），首先根据该特征的所有不同取值将原始数据进行划分，得到的数据子集分别计算该子集的经验熵，然后按照该子集本身样本占比加权（按概率）加和起来作为经验条件熵，两相作差即为该特征对应的信息增益值，而在计算所有特征对应的信息增益之后，我们选出其中信息增益最大的特征作为分割的特征对数据集进行划分，而且后面涉及到决策树的生成过程就是一个递归的对子数据集挑选最佳特征然后划分数据集的过程。 如果觉得抽象可以参照原书P62的例子，或者参考《机器学习实战》中决策树章节的ID3算法的实现就能大概知道是什么情况了。 信息增益比 以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比可以对这一问题进行校正。 信息增益比定义特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比：$$g_R(D,A)= \\frac{g(D,A)}{H_A(D)}$$其中$$H_A(D)=- \\sum_{i=1}^{n} \\frac{|D_{i}|}{|D|}log_2 \\frac{|D_{i}|}{|D|}$$$n$为特征$A$的取值个数，$D_i$表示据特征$A$的取值将$D$分成的子集 决策树的生成ID3核心思想 在决策树的各个结点上应用信息增益准则选择特征，递归地构建决策树。 递归终止条件：所有特征的信息增益（设置信息增益的阈值来判断是否进一步划分）均很小或没有特征可以选择（每选择一个特征则后期划分子树不再使用前面使用过的特征，因为子树已经是在该特征下属于同一取值的实例集合）为止。 ID3相当于用极大似然法进行概率模型的选择。 算法-决策树生成 输入：训练数据集$D$和特征集$A$，阈值$\\varepsilon$； 输出：决策树$T$ (1) （叶子结点）若$D$中所有实例属于同一类$C_k$,则$T$为单节点树，并将类$C_k$作为该结点的类标记，返回$T$ (2) (终止条件之没有特征可供选择)若$A= \\emptyset$,则$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记（多数表决规则），返回$T$ (3) 否则计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$ (4) (终止条件之阈值)若$A_g$的信息增益小于阈值$\\varepsilon$，则置$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$ (5) 否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将$D$划分为若干非空子集$D_i$，并将$D_i$中实例数最大的类作为标记构建子节点，返回$T_i$ (6) 对第$i$个子节点，以$D_i$为训练集，$A- \\lbrace A_g \\rbrace$为特征集，递归地调用(1)~(5)得到子树$T_i$并返回。 C4.5C4.5算法与ID3算法类似，不同之处在于，C4.5在生成的过程中，用信息增益比来选择特征。 Notes:上述决策树的生成算法只有树的生成，而且是针对训练集构造的树，容易产生过拟合。 决策树的剪枝过拟合 过拟合产生的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树，解决该问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。 Notes:决策树模型的复杂度主要受到叶子节点数目和树的深度的影响 剪枝定义在决策树学习中将已生成的树进行简化的过程 实现决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现 决策树的损失函数$$C_\\alpha(T)= \\sum_{t=1}^{|T|}|N_t|H_t(T)+ \\alpha|T|$$其中，树$T$的叶节点数为$|T|$，叶节点$t$有$|N_t|$个样本点，其中属于$k$类的数目为$|N_{tk}|$个 其中经验熵$H_t(T)$为$$H_t(T)=- \\sum_{k=1}^{K} \\frac{|N_{tk}|}{|N_t|}log_2 \\frac{|N_{tk}|}{|N_t|}$$ 令$C(T)=\\sum_{t=1}^{|T|}|N_t|H_t(T)$，则损失函数可表示为$$C_\\alpha(T)=C(T)+\\alpha|T|$$$C(T)$表示模型对训练数据的预测误差（个人理解为叶节点的经验熵与该叶节点的样本点数之积的加和，类似一种叶节点的总体的不确定性），即模型与训练数据的拟合程度，$|T|$表示模型的复杂度（叶节点数），参数$\\alpha$控制两者之间的影响 Notes: 剪枝就是当$\\alpha$确定时，选择损失函数最小的模型，及损失函数最小的子树。 利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。 算法-决策树剪枝 输入：生成算法产生的整棵树$T$，参数$\\alpha$ 输出：修剪后的子树$T\\alpha$ (1)计算每个结点的经验熵 (2)递归地从叶节点向上回缩:设叶节点回缩到其父节点之前与之后的整体树分别为$T_B$和$T_A$，如果其对应的损失函数有：$$C_{\\alpha}(T_A) \\leq C_{\\alpha}(T_B)$$则进行剪枝，即将父节点变为新的叶结点(关于这里叶节点的类别标记应该仍是以多数表决的方法)。 (3)返回(2)直至不能继续，得到损失函数最小的子树$T\\alpha$。 Notes:在向上回缩过程中判断损失函数值大小时，计算可以在局部进行，所以决策树的剪枝算法可以由一种动态规划的算法实现 CART(classification and regression tree)算法与ID3,C4.5的区别 CART假设决策树是二叉树，而ID3,C4.5生成的过程中并无此假设，这也导致了两者的根本不同，ID3,C4.5每次选择出最佳特征之后，是按照该特征的每一个取值划分子树；而CART则是对每一个特征、每一个特征的每一个取值计算基尼指数（分类树）然后从所有特征、所有特征对应的取值计算所得的基尼指数中最小的特征及特征值作为切分点来划分子树，而划分依据则是判断实例对应特征的值是否等于该选定的特征值 子树划分（特征选择）的准则不同 回归树，平方误差最小化准则 分类树，基尼指数最小化准则 CART生成回归树理解设$X, Y$分别为输入和输出变量，其中$Y$为连续变量，给定训练数据集$D= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $ 一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值，所以我们的主要目的是要构建回归树，也就是如何划分输入空间，因为一旦划分好输入空间，如将输入空间划分为$M$个单元$R_1, R_2,…,R_M$,并且在每个单元$R_m$上有一个固定的输出值$c_m$，那么回归树的模型就可以表示为$$f(x)=\\sum_{m=1}^Mc_mI(x \\in R_m)$$ 如何划分输入空间？仍然是采用启发式的方法（尝试），假设我们取$j$的某个值$s$来划分实例点，就可以根据各实例点对应该特征$j$的取值与选定的特征值$s$比较大小来划分，即$$R_1(j, s)= \\lbrace x|x^{(j)} \\leq s \\rbrace$$$$R_2(j, s)= \\lbrace x|x^{(j)} &gt; s \\rbrace$$ 如何选择最优切分点$s$和对应的切分变量$j$?逐一计算和比较，比较的标准是平方误差最小化，即$$min_{j,s} \\left[ min_{c_1} \\sum_{x_i \\in R_1(j,s)} (y_i-c_1)^2 + min_{c_2} \\sum_{x_i \\in R_2(j,s)} (y_i-c_2)^2 \\right]$$,这里的$c_1, c_2$就是各个区域的输出，那么各个区域的输出又是怎么得到的呢？ 如何得到各区域的输出？仍然是依照平方误差最小准则，计算$\\sum_{x_i \\in R_m}(y_i-f(x_i))^2$，所以计算得到的单元$R_m$上的$c_m$的最优值刚好是$R_m$上所有输入实例$x_i$对应的输出$y_i$的均值，即$$\\hat{c_m}=ave(y_i|x_i \\in R_m)$$ 小结一下，通过计算各区域的实例对应的输出的均值可以得到当前尝试的划分点和划分变量划分的各区域的输出，从而可以计算和比较不同的切分点和切分变量下的误差，并进一步根据平方误差最小化准则从中选取出最优的切分点和切分变量，而确定了切分点和切分变量就相当于确定了区域的一个划分，接下来针对已划分的区域进一步根据需求进行划分或者停止划分就最终将输入空间划分为多个区域，而每个区域对应有一个确定的输出值，也就是构建好了一颗回归树 算法-CART回归树 输入：训练数据集$D$ 输出：回归树$f(x)$在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树 (1) 选择最优切分点$s$和切分变量$j$，求解$$min_{j,s} \\left[ min_{c_1} \\sum_{x_i \\in R_1(j,s)} (y_i-c_1)^2 + min_{c_2} \\sum_{x_i \\in R_2(j,s)} (y_i-c_2)^2 \\right]$$遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小的$s, j$ (2) 用选定的$s, j$划分区域并决定相应的输出值：$$R_1(j, s)= \\lbrace x|x^{(j)} \\leq s \\rbrace; R_2(j, s)= \\lbrace x|x^{(j)} &gt; s \\rbrace$$$$\\hat{c_m}=ave(y_i|x_i \\in R_m)$$ (3) 继续对两个子区域调用步骤(1),(2)直至满足停止条件 (4) 将输入空间划分为$M$个区域$R_1, R_2,…,R_M$，生成决策树$$f(x)=\\sum_{m=1}^Mc_mI(x \\in R_m)$$ 分类树分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点 基尼指数(不纯度)Notes:基尼指数是一种比较流行的选择，此外也可以用分类错误来计算(参考机器学习技法) 定义分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$,则概率分布的基尼指数定义为$$Gini(p)=\\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^Kp_k^2$$对于给定的样本集合$D$,其基尼指数为$$Gini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|}\\right)^2$$,其中，$C_k$为$D$中属于第$k$类的样本子集，$K$是类的个数 特征$A$的条件下，集合$D$的基尼指数$$Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$$，其中样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1, D_2$两部分$$D_1=\\lbrace (x, y) \\in D|A(x)=a \\rbrace; D_2=D-D_1$$ 基尼指数$Gini(D)$表示集合$D$的不确定性，基尼指数$Gini(D,A)$表示经$A=a$分割后集合$D$的不确定性，基尼指数值越大，样本集合的不确定性也就越大。 算法-CART分类树 输入：训练数据集$D$，停止计算的条件（如结点中样本个数小于预定阈值，或样本集的基尼指数小于预定阈值(样本基本属于同一类)，或者没有更多的特征） 输出：CART决策树从根结点开始，递归地对每个结点进行以下操作： (1) 设结点的训练数据集为$D$，计算现有特征对该数据集的基尼指数，对每一个特征$A$，对其可能的每一个取值$a$，根据样本点对$A=a$的测试为“是”或“否”将$D$分割成$D_1$和$D_2$两部分，计算$A=a$时的基尼指数。 (2) 在所有可能的特征$A$以及它们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。 (3) 对两个节点递归地调用(1),(2)直至满足停止条件 (4) 生成CART决策树 CART剪枝基本原理 首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个子树序列$\\lbrace T_0,T_1,…,T_n \\rbrace$ 然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树 剪枝过程理解 基本原理如上所述，就是由先前生成的CART决策树开始逆向剪枝得到一系列不同的子树集合，然后使用另一份验证集来对上述子树测试选出其中损失函数最小的子树 而由于我们根据损失函数最小来选取决策树，而由损失函数的定义$$C_\\alpha(T)= \\sum_{t=1}^{|T|}|N_t|H_t(T)+ \\alpha|T|$$而我们主要是通过调整$\\alpha$的值来权衡模型对训练集的预测能力和模型复杂度，因此对于固定的$\\alpha$必然存在使损失函数$C_\\alpha(T)$最小的子树，而极端情况下，前面根据CART生成算法得到的决策树$T_0$可认为是模型复杂度最大时的树，即此时可认为$\\alpha=0$，而当$\\alpha=+\\infty$时对应的是以根结点为单节点树的简单模型 如此一来对于每一个$\\alpha$的值，存在一个不同的子树模型，我们可以通过慢慢的从原始生成树中剪去叶节点并收集新得到的决策树，最后可以得到一个决策树的集合，而这个修剪和收集的过程，为了保证收集到完备的决策树序列，我们每次在原始生成树的基础上比较每个结点的剪枝前后的损失函数减少程度（即比较以该节点为根的节点的模型的损失与以该节点为单节点的模型的损失之差），并选取损失函数减少程度最小的一个子树部分，从原始生成树中剪去该子树得到新的子树保存到子树序列，并作为进一步剪枝的起始子树，直到最后得到以根节点为单节点模型。 Notes:机器学习技法中的CART剪枝过程似乎有所不同，不是穷尽所有决策树，而是从完全决策树逆向剪枝，每次减去某个叶子然后得到这‘一层’间之后的决策树的模型中误差最小的，然后进行下一次剪枝，最后从这些决策树中根据泛化误差来选出最佳决策树。 算法-CART剪枝 输入：CART算法生成的决策树$T_0$ 输出：最优决策树$T_{\\alpha}$ (1) 设$k=0, T=T_0$ (2) 设$\\alpha = +\\infty $ (3) 自下而上对各内部结点$t$计算$C(T_t), |T_t|$，以及$$g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1}$$$$\\alpha=min(\\alpha,g(t))$$其中$T_t$表示以$t$为根节点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶节点个数 (4) 对$g(t)=\\alpha$的内部节点$t$进行剪枝，剪去$g(t)$最小的子树，并对叶节点$t$以多数表决法决定其类，得到树$T$ (5) 设$k=k+1, \\alpha_k=\\alpha, T_k=T$ (6) 若$T_k$不是由根结点及两个叶结点构成的树，就返回(3)，否则令$T_k=T_n$(即最后一次剪枝得到根结点为单节点模型) (7) 采用交叉验证在剪枝得到的子树序列$T_0,T_1,…,T_n$中选取最优子树$T_{\\alpha}$ 决策树算法对比如果按照特征选择、决策树生成和决策树剪枝三个步骤来比较的话 在特征选择上，三个决策树算法的选择标准都不一样：ID3(信息增益)、C4.5(信息增益比)、CART(分类：基尼指数，回归：最小二乘法)；此外就分类而言，ID3和C4.5分别是选择是信息增益、信息增益比最大的特征而CART则选择基尼指数最小的特征； ID3、C4.5每次选择特征并进行分支后便不再使用该特征，而CART则不同，因为CART每次是具体到选择某一特征的某一切分值，所以会存在特征的复用情况(当然是用过的特征值就不再使用了) 在决策树生成时：三者的终止条件大致类似（预设阈值(叶节点中的样本数的阈值以及信息增益、基尼指数的阈值)、特征集合非空等），但是ID3和C4.5生成过程中是根据每一个特征值来生成一个对应子树，而CART则是根据特征值是否等于选定的特征值来进行二分（回归中则是根据大于小于某选定值划分区域）；也就是说前两者是树，而CART是二叉树（而这也大概决定了为什么原始的CART可以用来做回归，而前两者不行）； 决策树剪枝方面：标准都是判断决策树整体的损失函数（预测误差+正则化项（模型复杂度））；不同的是，ID3,C4.5是局部的对比叶节点删除前后的损失来决定是否剪枝，而CART则是自下而上从最复杂的完整二叉树一直剪枝到最简单的单节点决策树得到一系列决策树，并最后在验证集上验证选择其中最佳的决策树（每次剪枝为了保证最后得到的决策树序列尽可能完备，每次选择前后损失函数减少最小的一种剪枝策略） 决策树生成算法ID3的实现按照机器学习实战第三章节的内容实现了简单的ID3算法 特征选择 计算熵 计算条件熵 计算信息增益 递归构建决策树并使用matplotlib实现决策树的可视化 使用pickle序列化保存生成的决策树模型 实例：使用决策树模型预测隐形眼镜的类型具体实验代码见github 决策树模型的优缺点参考机器学习技法 优点 可解释性–模拟人类决策过程 训练、预测效率较高–关于其切分方式，每次是在一个条件下的局部空间划分样本，而类似Adaboost则是每次在整个空间划分样本，所以就决策树而言相对高效 适用于类别类型数据–decision set(穷举类别特征值然后按照特征值的子集集合来划分样本) 能够很方便的由二分类模型转换为多分类模型–主要修改不纯度计算以及返回值的设置 能够处理缺失特征值–用其他的特征值来替代进行划分(一般要求替代特征划分结果接近缺失特征值) 易于实现 缺点 经验多于理论，大多数决策树模型是根据经验来判断的，效果好坏尚无较好的理论支撑 参考资料 《统计学习方法》 《机器学习实战》 机器学习技法","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"decision tree","slug":"decision-tree","permalink":"http://yoursite.com/tags/decision-tree/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"}]},{"title":"统计学习方法笔记(四) —— 朴素贝叶斯法","date":"2017-06-09T11:59:44.000Z","path":"2017/06/09/naive-bayes/","text":"朴素贝叶斯法是基于贝叶斯定理和特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布，然后基于此模型，对给定的输入$x$，利用贝叶斯定理求出后验概率最大的输出$y$。 关键词：贝叶斯定理、特征条件独立假设、实现简单、学习预测效率高、生成学习方法 基本方法 贝叶斯定理 特征条件独立假设 贝叶斯定理若$X$是定义在输入空间$R^n$上的随机向量，$Y$是定义在输出空间$\\lbrace c_1, c_2,…, c_K \\rbrace$上的随机变量，$P(X, Y)$是$X$和$Y$的联合概率分布，训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $由$P(X, Y)$独立同分布产生。 后验概率现对于实例$x$需要判断其所属分类，即我们需要知道x属于哪个分类的概率最大，用公式表示就是求取$$argmax_{c_k} P(Y=c_k | X=x)$$即后验概率，其中$k=1, 2,…, K$表示所有可能分类，而对于求解$P(Y=c_k | X=x)$，根据贝叶斯定理，我们需要知道训练数据的： 先验概率分布$$P(Y=c_k), k = 1,2,…,K$$ 条件概率分布，即每种分类下各个实例特征取不同值的概率 $$P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)},…,X^{(n)}=x^{(n)}|Y=c_k)$$然后根据贝叶斯定理进行计算求得后验概率 贝叶斯定理$$P(Y|X)=\\frac{P(X,Y)}{P(X)}=\\frac{P(X|Y)P(Y)}{\\sum P(Y)P(X|Y)}$$即$$P(Y=c_k|X=x)=\\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_k P(Y=c_k)P(X=x|Y=c_k)}$$但是如果直接这么求解，由于条件概率分布涉及到大量的参数，所以其估计在实际中是不可行的，所以我们在这里进行了一个大胆的假设 条件独立性假设假设用于分类的特征在类确定的条件下都是条件独立的，所以对于条件分布，我们有$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},…,X^{(n)}=x^{(n)} | Y=c_k)=\\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$这一假设使朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率 如此一来我们就可以得到最终的朴素贝叶斯分类器$$y=argmax_{c_k}P(Y=c_k) \\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$ 后验概率最大化后验概率最大化等价于期望风险最小化，证明如下 参数估计极大似然估计学习意味着估计先验概率和条件概率 先验概率的极大似然估计$$P(Y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)}{N}$$ 条件概率的极大似然估计$$P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl}, y_i=c_k)}{\\sum_{i=1}^{N}I(y_i=c_k)}$$其中假设第$j$个特征$x^{(j)}$可能取值的集合为$\\lbrace a_{j1}, a_{j2},…,a_{jS_j}$，$j=1, 2,…,n; l=1, 2,…,S_j; k=1,2,…,K; x_i^{(j)}$是第$i$个样本的第$j$个特征；$a_{jl}$为第$j$个特征可能取的第$l$个值，$I$为指示函数 算法 输入：训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $，其中$x_i=(x_i^{(1)}, x_i^{(2)},…,x_i^{(n)})^T, x_i^{(j)}$是第$i$个样本的第$j$个特征，$x_i^{(j)} \\in \\lbrace a_{j1}, a_{j2},…,a_{jS_j}$，$a_{jl}$为第$j$个特征可能取的第$l$个值，$j=1, 2,…,n; l=1, 2,…,S_j; y_i \\in \\lbrace c_1, c_2,…, c_K \\rbrace $; 实例$x$； 输出：实例$x$的分类 (1) 计算先验概率$$P(Y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)}{N}, k = 1,2,…,K$$计算条件概率$$P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl}, y_i=c_k)}{\\sum_{i=1}^{N}I(y_i=c_k)}$$ (2) 对于给定的实例$x=(x^{(1)}, x^{(2)},…,x^{(n)})^T$，计算$$P(Y=c_k)\\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$ (3) 确定实例$x$的类$$y=argmax_{c_k}P(Y=c_k)\\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$ 贝叶斯估计由于用极大似然估计可能会出现所要估计的概率值为0的情况(条件概率的某种取值的统计量为0)，会影响到后验概率的计算结果，使分类产生偏差，我们可以采取的做法是在随机变量各个取值的频数上赋予一个正数$\\lambda$，即贝叶斯估计，此时 条件概率的贝叶斯估计为$$P_{\\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl}, y_i=c_k)+ \\lambda}{\\sum_{i=1}^{N}I(y_i=c_k)+S_j \\lambda}$$ 先验概率的贝叶斯估计$$P_{\\lambda}(Y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)+\\lambda}{N+K\\lambda}$$常取$\\lambda=1$称为拉普拉斯平滑 Note:朴素贝叶斯法中假设输入变量都是条件独立的，如果假设他们之间存在概率依存关系，模型就变成了贝叶斯网络。 实现——机器学习实战按照机器学习实战第四章节的内容实现了基础的朴素贝叶斯分类器不同于k-nn和感知机将实力硬性的划分到某一类别，朴素贝叶斯分类器是返回某一实例属于某类别的概率，此外，朴素贝叶斯分类器是一种生成模型，在数据较为缺少的情况下依然有效。 本实验主要是针对文档的分类 根据文档中的词构建词集、词袋子 涉及文档的词向量的解析、提取 将文档中每一个词作为特征，计算先验概率、条件概率 实际应用中使用对数处理了下溢出等问题 实例 使用朴素贝叶斯分类器过滤侮辱性留言 使用朴素贝叶斯分类器过滤垃圾邮件 使用朴素贝叶斯分类器从个人广告中获取地域倾向具体实验代码见github","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"k-NN","slug":"k-NN","permalink":"http://yoursite.com/tags/k-NN/"}]},{"title":"剑指offer阅读笔记","date":"2017-06-09T02:33:40.000Z","path":"2017/06/09/offer/","text":"本文主要是个人针对《剑指offer》一书的一些笔记，并不涉及具体的题目或者方法，只是作为一个整体的知识点框架梳理，便于查缺补漏。 辅助网站 剑指offer第二版作者源代码（C++实现） 第一版面试题Java实现 牛客网上配套练习 第一章面试形式&amp;流程 电面 ——&gt; 远程 ——&gt; 现场 尽可能用形象化的语言把细节描述清楚 不确定问题时，主动提问 编程习惯&amp;调试能力 编程前理清思路 命名、缩进习惯 测试在前、开发在后(编程前尽可能考虑多个测试用例) 项目介绍 STAR模型 —— 简短项目背景，突出自己所做的工作和成绩 技术面5种素质 扎实的基础知识 高质量代码 分析问题时思路清晰 能优化时间、空间效率 学习、沟通能力 基础 编程语言——熟悉程度 数据结构：链表、树、栈、队列、哈希表 算法： 查找、排序：二分查找、归并排序、快排 动态规划、贪心 高质量代码 鲁棒性——检查空指针 特殊输入 边界条件 异常处理 Note: 考虑本身结构的取值范围 输入变量的可能取值 开发之前多想一些测试用例 分析&amp;思路——针对复杂问题 画图使抽象问题形象化 举例使抽象问题具体化 分解使复杂问题简单化 效率 首先要知道如何分析效率 数值各种数据结构的优缺点、并能选择合适的数据结构解决问题 熟练掌握常用的算法 软技能 沟通能力——思路、逻辑、合作 学习能力：读书、新概念 知识迁移能力 抽象建模能力 发散思维能力 关于提问（技术面） 忌 不要问与自己职位无关的问题 不要问薪水（HR面再详谈） 不要打听面试结果 荐 问与应聘职位或项目相关的问题 事先搜集应聘职位、项目背景相关信息 面试中留心面试官说过的话 第二章 —— 基础知识编程语言语言面试的3种类型 概念的理解：eg:关键字（特点及使用场合） 分析代码运行结果 写代码 数据结构 数组&amp;字符串 —— 基本 链表&amp;树 —— 常考，代码鲁棒性 栈 —— 递归 队列 —— 广度优先搜索 算法与数据操作 实现方式：循环或递归 排序&amp;查找：重点（重中之重：二分查找、归并排序、快速排序） 回溯法 —— 递归、栈、二维数组（矩阵）、迷宫问题 动态规划 分析：自上而下 ——&gt; 递归 实现：自下而上 ——&gt; 循环 涉及动态规划求解问题的四个特点 问题目标是求其最优解 整体问题的最优解依赖于各个子问题的最优解 将大问题分解为若干小问题之后，小问题之间还存在相互重叠的更小的公共子问题 自上而下分析问题，自下而上求解问题 贪婪 分解子问题时存在特殊选择 证明是最优解——需要较强的数学功底 位运算：与、或、异或、左移、右移 重要结论：将一个整数减去1后再与原来的整数做位与运算，得到的结果相当于把原整数的二进制表示中的最右边的1变成0 常考点：涉及统计二进制表示中1的个数 第三章——高质量的代码代码的规范性 书写——白板编程 布局——缩进 命名——表明用途、意图 代码的完整性 功能测试 边界测试 负面测试 ###代码的鲁棒性 容错性 防御性编程 在函数入口添加代码以验证用户输入是否符合要求 突破思维局限，多问几个“如果不…那么…” 处理无效的输入 小技巧：链表——双指针，当用一个指针遍历链表不能解决问题时，尝试用两个指针遍历链表，一种思路让两个指针速率不等；另一种思路让一个指针先遍历一段距离。 第四章——解决面试题的思路画图举例分解分治法/动态规划等 第五章——优化时间和空间效率时间效率 编程习惯、细节 循环vs.递归 数据结构&amp;算法功底 知识点：快速排序(partition)、红黑树、最大堆、最小堆 思维能力&amp;激情 时间效率与空间效率的平衡 以空间换时间 权衡：可以与面试官探讨 对于Java需要回顾集合相关的知识点 小结——如何降低时间复杂度 改用更高效的算法 空间换时间 用数组实现简单的哈希表 创建缓存保存中间的计算结果 递归–&gt; 保存求解子问题的结果避免重复计算 例外：针对嵌入式开发，空间换时间不一定可行","tags":[{"name":"校招","slug":"校招","permalink":"http://yoursite.com/tags/校招/"},{"name":"面试","slug":"面试","permalink":"http://yoursite.com/tags/面试/"},{"name":"Coding","slug":"Coding","permalink":"http://yoursite.com/tags/Coding/"},{"name":"剑指offer","slug":"剑指offer","permalink":"http://yoursite.com/tags/剑指offer/"}]},{"title":"统计学习方法笔记(三) —— K近邻","date":"2017-06-06T12:12:28.000Z","path":"2017/06/06/knn/","text":"k近邻算法概述 给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类 算法 输入：训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $，其中$x_i \\in R^n$为实例的特征向量,$y_i \\in \\lbrace c_1, c_2,…, c_K \\rbrace$ 为实例的类别,$i=1, 2, 3…, N$； 输出：实例$x$所属的类$y$ 根据所给的距离度量，在训练集中找到与$x$最近的k个点，涵盖这k个点的$x$的邻域记作$N_k(x)$； $N_k(x)$中根据分类决策规则（常用多数表决）决定$x$的类别$y$ $$y = argmax_{c_j} \\sum_{x_i \\in N_k(x)} I(y_i = c_j), i=1, 2,…, N; j=1, 2,…, K$$ 其中$I$为指示函数，当$y_i = c_j$时$I$为1，否则为0 Note: k近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型” k=1时称为最近邻算法 k近邻法没有显式的学习过程 k近邻模型——对特征空间的划分 k近邻法中，当训练集、距离度量、k值以及分类决策规则确定后，对于任何一个新的输入实例，它所属的类唯一地确定，这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。 k近邻法的三个基本要素k值的选择 k值 学习的近似误差 估计误差 特点 较小 下降 上升 整体模型变得复杂，对紧邻的实例点变得非常敏感，容易发生过拟合 较大 上升 下降 模型变得简单，但与输入实例较远的点也会对输入实例点产生影响 Note: 实际应用中，k值一般取一个比较小的数值，通常采用交叉验证法来选取最优的k值。 另外关于近似误差和估计误差，网上没有找到让我满意的答案，目前的一点理解如下： k值越小，学习的近似误差(approximation error)越小，估计误差(estimation error)越大，反之则相反 个人感觉这里有点像偏差和方差的区别，是否可以理解为近似误差即偏差，在k值较小时，选择的邻域范围较小，所以在空间内切割的“比较细”，但与此同时导致模型更加复杂，对于近邻的实例点非常敏感，而估计误差理解为方差，也就是说划分的邻域范围较大，所以平均下来根据分类决策规则可以减小邻域内的噪音点的影响，但是范围大的同时也会产生较远的点对于实例点也产生影响 搜索到的一些结果比如linian2763的博客 估计误差（estimation error）:度量预测结果与最优结果的相近程度近似误差（approximation error）:度量与最优误差之间的相近程度 此外还有stackexchange上的讨论，但是看得更加迷糊。 距离度量——实例点相似程度的反映常用距离度量 欧氏距离 $L_p$距离 Minkowski距离 其中对于$L_p$距离，设特征空间为n维实数向量空间$R^n$，$x_i, x_j \\in R^n$，$x_i = (x_i^{(1)}, x_i^{(2)},…, x_i^{(n)})^T, x_j = (x_j^{(1)}, x_j^{(2)},…, x_j^{(n)})^T$，则$x_i, x_j$的$L_p$距离定义为 $$L_p(x_i,x_j)=(\\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p)^{\\frac{1}{p}}$$ 其中$p \\geq 1$，且当$p=2$时转化为欧氏距离，$p=1$时转化为曼哈顿距离 Note: 不同的距离度量所确定的最近邻点是不同的 分类决策规则常用多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类，多数表决规则等价于经验风险最小化 实现方法——kd树(k维树) 特征空间维数大 训练数据容量大如何对训练数据进行快速k近邻搜索 线性扫描：耗时，不可行 使用特殊的结构存储训练数据，以减少计算距离的次数——kd树 构造kd树概述 kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构 Note: kd树是二叉树 kd树的每个结点对应于一个k维超矩形区域，而各实例点存在于不同的超矩形区域内，即在kd树的不同结点里 一般这个k维就是数据实例点的维度，即特征数，每次选定一个特征，然后根据范围内的实例点（记录）的该特征的值来进行二分，过程有点像快排里的分割 算法 输入：k维空间数据集$T= \\lbrace x_1, x_2,…, x_N \\rbrace $，其中$x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(k)})^T, i=1,2,…,N$； 输出：kd树 （1）开始：构造根结点，根结点对应于包含T的k维空间的超矩形区域。 选择$x^{(1)}$为坐标轴，以T中所有实例的$x^{(1)}$坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。由根结点生成深度为1的左、右结点，左子结点区域的$x^{(1)}$坐标对应的值小于切分点的子区域；右子结点区域的$x^{(1)}$坐标对应的值大于切分点的子区域。将落在切分超平面上的实例点保存在根结点。 （2）重复：对于深度为j的结点，选择$x^{(1)}$为切分的坐标轴，$l=(j \\mod k) + 1$，以该结点区域中所有实例的$x^{(1)}$坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。由该结点生成深度为j+1的左、右结点，左子结点区域的$x^{(1)}$坐标对应的值小于切分点的子区域；右子结点区域的$x^{(1)}$坐标对应的值大于切分点的子区域。将落在切分超平面上的实例点保存在该结点。 （3）直到两个区域没有实例存在时停止。 Note: 书中的方法是轮流按每个维度对某结点进行切分，各个维度（特征）可能会被重复利用来进行切分，当然此时的实例点范围不同，切分点也不同（中位数发生变化） 对于kd构造的实例，其一书中的例子比较直观，如下此外，在wiki里的一幅三维空间的配图也是非常直观 一个三维k-d树。第一次划分（红色）把根节点（白色）划分成两个节点，然后它们分别再次被划分（绿色）为两个子节点。最后这四个子节点的每一个都被划分（蓝色）为两个子节点。因为没有更进一步的划分，最后得到的八个节点称为叶子节点。 搜索kd树 利用kd树可以省去对大部分的数据点的搜索，从而减少搜索的计算量 为什么？ kd树的存储结构将搜索控制在空间的局部区域内，减少了搜索、距离计算的次数 直观上来说，首先根据构造好的kd树我们类似二分查找(二叉搜索树)的方法找到当前的最近邻点，这样一来目标点的最近邻一定是在以目标点为中心，目标点到当前最近点的超球体的内部(要么就是当前最近点，要么是超球体与另一超矩形区域重合部分存在的实力点) 如图，S为目标点，D为当前最近点，E是在另一分支里与超球体相交的范围内的点，为真实的最近点，后续经过回溯判断可以确定 扩展-球树通过观察上述图形可以发现，如果S与D的距离再远一点，即超球体再大一点，那么可能与左上方的矩形产生交集，这时就会造成需要搜索这两个矩形的操作，究其原因是因为kd树是将空间分割为超矩形区域造成的，而球树则通过将实例点划分到一个个超球体里解决了这一问题 建树 先构建一个超球体，这个超球体是可以包含所有样本的最小球体。 从球中选择一个离球的中心最远的点，然后选择第二个点离第一个点最远，将球中所有的点分配到离这两个聚类中心最近的一个上，然后计算每个聚类的中心，以及聚类能够包含它所有数据点所需的最小半径。这样我们得到了两个子超球体，和KD树里面的左右子树对应。 对于这两个子超球体，递归执行步骤2). 最终得到了一个球树。 Note:可以看出KD树和球树类似，主要区别在于球树得到的是节点样本组成的最小超球体，而KD得到的是节点样本组成的超矩形体，这个超球体要与对应的KD树的超矩形体小，这样在做最近邻搜索的时候，可以避免一些无谓的搜索。 算法 输入：已构造的kd树；目标点x； 输出：x的最近邻 （1）在kd树中找到包含目标点x的叶结点：思想有点像二分查找，从根节点出发，递归地向下访问kd树，若目标点x当前维的坐标小于切分点的坐标，则移动到左子节点，否则移动到右子节点，直到子节点为叶节点为止 （2）以此叶节点为“当前最近点” （3）递归地向上回退，在每个节点执行以下操作： （a）如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点” （b）当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。即以目标点为球心，以目标点与当前最近点间的距离为半径的球体是否与另一子结点对应的区域相交。如果相交，可能在另一个子结点对应的区域中存在更近的点。移动到另一个子结点，接着递归地进行最邻近搜索。如果不相交，则向上回退。 （4）当回退到根结点，搜索结束，此时的当前最近结点即为x的最邻近点。 Note: 若实例点随机分布，kd树搜索的平均计算复杂度是O(log N)，N为训练实例数 kd树更适用于训练实例数远大于空间维数时的k近邻搜索 关于这个算法的理解最好还是看书上的例子3.3，基本思路是一个重复二分查找和回溯的过程，另外关于kd树的搜索，有一篇很不错的文章，详见kd树的细致图文讲解，不同于书上止步于最近邻的搜索，该文章图文并茂的讲解了使用kd树来寻找k近邻的算法，并用一个例子一步一步的解析，大概思想是用一个大顶堆来记录k个近邻，先找到最近邻，然后向上回溯地寻找次近邻，寻找过程中需要判断堆是否已满，当前实例点与输入实例点的距离与堆中最远距离的大小(是否需要更新堆)，以及输入实例点与当前实例点所在的分割超平面的距离与堆中最远距离的大小关系(判断是否需要进一步到另一分支寻找) Note:可对比最近邻的搜索，其实就是原先是比较输入实例点到当前最近实例点的距离与输入实例点到当前最近实例点所在超平面的距离大小变成堆中最远距离与输入实例点到当前最近实例点所在超平面的距离大小 k-nn优缺点 KNN的主要优点有： 1） 理论成熟，思想简单，既可以用来做分类也可以用来做回归 2） 可用于非线性分类 3） 训练时间复杂度比支持向量机之类的算法低，仅为O(n) 4） 和朴素贝叶斯之类的算法比，对数据没有假设，准确度高，对异常点不敏感 5） 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合 6）该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分 KNN的主要缺点有： 1）计算量大，尤其是特征数非常多的时候 2）样本不平衡的时候，对稀有类别的预测准确率低 3）KD树，球树之类的模型建立需要大量的内存 4）使用懒散学习方法，基本上不学习，导致预测时速度比起逻辑回归之类的算法慢 5）相比决策树模型，KNN模型可解释性不强 实现——机器学习实战按照机器学习实战关于k-NN的章节实现了原始的k-NN分类器，并做了几个小实例： 首先是一个基础的原理理解，对于二维空间的点按照坐标之间的距离进行分类，意图是理解k-NN的基本原理，即基于实例的学习算法，按照距离分类 在实现了原始分类器的基础上构建小应用，使用约会网站的异性数据，针对三项特征来判断新输入的异性是否有魅力 同样基于原始分类器，针对二进制点阵构成的数字图像文本文件进行识别和分类 上述三个例子都是使用的同一个原始分类器，不同之处仅仅是输入的数据有所改变，此外为了使输入数据能够被原始分类器处理，针对不同数据的特征进行了不同的处理： 针对约会网站数据由于各项特征的取值范围不同可能对于距离计算产生影响所以进行了归一化操作 对于图像文本文件将其由32*32的二进制点阵转换为1*1024的向量便于分类器处理 此外还有一些基础的文件格式化为矩阵以及可视化操作 原始的Jupyter Notebook可以参考我的github 参考资料 《统计学习方法》 《机器学习实战》 K近邻法(KNN)原理小结 kd树算法之详细篇","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"k-NN","slug":"k-NN","permalink":"http://yoursite.com/tags/k-NN/"}]},{"title":"统计学习方法笔记(二) —— 感知机","date":"2017-06-03T08:53:02.000Z","path":"2017/06/03/Perceptron/","text":"概述 感知机（perceptron）是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值。感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于判别模型。感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此导入了基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。感知机预测是用学习得到的感知基模型对新的输入实例进行分类。感知机是神经网络与支持向量机的基础。关键字：二分类，线性分类模型，分离超平面，判别模型，基于误分类的损失函数，随机梯度下降法 感知机模型——分离超平面数学表达$$f(x) = sign(w \\cdot x + b)$$ 输入空间：$R^n$ 输出空间：$\\lbrace+1, -1 \\rbrace$ 假设空间：$\\lbrace f|f(x) = w \\cdot x + b\\rbrace$，表示定义在特征空间中的所有线性分类模型或线性分类器 $w \\in R^n$，其中$w$表示权值向量，几何意义为超平面的法向量 $b \\in R$，$b$表示偏置，几何意义为超平面的截距 Note:模型学习的目的在于通过训练集求得模型的参数$w$和$b$ 几何解释 感知机的几何解释：线性方程$w \\cdot x + b = 0$ 对应特征空间$R^n$中的一个超平面$S$,其中$w$是超平面的法向量，$b$是超平面的截距，这个超平面将特征空间划分为两个部分，位于两部分的点（特征向量）分别被分为正、负两类，因此，超平面$S$称为分离超平面。 感知机的学习策略数据集的线性可分性给定一个数据集$T={(x_1, y_1), (x_2, y_2),…,(x_N, y_N)}$,其中$x_i \\in R^n$,$y_i \\in \\lbrace+1, -1 \\rbrace$,$i=1, 2, 3…, N$，若存在超平面$S$设为$w \\cdot x + b = 0$将数据集的正实例点和负实例点完全正确地划分到$S$两侧，则称数据集$T$为线性可分数据集 学习策略 目标：求得一个能将训练集正实例点和负实例点完全正确分开的分类超平面——&gt;确定$w, b$ 转化：经验损失函数最小化——基于误分类 直观思路：误分类点总数，非$w, b$的连续可导函数，不易优化 修改：误分类点到超平面$S$的总距离$$L(w, b)=-\\sum_{x_i \\in M} y_i(w \\cdot x_i + b)$$,其中$M$为误分类点的集合，损失函数$L(w, b)$是$w, b$的连续可导函数。(简单的解释一下，此处计算误差取的是所有误分类点的集合，所以式中$y_i(w\\cdot x_i+b)$符号为负，这是前面的负号的由来，另外表示点到平面的距离的主要是$w\\cdot x_i+b$这部分，其中$x_i$为平面外一点，$w,b$分别为平面的法向量和截距，这里的计算省略了绝对值符号（原因同负号的由来），另外省略了分子法向量的平方和，应该是求解最小化误差时，该项不影响所以简化了计算，具体的推导可以参见下面的链接) Note: 感知机的学习策略是在假设空间中选取使损失函数最小的模型参数$w, b$，即感知机模型。 点到平面的距离推导 范数的通俗解释 另外关于范数、规范化的理解，这篇博文写的深入浅出 感知机学习算法原始形式概述随机梯度下降法 输入：训练集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $，其中$x_i \\in R^n$,$y_i \\in \\lbrace+1, -1 \\rbrace$,$i=1, 2, 3…, N$，学习率$\\eta (0&lt; \\eta \\leq 1)$ 输出：$w, b$;感知机模型$f(x) = sign(w \\cdot x + b)$ (1) 选取初值$w_0, b_0$ (2) 在训练集中选取数据$(x_i, y_i)$ (3) if $y_i(w \\cdot x_i + b) \\leq 0$即为误分类点 $w \\leftarrow w + \\eta y_i x_i$ $b \\leftarrow b + \\eta y_i $ (4) 转至(2)，直至训练集中没有误分类点 几何解释当一个实例点被误分类，即位于分离超平面的错误的一侧时，则调整$w, b$的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。 随机梯度下降法 首先任意选取一个超平面$w_0, b_0$，然后用随机梯度下降法不断地极小化目标函数 $$L(w, b)= - \\sum_{x_i \\in M} y_i(w \\cdot x_i + b)$$ 极小化过程中不是一次使$M$中的所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降 若误分类点集合M固定，损失函数的梯度计算 $$\\frac{\\partial L(w,b)}{\\partial w}=- \\sum_{x_i \\in M}y_i x_i$$ $$\\frac{\\partial L(w,b)}{\\partial b}=- \\sum_{x_i \\in M}y_i$$ 随机选取一个误分类点$(x_i, y_i)$对$w, b$进行更新： $$w \\leftarrow w + \\eta y_i x_i$$ $$b \\leftarrow b + \\eta y_i $$ 通过迭代可使损失函数$L(w, b)$不断减小，直到为0。 算法的收敛性证明如下 另外可以看看其他人写的证明过程 Note: 当训练数据集线性可分时，感知机学习算法存在无穷多个解，其解由于不同的初值或不同的迭代顺序而可能有所不同。相对的，线性可分支持向量机则克服了这一点，根据间隔最大化从无穷多个超平面中唯一确定了一个 对偶形式基本思想将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得$w, b$。 假设初始值$w_0, b_0$均为0，对误分类点$(x_i, y_i)$，通过 $$w \\leftarrow w + \\eta y_i x_i$$ $$b \\leftarrow b + \\eta y_i $$ 逐步修改$w, b$，假设修改$n$次，则$w, b$可分别表示为： $$w= \\sum_{i=1}^{N} \\alpha_i y_i x_i$$ $$b= \\sum_{i=1}^{N} \\alpha_i y_i$$ 每个实例点对应有一个$\\alpha_i$满足$\\alpha_i \\geq 0$且当$\\eta = 1$时，$\\alpha_i$就表示第$i$个实例点由于误分类而进行更新的次数。 实例点更新次数越多，意味着它距离分离超平面越接近，也就越难以正确分类，换言之，这种实例对学习结果影响最大（有点支持向量机中的支持向量的意思）。 算法 输入：线性可分的数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $，其中$x_i \\in R^n$,$y_i \\in \\lbrace +1, -1 \\rbrace$,$i=1, 2, 3…, N$，学习率$\\eta (0&lt; \\eta \\leq 1)$ 输出：$\\alpha, b$；感知机模型$f(x) = sign(\\sum_{j=1}^{N} \\alpha_j y_j x_j \\cdot x + b)$,其中$\\alpha=(alpla_1, alpla_2,…,alpla_N)^T$(1): $\\alpha \\leftarrow 0$, $b \\leftarrow 0$(2): 计算所有样本内积形成的Gram矩阵$G$， $$G=[x_i \\cdot x_j ]_{N \\times N}$$ (3): 在训练集中选取数据$(x_i, y_i)$，若 $$y_i (\\sum_{j=1}^{N} \\alpha_j y_j x_j \\cdot x_i + b) \\leq 0$$ （计算过程中可通过查$G$的值来提高效率）则更新： $$\\alpha_i \\leftarrow \\alpha_i + \\eta$$ $$b \\leftarrow b + \\eta y_i$$(4): 转至(3)直至没有误分类数据 Note:为什么要引入对偶形式？ 首先原始形式中，书中为何要使用随机梯度下降而非批量梯度下降法，个人搜索到的一篇博文里感觉说的有点道理，引用如下 用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，只有误分类的M集合里面的样本才能参与损失函数的优化。所以我们不能用最普通的批量梯度下降,只能采用随机梯度下降（SGD）或者小批量梯度下降（MBGD）。 此外，关于对偶形式的优势，小结一下： 对偶形式将权重向量$w$转化为实例$x_i$和标记$y_i$的线性组合形式，且在原书中也提到，对偶形式中的训练实例仅以内积的形式出现，所以可以预先使用Gram矩阵存储，也就是空间换时间的方法提高计算效率 书中这里应该也有点为后续介绍支持向量机做铺垫，所以这里为核函数的引入埋一个伏笔，毕竟感知机是神经网络与支持向量机的基础，而且后面书中在支持向量机部分的讲解也多次使用对偶形式的求解 参考资料 李航《统计学习方法》 点到平面的距离推导 范数的通俗解释 关于范数、规范化的理解 http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf http://www.cnblogs.com/pinard/p/6042320.html https://www.zhihu.com/question/26526858","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"Perceptron","slug":"Perceptron","permalink":"http://yoursite.com/tags/Perceptron/"}]},{"title":"统计学习方法笔记(一)","date":"2017-06-02T06:42:52.000Z","path":"2017/06/02/Note-StatisticalML/","text":"本系列文章是个人根据阅读李航博士的《统计学习方法》一书，辅以《机器学习实战》、scikit-learn官方文档等材料整理出来的笔记。 统计学习相关概念定义 统计学习(statistical learning)是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测和分析的一门学科 研究对象数据 研究目的对数据进行预测和分析：学习什么样的模型和如何学习模型（准确、效率） 研究方法构建模型并应用模型进行预测和分析（统计学习方法三要素） 模型：模型的假设空间，即学习模型的集合(学习什么样的模型) 策略：模型选择的准则(用什么标准选择模型) 算法：模型学习的算法，求解最优模型的算法(如何学习模型) 分类 监督学习 非监督学习 半监督学习 强化学习 监督学习概览 输入空间：输入所有可能取值的集合 输出空间：输出所有可能取值的集合 特征空间： 每个具体的输入是一个实例，通常由特征向量表示，所有特征向量存在的空间称为特征空间 有时输入空间与特征空间为相同的空间，不予区分，有时为不同的空间，需要将实例从输入空间映射到特征空间，模型实际上都是定义在特征空间上 假设空间：学习模型的集合（假设要学习的模型属于某个函数集合），模型属于由输入空间到输出空间的映射的集合，这个集合即假设空间，假设空间的确定意味着学习范围的确定。 监督学习的模型 概率模型：条件概率分布$P(Y|X)$ 非概率模型：决策函数$Y=f(X)$ 联合概率分布监督学习假设输入与输出的随机变量X和Y遵循联合概率分布$P(X, Y)$(监督学习关于数据的基本假设) 监督学习分类 回归问题：输入变量与输出变量均为连续变量的预测问题。 分类问题：输出变量为有限个离散变量的预测问题。 标注问题：输入变量和输出变量均为变量序列的预测问题。 问题形式化学习与预测——训练与测试 学习：利用给定的训练数据集，通过学习（训练）得到一个模型； 预测：对于给定的测试样本中的输入，由所得到的模型给出相应的输出。 统计学习方法三要素(针对监督学习)模型学习什么样的模型，在监督学习过程中，模型指的就是所要学习的条件概率分布或决策函数。 策略（评价标准）模型选择的准则：按照什么样的准则学习或选择最优的模型 损失函数（代价函数）：一次预测的好坏的度量，常用的包括0-1损失函数、平方损失函数、绝对损失函数、对数(似然)损失函数 风险函数（期望损失）：度量平均意义下模型预测的好坏 经验风险，用来描述模型与训练数据的契合程度， 结构风险，用来描述模型的某些性质，表示了我们希望获得具有何种性质的模型（比如复杂度较小的模型） 为引入领域知识和用户意图提供了途径 有助于削减假设空间，从而降低了最小化训练误差的过拟和风险，所以可以被视为正则化项，从贝叶斯的角度来看，可认为是提供了模型的先验概率，常用的正则化项为$L_p$范数 将监督学习的问题转化为经验风险或结构风险函数的最优化问题（结构风险最小的模型即最优模型） 《统计学习方法》与《机器学习》上对于结构风险似乎有点区别 Note: 在模型选择中，理论上应该使用期望风险函数作为标准，但是由于输入，输出的基本假设$P(X, Y)$联合分布未知（若已知则可直接求解P(Y|X)，不用学习），所以试图使用经验风险（平均损失）来替代，但又由于现实中限于样本容量不会很大，所以要对经验风险进行矫正，于是有了结构风险，在经验风险的基础上添加正则化项来防止过拟合。 最大似然（MLE），最大后验（MAP）都是构造目标函数的方法 在软间隔SVM中，表示最大间隔的部分对应结构风险，而罚项、松弛变量的部分则对应经验风险，参见周志华《机器学习》6.4 这里关于结构风险到底是式中的正则化项(《机器学习》)还是经验风险+正则化项(《统计学习方法》)先不做讨论(找了好久也找不到，提问也没人回应)，简单的说一下一般如何来操作结构风险最小化：参考 结构风险最小化是一种对假设空间的复杂度(似然函数的VC维)与训练误差(经验风险)之间的折中，一般实施步骤如下： 根据该领域的先验知识选择某一族模型，比如根据函数的次数、神经网络的隐层数目等等； 将该族模型根据其本身的复杂度递增的顺序划分成一层一层嵌套的集合 对每一个子集根据经验风险最小化(训练误差最小)选择该子集的最优模型 得到一系列模型后，综合比较并选择出经验风险和VC置信度综合最小的模型 算法（求解&amp;优化）模型学习的算法，即学习模型的具体计算方法 用什么样的计算方法求解最优模型（寻找全局最优解） 如何高效实现 Note: 一句话小结：针对某一问题，我们提出了一种假设（模型），然后制订了解决该问题的目标（策略/损失函数）；接下来就是采取某种方式来达成这一目标（算法/优化） 常用优化方法大致可以分为两类，一类使用函数的梯度信息，包括一阶的方法，例如梯度下降，以及二阶的方法，例如牛顿法等。当然，还有和梯度无关的方法，例如 fixed point iteration，坐标下降等等 模型选择模型评估训练误差&amp;测试误差 训练误差：对判断给定的问题是不是一个容易学习的问题是有意义的，但若一味追求减小训练误差，会出现过拟合的情况（所选模型的复杂度比真模型高） 测试误差：反映了学习方法对未知的测试数据集的预测能力，测试误差小的方法具有更好的预测能力所以需要选择复杂度适当的模型，而模型选择常用的两种方法分别是正则化和交叉验证。 正则化结构风险最小化策略的实现，在经验风险的基础上加一个正则化项/罚项 $$ R_{srm}(f)=\\frac{1}{N}\\sum L(y_i,f(x_i))+\\lambda J(f) $$ 作用是为了选择经验风险和模型复杂度同时较小的模型(在所有可能选择的模型中，能够很好地解释已知数据并且十分简单的，才是最好的模型) 交叉验证重复的使用数据 数据集划分 训练集：训练模型，通过输入数据学习函数中的参数值，得到拟合了数据的“分类器/回归器”等 验证集：模型选择，从不同的模型中选择最佳模型 从不同类的模型中选择（比如从SVM,GBM,决策树里选择模型） 从同类模型不同超参数(hyperparameter)组合里选择最优超参数组合。 测试集：最终对学习方法的评估常用方法 简单交叉验证 S折交叉验证 留一交叉验证 Note:关于数据集划分的一点理解理解可以参看交叉验证，而关于模型选择可以进一步了解参数与超参数 学习的泛化能力定义学习方法的泛化能力(generalization ability)是指由该方法学习到的模型对未知数据的预测能力。 评价方法 理论上：泛化误差即期望风险$$ R_{exp}(f)=E_p[L(Y, f(X))]=\\int L(y,f(x))P(x,y)dxdy $$ 实际应用中：常常用测试误差衡量 泛化误差上界通过比较两种学习方法的泛化误差上界来比较其优劣（越小越好） 样本容量越大，泛化误差上界越小 假设空间越大，泛化误差上界越大 训练误差小的模型，其泛化误差也会小 生成模型与判别模型监督学习模型的一般形式 概率模型：条件概率分布$P(Y|X)$ 非概率模型：决策函数$Y=f(X)$ 监督学习方法 - 生成方法 判别方法 定义 由数据学习联合概率分布$P(X, Y)$,然后求出$P(Y&#124;X)$作为预测的模型,$P(Y&#124;X)=\\frac{P(X,Y)}{P(X)}$ 由数据直接学习决策函数$f(x)$或条件概率分布$P(Y&#124;X)$作为预测模型 特点 1.可还原出$P(X, Y)$；2.学习收敛速度更快；3.存在隐变量时仍可用 1.直接面对预测，准确率更高；2.便于数据抽象，特征定义和使用，可简化学习问题 典型模型 朴素贝叶斯法、隐马尔可夫模型 k-近邻、感知机、决策树、逻辑斯谛回归模型、最大熵模型、SVM、提升方法、条件随机场等 Note 模型表示了给定输入X产生输出Y的生成关系 判别方法关心的是对给定的输入X，应该预测什么样的输出Y 监督学习方法的应用分类问题在监督学习中，当输入变量Y取有限个离散值时，预测问题便成为分类问题 多类分类问题，包括二分类 评价指标 分类准确率 精确率&amp;召回率（二分类）——正类、负类、F1值 标注问题可以认为是分类问题的一种推广，输入观测序列，输出标记序列(状态序列) 学习&amp;标注——条件概率分布 评价指标 标注准确率 精确率、召回率 常用统计学习方法 隐马尔可夫模型 条件随机场 应用领域 信息抽取 自然语言处理 回归问题回归用于预测输入变量（自变量）和输出变量（因变量）之间的关系——函数拟合 学习&amp;预测 一元回归 vs 多元回归 线性回归 vs 非线性回归 损失函数：常用平方损失函数—— 最小二乘法 参考资料 李航《统计学习方法》 周志华《机器学习》 知乎-最小二乘、极大似然、梯度下降有何区别？","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"}]},{"title":"关于交叉验证和模型选择的一点思考","date":"2017-06-01T12:28:36.000Z","path":"2017/06/01/cv/","text":"首先按照《统计学习方法》第一章的内容，常用的模型选择方法有两种，按照结构风险最小化思想的具体实现即正则化以及数据驱动的交叉验证方法。 CV是用于模型的(评估)挑选和超参数(关于超参数和参数的区别请见另一篇memo)调整的，首先明确一点，所谓模型，指的是我们用来描述输入数据与最终需要预测的输出数据之间的关联的方法，而不是不同数据拟合的模型得到的实例，比如我们可以说一个线性回归模型，但是不会说用不同数据拟合的线性回归器为不同的模型； 为什么要用CV，这涉及到数据划分的问题，首先我们训练好一个模型后需要评估这个模型的效果，但是如果把全部数据都投入训练就没有数据来进行验证了，常规而言，可以将数据集划分为训练集和验证集比如80%训练、20%验证，但是这样会有一些问题，首先有可能你很不巧的将一些特殊数据划分到这20%的验证集里了(要么效果很好、要么效果很糟糕)，这样的话仅仅这么一组验证集的评估效果就很不稳定也不确切；此外我们划分训练集和验证集就减少了训练数据，而一般而言数据越多训练出的模型方差是越小的，从这个角度来看数据越多一般我们训练的效果更好； 那么有没有方法既可以用上所有数据进行验证和评估、最后又可以用所有数据进行训练呢？所以我们就需要交叉验证，比如五折交叉验证，假设我们仍将数据集8-2划分（80%-20%），对于五折交叉验证，我们就是针对模型进行了五次训练，每次取80%训练数据、20%验证数据，并最终保证每个数据都曾在这五次训练验证中作为20%的验证集对模型进行过评估，这样我们就可以确保我们使用了所有数据对我们的模型进行评估 为什么不用CV中得到的预测器进行预测？一般而言，在做交叉验证时确实可能出现某一组交叉验证的得分较高，我们会试图用这一组数据进行模型的拟合和最终预测，但是这种得分高只是一种表面现象，首先数据量少了，其次这一组验证集并非独立的，是在整个交叉验证中随机生成的，所以这个预测器的结果到底好不好还需要在对额外的数据进行测试，如果数据充足也许你可以用嵌套式的交叉验证进行实验，即第一步用常规内层交叉验证确定最佳模型，然后采用数据驱动的方式(外层交叉验证)拟合最佳的一组预测器 所以CV的作用是用来对不同模型（SVM和生成树等）或者不同(超)参数组合的模型中评估各个模型的效果，而得到最佳(超)参数组合的模型；接下来将所有训练集投入进去拟合预测得到拟合好的预测器最后对测试集进行预测 那么把所有训练集放到模型中会不会导致过拟合呢？答案是不会，过拟合产生的原因是模型过于复杂（模型的参数），而不是数据增加导致，（而不是传入参数的值），增加数据一般而言更有利于训练集的训练，因为一般而言，训练数据越充足，越能反映出真实数据的分布情况，得到一种近似无偏估计的结果 补充：关于生成树模型中的early_stopping，按照《统计学习方法》一书的第12章总结部分P213所述 提升方法没有显示的正则化项，通常通过早停止(early stopping)的方法达到正则化的效果 所以early_stopping属于正则化的范畴，是另一种模型选择的具体方法。 参考资料https://stats.stackexchange.com/a/52277/152084http://scikit-learn.org/stable/modules/cross_validation.htmlhttps://stats.stackexchange.com/a/52312/152084","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"CV","slug":"CV","permalink":"http://yoursite.com/tags/CV/"},{"name":"model selection","slug":"model-selection","permalink":"http://yoursite.com/tags/model-selection/"}]},{"title":"机器学习中模型的参数和超参数","date":"2017-06-01T12:24:11.000Z","path":"2017/06/01/parameter-hyperparameter/","text":"一直以来对于机器学习中的模型训练和模型选择存在一个误区，首先机器学习力的模型通俗来说就是一个函数关系，表明输入数据到输出数据的映射，基本的假设前提是输入数据和输出数据符合某种联合概率分布，而模型训练的过程其实就是在确定函数式的具体参数值的过程，比如假设你要做一个多项式回归分析的模型，比如$f(x)=w_1x_1+w_2x_2+w_3x_3$，那么模型训练的过程中其实就是在学习对应的w的值，那么问题来了，实战中所谓的模型调参来选择模型又指的是什么呢？既然训练已经把参数都确定下来了，那我们调整的参数又是什么？原来这里有个误区在于模型中的parameter和hyperparameter的区别，按照搜集到的资料来看，其实模型中可以分为两种参数，一种是在训练过程中学习到的参数，即parameter也就是上面公式里的w，而另一种参数则是hyperparameter，这种参数是模型中学习不到的，是我们预先定义的，而模型的调参其实指的是调整hyperparameter，而且不同类型的模型的hyperparameter也不尽相同，比如SVM中的C,树模型中的深度、叶子数以及比较常规的学习率等等，这种参数是在模型训练之前预先定义的，所以关于模型的选择其实更多的指的是选择最佳的hyperparameter组合。 参考资料https://datascience.stackexchange.com/a/14234/31117https://www.quora.com/What-are-hyperparameters-in-machine-learning","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"memo","slug":"memo","permalink":"http://yoursite.com/tags/memo/"},{"name":"parameter","slug":"parameter","permalink":"http://yoursite.com/tags/parameter/"},{"name":"hyperparameter","slug":"hyperparameter","permalink":"http://yoursite.com/tags/hyperparameter/"}]},{"title":"计算机与生活","date":"2017-05-30T11:57:01.000Z","path":"2017/05/30/computer-life/","text":"阅读完吴军的《浪潮之巅》，给人的感觉颇有点当初看纪录片《互联网时代》的感觉，简单而言就是你透过作者的眼睛快速浏览了一遍计算机这个行业的发展历程；手头的这本是第二版，看出版时间也是2012年了，到现在算算有点老了，不过对于12年以前发生的一些事情还是能有一个大致的梳理； 这本书上册主要讲述了一些在历史中唱过主角的公司，关于这些公司的兴衰以及作者自己的一些分析判断，下册则更宽泛一点，除了谈谈部分公司，还科普了一些计算机工业界以及商业方面的概念，比如谈信息产业的一些规律，一些公司的商业模式甚至风险投资，金融风暴等等； 下面我想简单谈一谈自己读过这本书的一点点体会，关于计算机和人类的生活。 我粗略的将计算机的发展历程划分为四个大的阶段，分别是计算时代—自动化时代—互联网时代—数据时代；计算时代比较有代表性的是计算机ENIAC，那个时候还主要用作军事用途，用来计算导弹轨迹之类的，离走进我们的生活还比较远，第二阶段自动化时代比较有代表性的就是微机了（当然这里跳过了面向企业的工作站，主要谈与个人生活相关的），按照作者的说法，这一阶段最有代表性的是三个公司，苹果的个人电脑（一体化）和微软与Intel的联盟（WinTel体系），这一阶段计算机真正开始走入普通人的家庭生活，主要是用于协助处理一些日常的办公工作，属于单机阶段；然后是大家比较熟悉的互联网时代，这一阶段的强大之处在于将计算机连接起来，于是每个用户都不再是孤立的，随着因特网的出现，有一部分人将本地的内容放到了网上，这个时候网上的内容不多，但是比较杂乱无序，所以随之诞生了门户网站比如雅虎等等，他们将互联网上的内容分门别类从而使用户便于查看和获取信息；而随着建立网站的门槛降低，互联网上的内容越来越多，单纯靠人工分类的的门户网站已经满足不了人们的需求了，于是搜索引擎（代表是Google）应运而生，帮助用户在浩瀚的互联网中快速找到自己所需的信息； 如果说互联网时代的关键词是‘内容’；那么数据时代的关键词就是平台了。这个时候，人们已经不满足于只是从互联网上获取信息了，就像不满足于只是从电视接受固定的节目一样，用户开始有‘发出自己的声音’的诉求，也就是创作；于是各个平台相继出现，最有代表性的，早期的Blog可以允许用户自己在网上发布自己的文章，然后是Facebook、Twitter不仅为用户构建了虚拟的社交圈，而且让用户可以在这个圈子里随时随地发出自己的声音并与其他人互动，还有YouTube把平台交给用户，让用户自己去发布自己的视频和收看其他用户的视频；这一阶段的特点是‘平台’ ，比较有代表性的公司并不提供内容，而是让用户自己来做内容的创造者。 而随之而来的是数据的爆发，互联网上的信息变得更加庞杂，在不断炒来炒去的概念‘云计算’，‘大数据’的驱动下，计算机的同学们纷纷投身到数据分析/数据挖掘的工作中，希望能够从庞杂的数据中挖掘出与实际业务相关的有价值的信息；那么，下一步人们的需求在哪呢？是通过数据挖掘对一个人建立画像从而实现对每个人的私人定制服务？还是现在炙手可热的基于VR/AR的虚拟社交？按照前段时间看到的一个关于区块链的演讲，在未来每个人都可以被“数字化”，真实世界的你可以投影到虚拟世界成为一个数字化的、独一无二的你，这大概是一种趋势吧。 最后还是推荐下这本书，作为科普类的读物挺不错的。","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"IT","slug":"IT","permalink":"http://yoursite.com/tags/IT/"}]},{"title":"来来来，我教你搭个博客好不好哇","date":"2017-05-30T10:40:58.000Z","path":"2017/05/30/my-blog/","text":"先把我的博客贴一下Lancelot’s Desert 端午节两天时间宅实验室把自己搭建一个博客这么个‘历史遗留问题’解决了下，其实之前也用Hugo搭建过，最后给弄崩了，这次尝试下Hexo，发现倒是异常的顺利，一方面Hexo的整体生态比较完备，另一方面网上找到的教程也很靠谱，下面就把我搭建过程中的一些步骤和踩过的坑记录下。 基础博客搭建首先声明下，基础博客搭建基本上和我所参照的崔斯特的教程是一致的，只是在顺序和语言上重新组织了一下，因为原博主写的教程已经很简明扼要了。 准备工作 git下载 github账号 node.js 另外的话写博客最好熟悉一点markdown的基本语法，常用的指令不多而且很简单，用的熟练是非常不错的工具，这里也推荐一个markdown的不错的在线编辑器Cmd markdown即时预览的，此外，类似简书、SegmentFault这些网站也是支持MarkDown编辑的，所以如果熟悉这个语法的话还是很方便的。 下载和注册基础搭建的话首先需要下载git和node.js(npm)来进行控制台命令的一些输入，以及需要一个github账号，因为我们的博客是挂载在github上的。 在下载好git和node.js并注册好一个github账号后，首先新建一个repository，名称和你的账户名一致，后面添加.github.io域名，比如我的用户名是LancelotHolmes,那么我的repository命名就是LancelotHolmes.github.io 环境配置在完成上述操作并安装好git和node.js之后，我们选择一个路径新建一个文件夹比如我是在D盘的MyBlog，然后执行git bash，可以在开始里搜索，也可以右键然后选择git bash here,在出现的git控制台中输入之前注册的github账号相对应信息，比如1git config --global user.name \"你的账户名\" 1git config --global user.email \"注册github账号时的邮箱\" 如图 然后是安装Hexo,直接输入 1npm install -g hexo-cli 开始搭建同样在MyBlog(或者你命名的目录下)，仍然是刚刚的控制台界面，输入1hexo init blog 成功的时候会显示 1INFO Start blogging with Hexo! 接下来进入blog目录下，输入123hexo cleanhexo ghexo s 或者你也可以新建一个generate.sh脚本文件将上面三条语句写入,因为后面线下测试会多次用到，可以直接在控制台输入./generate.sh，然后在浏览器里输入http://localhost:4000/,这个时候你就可以看到一个网页的基本雏形，这是由于你的\\blog\\source\\_posts路径下已经有一个基本的markdown文件了，而且本身下载的Hexo带了一个landscape的主题文件，在路径\\blog\\themes下可以看到 配置githubSSH回到控制台，现在我们需要生成SSH，仍然是在git控制台里，输入 1ssh-keygen -t rsa -C \"Github的注册邮箱地址\" 基本是一直回车，到出现信息1Your public key has been saved in /c/Users/user/.ssh/id_rsa.pub. 在信息对应的路径下找到这个文件，打开它(我是用sublime text打开的)，在你的github界面，右上角头像里选择Settings,左侧选择SSH and GPG keys然后新建一个SSH key,名称随你定，比如我是设置的blog,内容的话把刚刚id_rsa.pub里的内容全选复制进去就好了。 站点配置在blog目录下，用sublime或者其他编辑器打开_config.yml文件，找到对应的字段修改下面的基本参数信息，这里需要注意一下存在多个_config.yml文件，一个是在blog目录下，作为站点配置文件，一般用来做一些常规的配置，另外在各个主题的目录下还有一个_config.yml文件用来进行特定主体的一些个性化设置，后面会经常用到这两个文件，另外就是下面的配置注意:之后的空格 博客基本信息1234title: 博客名称subtitle: 副标题description: 网页描述author: 作者名 推送设置（这里的repo注意修改为自己的github的对应格式）1234deploy: type: git repo: https://github.com/LancelotHolmes/LancelotHolmes.github.io.git branch: master 新建文章在控制台输入 1hexo n \"文章名称\" 这样会在\\blog\\source\\_posts目录下生成对应的markdown文件，你可以通过编辑器打开它并书写你的文章，比如保存后同样的执行./generate.sh然后打开http://localhost:4000/就可以看到你刚刚写好的的文章了。 当然我这里展示的界面略有不同，因为我这里设置主题，后面会具体介绍。 推送到github线下测试发现没有什么问题我们就可以推送到github了，输入如下命令，或者保存为脚本文件deploy.sh然后执行./deploy.sh第一次部署到github时可能会出错error deployer not found:github，可以在控制台输入,注意--save必不可少1npm install hexo-deployer-git --save 12hexo cleanhexo d -g 过程中会让你输入你的github账号和密码，推送成功后，你就可以通过在浏览器输入你的静态站点名称访问你的博客了，比如LancelotHolmes.github.io,至此一个基本的博客就搭好了，接下来你每次需要写文章只需要经过如下步骤 在blog路径下打开git bash控制台然后输入hexo n &quot;文章名&quot; 在路径\\blog\\source\\_posts中编辑对应的markdown，编辑好后保存 执行generate.sh进行线下预览（可选） 执行deploy.sh推送到github就可以通过你的站点访问啦 配置yilia主题前面也给大家看到了我的博客的截图，这里我使用的是yilia主题，目前Hexo主题里面比较热门的两大主题是Next和yilia,这里我就我所配置的一些功能和踩过的坑记录一下。主要包括一些基本的配置以及优化 基本配置基本的主题下载和安装可以直接参照yilia的github对应的教程，同样在blog目录下执行git bash控制台，输入1git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 然后修改站点配置文件，Hexo根目录下(即blog目录的)的 _config.yml1theme: yilia 然后再该文件末尾添加如下语句123456789101112131415161718jsonContent: meta: false pages: false posts: title: true date: true path: true text: true raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true 添加disqus评论目前使用的评论比较多，我早期Hugo时使用过disqus也就还是用这个了，主要是需要注册一个disque账号，然后修改主题目录下的_config.yml 文件，特别注意这里的路径是\\blog\\themes\\yilia,不再是blog下的文件，后面大部分配置都是针对这个文件1disqus: LancelotHolmes 这里的修改为你注册的disqus的short-name,yilia主题下的配置会优先覆盖blog下的配置，如果你是使用其他的评论比如多说，顺言之类的应该适时修改相应的字段后面的false为对应的值，效果如下 添加menu原始的主题menu是这样的123menu: 主页: / 随笔: /tags/随笔/ ，我们可以修改为我们所需要的‘类别’，注意由于yilia作者没有预先设置类别（category）而是把他当作tags使，所以这里配置时链接到对应的tags下的路径，修改如下1234menu: 主页: / 阅读: /tags/Reading/ 机器学习: /tags/ML/ 可以根据需要在新建文章是设置标签自动生成对应的路径，可以在\\blog\\public\\tags下看到，然后修改menu下的对应字段即可 RSS &amp; Sitemap为什么要用RSS,可以看这篇短文，主要是为了方便对你的博客感兴趣的人将你的博客添加到他的订阅列表中，一旦你有更新他可以在第一时间接收到推送。而sitemap则主要是给搜索引擎用的，方便你的站点能够被google收录，当然这里首先需要绑定一个域名，后续我们会具体介绍。这部分主要是参照voidking和magicwangs的博客，执行如下语句123npm install hexo-generator-feed --savenpm install hexo-generator-sitemap --save 然后照常的执行generate.sh，你可以在路径\\blog\\public下看到生成的文件atom.xml和sitemap.xml,接下来在主题目录下的_config.yml 文件，特别注意这里的路径是\\blog\\themes\\yilia里添加123456# SubNavsubnav: github: \"#\" weibo: \"#\" rss: /atom.xml ... 此外在blog目录下的站点配置文件里(这回是在\\blog路径下的_config.yml)添加如下语句1234567891011# Sitemapsitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xm# RSSfeed: type: atom path: atom.xml limit: 100 头像设置有采用本地图片的，也有将图片传到网上制成外链的，我就是用后面那种方法，这里给大家推荐一个不错的工具sm.ms，方便你把你的本地图片传到网上制成markdown、html、url等格式的外部链接制作好后，修改主题目录下的_config.yml 文件，注意这里的路径是\\blog\\themes\\yilia里 12#你的头像urlavatar: https://ooo.0o0.ooo/2017/05/29/592be5c575c4c.jpg 链接当然是你刚刚生成的url的链接 其他社交外链同样是修改主题目录下的_config.yml 文件，注意这里的路径是\\blog\\themes\\yilia里，位置与之前的rss的地方差不多，根据你想展示的社交平台设置，注意链接是你的社交平台的主页之类的，比如1234# SubNavsubnav: github: \"https://github.com/LancelotHolmes/\" weibo: \"http://weibo.com/2925991784/profile?topnav=1&amp;wvr=6\" 文章截断直接生成的文章在主页会全部显示，如果不处理会占据较大的篇幅，我们可以在文章的特定位置设置文章截断，这样主页展示的就是部分文字，首先仍然是修改主题目录下的_config.yml 文件，注意这里的路径是\\blog\\themes\\yilia里 1234# Content# 文章太长，截断按钮文字excerpt_link: more 然后在你的文章的md文件里你想要阶段的位置插入语句1&lt;!-- more --&gt; 例如 这样就实现了文章的截断，而需要阅读全文只需要点击相应的按钮或者标题即可 favicon这个主要是为了好玩，就是你的网页打开后的在浏览器的上面的一个小图标，比如github和我的博客的favicon 我是在freefavicon上直接选的一个，如果你有兴趣可以自己制作1616的图片就行了，将图片复制到路径\\blog\\public下，然后在*主题目录下的_config.yml 文件里修改即可 1favicon: /favicon.png 文章目录这个是作者目前没有实现的部分，但是有其他的方法，主要参照的是这个post,修改主要涉及这么两个文件 在/themes/yilia/layout/_partial/article.ejs文件里18行左右的位置插入 1234567891011121314&lt;% if (!index)&#123; %&gt; &lt;% if (toc(post.content))&#123; %&gt; &lt;div id=\"toc\" class=\"article-toc\"&gt; &lt;h2&gt;目录&lt;/h2&gt; &lt;%- toc(post.content) %&gt; &lt;/div&gt; &lt;script type=\"text/javascript\"&gt; var _article = document.getElementsByClassName('article')[0]; &lt;!-- setTimeout(\"_article.style.marginRight = '211px'\",0); --&gt; setTimeout(\"_article.className += ' article2'\",0); setTimeout(\"document.getElementById('toc').style.right = '15px'\", 0); &lt;/script&gt; &lt;% &#125; %&gt; &lt;% &#125; %&gt; 在\\themes\\yilia\\source-src\\css\\article.scss文件的末尾添加 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@media (max-width: 1099px)&#123; #toc&#123; display: none; &#125; &#125; @media (min-width: 1100px) &#123; #toc&#123; z-index: 999; background-color: #fff; padding: 0 1em; border:1px solid #ddd; position: fixed; top: 100px; right: -180px; transition: right .5s ease-in; width: 150px; h2&#123; margin-bottom:10px; &#125; ol&#123; padding-left: 0!important; &#125; line-height: 1.3em; font-size: 0.8em; float: right; .toc&#123; padding: 0; li&#123; list-style-type: none; margin: .5em 0 .5em; ol&#123; margin: .5em 0 .5em 1em; &#125; &#125; &#125; &#125; .article2&#123; margin-right: 211px; transition: margin-right .5s ease-in; &#125;&#125;.toc-item span &#123;display: table-cell;&#125;span.toc-text &#123;padding-left: 3px;&#125; 即可实现，如图如果没有效果，可以尝试在需要目录的文章头部添加如下语句123456---title: 使用机器学习识别出拍卖场中作弊的机器人用户date: 2017-05-29 19:27:51tags: [ML, Kaggle, Python]toc: true--- 以上内容基本可以搭建起一个还不错的博客了，当然如果你需要其他的一些好玩的功能比如标签云啊，浏览量、动态特效之类的就多多使用搜索引擎咯，另外，遇到什么问题大部分都能在对应的github的issues找到答案。 域名绑定最后就介绍下域名的绑定，如果你也不满足使用github的二级域名，而是想使用专属于你自己的域名，那么这部分就是为你而准备的。 这部分也是参照了不少文章，比如Setsuna和水瓶座IOSer，还有zhaozhiming下面我就简单的叙述一下。 使用工具 namesilo: 购买域名的地方，经过一番查阅感觉这个相对靠谱 DNSPod: 国内的免费DNS服务，后面将域名的dns转移到这个上面 首先要绑定域名自然需要购买一个域名，国内的话可以试试阿里云、万网，学生的话好像可以试试腾讯云的云服务器，是会送免费的.cn域名，但是国内购买的话需要去公安局备份好象，感觉比较麻烦我就是用了国外的，比如namesilo，其他的介绍可以看看zhaozhiming的对应观点，当然他和我一样也是在知乎上的一个地方看到的； 在namesilo上注册的教程可以看这个,选择好域名并付款后（支持支付宝）的基本设置可以看这个，接下来： CNAME在你的本地站点目录里的source目录下添加一个CNAME文件，不带后缀，用编辑器打开并输入你购买的域名，不要http也不要www,比如我的域名是izhaoyi.top,我就写入izhaoyi.top然后保存以后执行deploy。 DNS设置namesilo登陆namesilo之后，右上角的Manage My Domains点击进入后,选择然后下拉，选择在上面手动添加两条记录，如图然后回到域名管理界面，选中域名那一栏最前面的勾选框，然后选择上面的Change Namesevers图标最后在在NameServer1和NameServer2中填写 DNSPod 的 nameserver 地址f1g1ns1.dnspod.net，f1g1ns2.dnspod.net DNSPod同样完成注册后，在域名注册中需要手动添加你购买的域名，并添加一些记录，最终如图这里的192.30.252.153是github pages的ip地址，固定设置成这个就好，然后稍微等一段时间（也许不用等）应该就可以通过访问你的域名比如izhaoyi.top或者你之前的github站点名比如LancelotHolmes.github.io访问你的博客啦。 最后，欢迎大家来我的博客踩踩，也欢迎跟我互加友链啊~","tags":[{"name":"Coding","slug":"Coding","permalink":"http://yoursite.com/tags/Coding/"},{"name":"blog","slug":"blog","permalink":"http://yoursite.com/tags/blog/"},{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"yilia","slug":"yilia","permalink":"http://yoursite.com/tags/yilia/"}]},{"title":"使用机器学习识别出拍卖场中作弊的机器人用户(二)","date":"2017-05-29T12:52:19.000Z","path":"2017/05/29/HumenOrRobot2/","text":"原创文章，首发于SegmentFault 本文承接上一篇文章:使用机器学习识别出拍卖场中作弊的机器人用户 本项目为kaggle上Facebook举行的一次比赛，地址见数据来源，完整代码见我的github,欢迎来玩~ 代码 数据探索——Data_Exploration.ipynb 数据预处理&amp;特征工程——Feature_Engineering.ipynb &amp; Feature_Engineering2.ipynb 模型设计及评测——Model_Design.ipynb 项目数据来源 kaggle项目所需额外工具包 numpy pandas matplotlib sklearn xgboost lightgbm mlxtend: 含有聚和算法Stacking项目整体运行时间预估为60min左右，在Ubuntu系统，8G内存，运行结果见所提交的jupyter notebook文件 由于文章内容过长，所以分为两篇文章，总共包含四个部分 数据探索 数据预处理及特征工程 模型设计 评估及总结 特征工程(续)12345import numpy as npimport pandas as pdimport pickle%matplotlib inlinefrom IPython.display import display 12# bids = pd.read_csv('bids.csv')bids = pickle.load(open('bids.pkl')) 12print bids.shapedisplay(bids.head()) (7656329, 9) bid_id bidder_id auction merchandise device time country ip url 0 8dac2b259fd1c6d1120e519fb1ac14fbqvax8 ewmzr jewelry phone0 9759243157894736 us 69.166.231.58 vasstdc27m7nks3 1 668d393e858e8126275433046bbd35c6tywop aeqok furniture phone1 9759243157894736 in 50.201.125.84 jmqlhflrzwuay9c 2 aa5f360084278b35d746fa6af3a7a1a5ra3xe wa00e home goods phone2 9759243157894736 py 112.54.208.157 vasstdc27m7nks3 3 3939ac3ef7d472a59a9c5f893dd3e39fh9ofi jefix jewelry phone4 9759243157894736 in 18.99.175.133 vasstdc27m7nks3 4 8393c48eaf4b8fa96886edc7cf27b372dsibi jefix jewelry phone5 9759243157894736 in 145.138.5.37 vasstdc27m7nks3 1bidders = bids.groupby('bidder_id') 针对国家、商品单一特征多类别转换为多个独立特征进行统计1234567891011121314cates = (bids['merchandise'].unique()).tolist()countries = (bids['country'].unique()).tolist()def dummy_coun_cate(group): coun_cate = dict.fromkeys(cates, 0) coun_cate.update(dict.fromkeys(countries, 0)) for cat, value in group['merchandise'].value_counts().iteritems(): coun_cate[cat] = value for c in group['country'].unique(): coun_cate[c] = 1 coun_cate = pd.Series(coun_cate) return coun_cate 1bidder_coun_cate = bidders.apply(dummy_coun_cate) 12display(bidder_coun_cate.describe())bidder_coun_cate.to_csv('coun_cate.csv') - ad ae af ag al am an ao ar at … count 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 … mean 0.002724 0.205629 0.054774 0.001059 0.048570 0.023907 0.000303 0.036314 0.120442 0.052655 … std 0.052121 0.404191 0.227555 0.032530 0.214984 0.152770 0.017395 0.187085 0.325502 0.223362 … min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 … 25% 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 … 50% 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 … 75% 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 … max 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 … 同样的，对于每个用户需要统计他对于自己每次竞拍行为的时间间隔情况 12345678910111213141516171819def bidder_interval(group): time_diff = np.ediff1d(group['time']) bidder_interval = &#123;&#125; if len(time_diff) == 0: diff_mean = 0 diff_std = 0 diff_median = 0 diff_zeros = 0 else: diff_mean = np.mean(time_diff) diff_std = np.std(time_diff) diff_median = np.median(time_diff) diff_zeros = time_diff.shape[0] - np.count_nonzero(time_diff) bidder_interval['tmean'] = diff_mean bidder_interval['tstd'] = diff_std bidder_interval['tmedian'] = diff_median bidder_interval['tzeros'] = diff_zeros bidder_interval = pd.Series(bidder_interval) return bidder_interval 1bidder_inv = bidders.apply(bidder_interval) 12display(bidder_inv.describe())bidder_inv.to_csv('bidder_inv.csv') - tmean tmedian tstd tzeros count 6.609000e+03 6.609000e+03 6.609000e+03 6609.0 mean 2.933038e+12 1.860285e+12 3.440901e+12 122.986231 std 8.552343e+12 7.993497e+12 6.512992e+12 3190.805229 min 0.000000e+00 0.000000e+00 0.000000e+00 0.000000 25% 1.192853e+10 2.578947e+09 1.749995e+09 0.000000 50% 2.641139e+11 5.726316e+10 5.510107e+11 0.000000 75% 1.847456e+12 6.339474e+11 2.911282e+12 0.000000 max 7.610295e+13 7.610295e+13 3.800092e+13 231570.000000 按照用户-拍卖场分组进一步分析之前的统计是按照用户进行分组，针对各个用户从整体上针对竞标行为统计其各项特征，下面根据拍卖场来对用户进一步细分，看一看每个用户在不同拍卖场的行为模式,类似上述按照用户分组来统计各个用户的各项特征，针对用户-拍卖场结对分组进行统计以下特征 基本计数统计，针对各个用户在各个拍卖场统计设备、国家、ip、url、商品类别、竞标次数等特征的数目作为新的特征 时间间隔统计：统计各个用户在各个拍卖场每次竞拍的时间间隔的 均值、方差、中位数和0值 针对商品类别、国家进一步转化为多类别进行统计 123456789101112131415161718192021222324252627282930313233343536def auc_features_count(group): time_diff = np.ediff1d(group['time']) if len(time_diff) == 0: diff_mean = 0 diff_std = 0 diff_median = 0 diff_zeros = 0 else: diff_mean = np.mean(time_diff) diff_std = np.std(time_diff) diff_median = np.median(time_diff) diff_zeros = time_diff.shape[0] - np.count_nonzero(time_diff) row = dict.fromkeys(cates, 0) row.update(dict.fromkeys(countries, 0)) row['devices_c'] = group['device'].unique().shape[0] row['countries_c'] = group['country'].unique().shape[0] row['ip_c'] = group['ip'].unique().shape[0] row['url_c'] = group['url'].unique().shape[0]# row['merch_c'] = group['merchandise'].unique().shape[0] row['bids_c'] = group.shape[0] row['tmean'] = diff_mean row['tstd'] = diff_std row['tmedian'] = diff_median row['tzeros'] = diff_zeros for cat, value in group['merchandise'].value_counts().iteritems(): row[cat] = value for c in group['country'].unique(): row[c] = 1 row = pd.Series(row) return row 1bidder_auc = bids.groupby(['bidder_id', 'auction']).apply(auc_features_count) 1bidder_auc.to_csv('bids_auc.csv') 1print bidder_auc.shape (382336, 218) 模型设计与参数评估合并特征对之前生成的各项特征进行合并产生最终的特征空间 1234import numpy as npimport pandas as pd%matplotlib inlinefrom IPython.display import display 首先将之前根据用户分组的统计特征合并起来，然后将其与按照用户-拍卖场结对分组的特征合并起来，最后加上时间特征，分别于训练集、测试集连接生成后续进行训练和预测的特征数据文件 123456789101112131415161718192021222324def merge_data(): train = pd.read_csv('train.csv') test = pd.read_csv('test.csv') time_differences = pd.read_csv('tdiff.csv', index_col=0) bids_auc = pd.read_csv('bids_auc.csv') bids_auc = bids_auc.groupby('bidder_id').mean() bidders = pd.read_csv('cnt_bidder.csv', index_col=0) country_cate = pd.read_csv('coun_cate.csv', index_col=0) bidder_inv = pd.read_csv('bidder_inv.csv', index_col=0) bidders = bidders.merge(country_cate, right_index=True, left_index=True) bidders = bidders.merge(bidder_inv, right_index=True, left_index=True) bidders = bidders.merge(bids_auc, right_index=True, left_index=True) bidders = bidders.merge(time_differences, right_index=True, left_index=True) train = train.merge(bidders, left_on='bidder_id', right_index=True) train.to_csv('train_full.csv', index=False) test = test.merge(bidders, left_on='bidder_id', right_index=True) test.to_csv('test_full.csv', index=False) 1merge_data() 1234train_full = pd.read_csv('train_full.csv')test_full = pd.read_csv('test_full.csv')print train_full.shapeprint test_full.shape (1983, 445) (4626, 444) 123456789train_full['outcome'] = train_full['outcome'].astype(int)ytrain = train_full['outcome']train_full.drop('outcome', 1, inplace=True)test_ids = test_full['bidder_id']labels = ['payment_account', 'address', 'bidder_id']train_full.drop(labels=labels, axis=1, inplace=True)test_full.drop(labels=labels, axis=1, inplace=True) 设计交叉验证模型选择根据之前的分析，由于当前的数据集中存在正负例不均衡的问题，所以考虑选取了RandomForestClassfier, GradientBoostingClassifier, xgboost, lightgbm等四种模型来针对数据及进行训练和预测，确定最终模型的基本思路如下： 对四个模型分别使用评价函数roc_auc进行交叉验证并绘制auc曲线，对各个模型的多轮交叉验证得分取平均值并输出 根据得分确定最终选用的一个或多个模型 若最后发现一个模型的表现大幅度优于其他所有模型，则选择该模型进一步调参 若最后发现多个模型表现都不错，则进行模型的集成，得到聚合模型 使用GridSearchCV来从人为设定的参数列表中选择最佳的参数组合确定最终的模型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from scipy import interpimport matplotlib.pyplot as pltfrom itertools import cycle# from sklearn.cross_validation import StratifiedKFoldfrom sklearn.model_selection import StratifiedKFoldfrom sklearn.metrics import roc_auc_score, roc_curve, aucdef kfold_plot(train, ytrain, model):# kf = StratifiedKFold(y=ytrain, n_folds=5) kf = StratifiedKFold(n_splits=5) scores = [] mean_tpr = 0.0 mean_fpr = np.linspace(0, 1, 100) exe_time = [] colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue']) lw = 2 i=0 for (train_index, test_index), color in zip(kf.split(train, ytrain), colors): X_train, X_test = train.iloc[train_index], train.iloc[test_index] y_train, y_test = ytrain.iloc[train_index], ytrain.iloc[test_index] begin_t = time.time() predictions = model(X_train, X_test, y_train) end_t = time.time() exe_time.append(round(end_t-begin_t, 3))# model = model# model.fit(X_train, y_train) # predictions = model.predict_proba(X_test)[:, 1] scores.append(roc_auc_score(y_test.astype(float), predictions)) fpr, tpr, thresholds = roc_curve(y_test, predictions) mean_tpr += interp(mean_fpr, fpr, tpr) mean_tpr[0] = 0.0 roc_auc = auc(fpr, tpr) plt.plot(fpr, tpr, lw=lw, color=color, label='ROC fold %d (area = %0.2f)' % (i, roc_auc)) i += 1 plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k', label='Luck') mean_tpr /= kf.get_n_splits(train, ytrain) mean_tpr[-1] = 1.0 mean_auc = auc(mean_fpr, mean_tpr) plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--', label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw) plt.xlim([-0.05, 1.05]) plt.ylim([-0.05, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Receiver operating characteristic') plt.legend(loc='lower right') plt.show() # print 'scores: ', scores print 'mean scores: ', np.mean(scores) print 'mean model process time: ', np.mean(exe_time), 's' return scores, np.mean(scores), np.mean(exe_time) 收集各个模型进行交叉验证的结果包括每轮交叉验证的auc得分、auc的平均得分以及模型的训练时间 123dct_scores = &#123;&#125;mean_score = &#123;&#125;mean_time = &#123;&#125; RandomForestClassifier12from sklearn.model_selection import GridSearchCVimport time 12345678910from sklearn.ensemble import RandomForestClassifierdef forest_model(X_train, X_test, y_train):# begin_t = time.time() model = RandomForestClassifier(n_estimators=160, max_features=35, max_depth=8, random_state=7) model.fit(X_train, y_train) # end_t = time.time()# print 'train time of forest model: ',round(end_t-begin_t, 3), 's' predictions = model.predict_proba(X_test)[:, 1] return predictions 12dct_scores['forest'], mean_score['forest'], mean_time['forest'] = kfold_plot(train_full, ytrain, forest_model)# kfold_plot(train_full, ytrain, model_forest) mean scores: 0.909571935157 mean model process time: 0.643 s 123456from sklearn.ensemble import GradientBoostingClassifierdef gradient_model(X_train, X_test, y_train): model = GradientBoostingClassifier(n_estimators=200, random_state=7, max_depth=5, learning_rate=0.03) model.fit(X_train, y_train) predictions = model.predict_proba(X_test)[:, 1] return predictions 1dct_scores['gbm'], mean_score['gbm'], mean_time['gbm'] = kfold_plot(train_full, ytrain, gradient_model) mean scores: 0.911847771023 mean model process time: 4.1948 s 123456789import xgboost as xgbdef xgboost_model(X_train, X_test, y_train): X_train = xgb.DMatrix(X_train.values, label=y_train.values) X_test = xgb.DMatrix(X_test.values) params = &#123;'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': 1, 'seed': 7, 'max_depth': 6, 'eta': 0.01&#125; model = xgb.train(params, X_train, 600) predictions = model.predict(X_test) return predictions /home/lancelot/anaconda2/envs/udacity/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20. &quot;This module will be removed in 0.20.&quot;, DeprecationWarning) 1dct_scores['xgboost'], mean_score['xgboost'], mean_time['xgboost'] = kfold_plot(train_full, ytrain, xgboost_model) mean scores: 0.915372340426 mean model process time: 3.1482 s 1234567import lightgbm as lgbdef lightgbm_model(X_train, X_test, y_train): X_train = lgb.Dataset(X_train.values, y_train.values) params = &#123;'objective': 'binary', 'metric': &#123;'auc'&#125;, 'learning_rate': 0.01, 'max_depth': 6, 'seed': 7&#125; model = lgb.train(params, X_train, num_boost_round=600) predictions = model.predict(X_test) return predictions 1dct_scores['lgbm'], mean_score['lgbm'], mean_time['lgbm'] = kfold_plot(train_full, ytrain, lightgbm_model) mean scores: 0.921512158055 mean model process time: 0.3558 s 模型比较比较四个模型在交叉验证机上的roc_auc平均得分和模型训练的时间 123456789101112131415def plot_model_comp(title, y_label, dct_result): data_source = dct_result.keys() y_pos = np.arange(len(data_source)) # model_auc = [0.910, 0.912, 0.915, 0.922] model_auc = dct_result.values() barlist = plt.bar(y_pos, model_auc, align='center', alpha=0.5) # get the index of highest score max_val = max(model_auc) idx = model_auc.index(max_val) barlist[idx].set_color('r') plt.xticks(y_pos, data_source) plt.ylabel(y_label) plt.title(title) plt.show() print 'The highest auc score is &#123;0&#125; of model: &#123;1&#125;'.format(max_val, data_source[idx]) 1plot_model_comp('Model Performance', 'roc-auc score', mean_score) The highest auc score is 0.921512158055 of model: lgbm 123456789101112131415def plot_time_comp(title, y_label, dct_result): data_source = dct_result.keys() y_pos = np.arange(len(data_source)) # model_auc = [0.910, 0.912, 0.915, 0.922] model_auc = dct_result.values() barlist = plt.bar(y_pos, model_auc, align='center', alpha=0.5) # get the index of highest score min_val = min(model_auc) idx = model_auc.index(min_val) barlist[idx].set_color('r') plt.xticks(y_pos, data_source) plt.ylabel(y_label) plt.title(title) plt.show() print 'The shortest time is &#123;0&#125; of model: &#123;1&#125;'.format(min_val, data_source[idx]) 1plot_time_comp('Time of Building Model', 'time(s)', mean_time) The shortest time is 0.3558 of model: lgbm 12345678910111213141516171819auc_forest = dct_scores['forest']auc_gb = dct_scores['gbm']auc_xgb = dct_scores['xgboost']auc_lgb = dct_scores['lgbm']print 'std of forest auc score: ',np.std(auc_forest)print 'std of gbm auc score: ',np.std(auc_gb)print 'std of xgboost auc score: ',np.std(auc_xgb)print 'std of lightgbm auc score: ',np.std(auc_lgb)data_source = ['roc-fold-1', 'roc-fold-2', 'roc-fold-3', 'roc-fold-4', 'roc-fold-5']y_pos = np.arange(len(data_source))plt.plot(y_pos, auc_forest, 'b-', label='forest')plt.plot(y_pos, auc_gb, 'r-', label='gbm')plt.plot(y_pos, auc_xgb, 'y-', label='xgboost')plt.plot(y_pos, auc_lgb, 'g-', label='lightgbm')plt.title('roc-auc score of each epoch')plt.xlabel('epoch')plt.ylabel('roc-auc score')plt.legend()plt.show() std of forest auc score: 0.0413757504568 std of gbm auc score: 0.027746291638 std of xgboost auc score: 0.0232931322563 std of lightgbm auc score: 0.0287156755513 单从5次交叉验证的各模型roc-auc得分来看，xgboost的得分相对比较稳定 聚合模型由上面的模型比较可以发现，四个模型的经过交叉验证的表现都不错，但是综合而言，xgboost和lightgbm更胜一筹，而且两者的训练时间也相对更短一些，所以接下来考虑进行模型的聚合，思路如下： 先通过GridSearchCV分别针对四个模型在整个训练集上进行调参获得最佳的子模型 针对子模型使用 stacking: 第三方库mlxtend里的stacking方法对子模型进行聚合得到聚合模型，并采用之前相同的cv方法对该模型进行打分评价 voting: 使用sklearn内置的VotingClassifier进行四个模型的聚合 最终对聚合模型在一次进行cv验证评分，根据结果确定最终的模型 先通过交叉验证针对模型选择参数组合 12345678910def choose_xgb_model(X_train, y_train): tuned_params = [&#123;'objective': ['binary:logistic'], 'learning_rate': [0.01, 0.03, 0.05], 'n_estimators': [100, 150, 200], 'max_depth':[4, 6, 8]&#125;] begin_t = time.time() clf = GridSearchCV(xgb.XGBClassifier(seed=7), tuned_params, scoring='roc_auc') clf.fit(X_train, y_train) end_t = time.time() print 'train time: ',round(end_t-begin_t, 3), 's' print 'current best parameters of xgboost: ',clf.best_params_ return clf.best_estimator_ 1bst_xgb = choose_xgb_model(train_full, ytrain) train time: 48.141 s current best parameters of xgboost: {&apos;n_estimators&apos;: 150, &apos;objective&apos;: &apos;binary:logistic&apos;, &apos;learning_rate&apos;: 0.05, &apos;max_depth&apos;: 4} 12345678910def choose_lgb_model(X_train, y_train): tuned_params = [&#123;'objective': ['binary'], 'learning_rate': [0.01, 0.03, 0.05], 'n_estimators': [100, 150, 200], 'max_depth':[4, 6, 8]&#125;] begin_t = time.time() clf = GridSearchCV(lgb.LGBMClassifier(seed=7), tuned_params, scoring='roc_auc') clf.fit(X_train, y_train) end_t = time.time() print 'train time: ',round(end_t-begin_t, 3), 's' print 'current best parameters of lgb: ',clf.best_params_ return clf.best_estimator_ 1bst_lgb = choose_lgb_model(train_full, ytrain) train time: 12.543 s current best parameters of lgb: {&apos;n_estimators&apos;: 150, &apos;objective&apos;: &apos;binary&apos;, &apos;learning_rate&apos;: 0.05, &apos;max_depth&apos;: 4} 先使用stacking集成两个综合表现最佳的模型lgb和xgb，此处元分类器使用较为简单的LR模型来在已经训练好了并且经过参数选择的模型上进一步优化预测结果 12345678910from mlxtend.classifier import StackingClassifierfrom sklearn import linear_modeldef stacking_model(X_train, X_test, y_train): lr = linear_model.LogisticRegression(random_state=7) sclf = StackingClassifier(classifiers=[bst_xgb, bst_lgb], use_probas=True, average_probas=False, meta_classifier=lr) sclf.fit(X_train, y_train) predictions = sclf.predict_proba(X_test)[:, 1] return predictions 1dct_scores['stacking_1'], mean_score['stacking_1'], mean_time['stacking_1'] = kfold_plot(train_full, ytrain, stacking_model) mean scores: 0.92157674772 mean model process time: 0.7022 s 可以看到相对之前的得分最高的模型lightgbm，将lightgbm与xgboost经过stacking集成并且使用lr作为元分类器得到的auc得分有轻微的提升，接下来考虑进一步加入另外的RandomForest和GBDT模型看看增加一点模型的差异性使用Stacking是不是会有所提升 123456789def choose_forest_model(X_train, y_train): tuned_params = [&#123;'n_estimators': [100, 150, 200], 'max_features': [8, 15, 30], 'max_depth':[4, 8, 10]&#125;] begin_t = time.time() clf = GridSearchCV(RandomForestClassifier(random_state=7), tuned_params, scoring='roc_auc') clf.fit(X_train, y_train) end_t = time.time() print 'train time: ',round(end_t-begin_t, 3), 's' print 'current best parameters: ',clf.best_params_ return clf.best_estimator_ 1bst_forest = choose_forest_model(train_full, ytrain) train time: 42.201 s current best parameters: {&apos;max_features&apos;: 15, &apos;n_estimators&apos;: 150, &apos;max_depth&apos;: 8} 12345678910def choose_gradient_model(X_train, y_train): tuned_params = [&#123;'n_estimators': [100, 150, 200], 'learning_rate': [0.03, 0.05, 0.07], 'min_samples_leaf': [8, 15, 30], 'max_depth':[4, 6, 8]&#125;] begin_t = time.time() clf = GridSearchCV(GradientBoostingClassifier(random_state=7), tuned_params, scoring='roc_auc') clf.fit(X_train, y_train) end_t = time.time() print 'train time: ',round(end_t-begin_t, 3), 's' print 'current best parameters: ',clf.best_params_ return clf.best_estimator_ 1bst_gradient = choose_gradient_model(train_full, ytrain) train time: 641.872 s current best parameters: {&apos;n_estimators&apos;: 100, &apos;learning_rate&apos;: 0.03, &apos;max_depth&apos;: 8, &apos;min_samples_leaf&apos;: 30} 1234567def stacking_model2(X_train, X_test, y_train): lr = linear_model.LogisticRegression(random_state=7) sclf = StackingClassifier(classifiers=[bst_xgb, bst_forest, bst_gradient, bst_lgb], use_probas=True, average_probas=False, meta_classifier=lr) sclf.fit(X_train, y_train) predictions = sclf.predict_proba(X_test)[:, 1] return predictions 1dct_scores['stacking_2'], mean_score['stacking_2'], mean_time['stacking_2'] = kfold_plot(train_full, ytrain, stacking_model2) mean scores: 0.92686550152 mean model process time: 4.0878 s 可以看到四个模型的聚合效果比用两个模型的stacking聚合效果要好不少，接下来尝试使用voting对四个模型进行聚合 12345678from sklearn.ensemble import VotingClassifierdef voting_model(X_train, X_test, y_train): vclf = VotingClassifier(estimators=[('xgb', bst_xgb), ('rf', bst_forest), ('gbm',bst_gradient), ('lgb', bst_lgb)], voting='soft', weights=[2, 1, 1, 2]) vclf.fit(X_train, y_train) predictions = vclf.predict_proba(X_test)[:, 1] return predictions 1dct_scores['voting'], mean_score['voting'], mean_time['voting'] = kfold_plot(train_full, ytrain, voting_model) mean scores: 0.926889564336 mean model process time: 4.055 s 再次比较单模型与集成模型的得分 1plot_model_comp('Model Performance', 'roc-auc score', mean_score) The highest auc score is 0.926889564336 of model: voting 由上可以看到最终通过voting将四个模型进行聚合可以得到得分最高的模型，确定为最终用来预测的模型 综合模型，对测试文件进行最终预测12345678910111213141516# predict(train_full, test_full, y_train)def submit(X_train, X_test, y_train, test_ids): predictions = voting_model(X_train, X_test, y_train) sub = pd.read_csv('sampleSubmission.csv') result = pd.DataFrame() result['bidder_id'] = test_ids result['outcome'] = predictions sub = sub.merge(result, on='bidder_id', how='left') # Fill missing values with mean mean_pred = np.mean(predictions) sub.fillna(mean_pred, inplace=True) sub.drop('prediction', 1, inplace=True) sub.to_csv('result.csv', index=False, header=['bidder_id', 'prediction']) 1submit(train_full, test_full, ytrain, test_ids) 最终结果提交到kaggle上进行评分，得分如下 以上就是整个完整的流程，当然还有很多模型可以尝试，很多聚合方法也可以使用，此外，特征工程部分还有很多空间可以挖掘，就留给大家去探索啦~ 参考资料 Chen, K. T., Pao, H. K. K., &amp; Chang, H. C. (2008, October). Game bot identification based on manifold learning. In Proceedings of the 7th ACM SIGCOMM Workshop on Network and System Support for Games (pp. 21-26). ACM. Alayed, H., Frangoudes, F., &amp; Neuman, C. (2013, August). Behavioral-based cheating detection in online first person shooters using machine learning techniques. In Computational Intelligence in Games (CIG), 2013 IEEE Conference on (pp. 1-8). IEEE. https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot/data http://stats.stackexchange.com/a/132832/152084 https://en.wikipedia.org/wiki/Receiver_operating_characteristic https://en.wikipedia.org/wiki/Random_forest https://en.wikipedia.org/wiki/Gradient_boosting https://xgboost.readthedocs.io/en/latest//parameter.html#parameters-for-tree-booster https://github.com/Microsoft/LightGBM https://en.wikipedia.org/wiki/Receiver_operating_characteristic http://stackoverflow.com/questions/29530232/python-pandas-check-if-any-value-is-nan-in-dataframe http://pandas.pydata.org/pandas-docs/stable/missing_data.html http://stackoverflow.com/a/18272653/6653189 http://www.cnblogs.com/jasonfreak/p/5720137.html kaggle ensembling guide","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"使用机器学习识别出拍卖场中作弊的机器人用户","date":"2017-05-29T11:27:51.000Z","path":"2017/05/29/HumenOrRobot/","text":"原创文章，首发于SegmentFault 本项目为kaggle上Facebook举行的一次比赛，地址见数据来源，完整代码见我的github,欢迎来玩~ 代码 数据探索——Data_Exploration.ipynb 数据预处理&amp;特征工程——Feature_Engineering.ipynb &amp; Feature_Engineering2.ipynb 模型设计及评测——Model_Design.ipynb 项目数据来源 kaggle项目所需额外工具包 numpy pandas matplotlib sklearn xgboost lightgbm mlxtend: 含有聚和算法Stacking项目整体运行时间预估为60min左右，在Ubuntu系统，8G内存，运行结果见所提交的jupyter notebook文件 由于文章内容过长，所以分为两篇文章，总共包含四个部分 数据探索 数据预处理及特征工程 模型设计 评估及总结 数据探索1234import numpy as npimport pandas as pd%matplotlib inlinefrom IPython.display import display 123df_bids = pd.read_csv('bids.csv', low_memory=False)df_train = pd.read_csv('train.csv')df_test = pd.read_csv('test.csv') 1df_bids.head(3) bid_id bidder_id auction merchandise device time country ip url 0 8dac2b259fd1c6d1120e519fb1ac14fbqvax8 ewmzr jewelry phone0 9759243157894736 us 69.166.231.58 vasstdc27m7nks3 1 668d393e858e8126275433046bbd35c6tywop aeqok furniture phone1 9759243157894736 in 50.201.125.84 jmqlhflrzwuay9c 2 aa5f360084278b35d746fa6af3a7a1a5ra3xe wa00e home goods phone2 9759243157894736 py 112.54.208.157 vasstdc27m7nks3 12df_train.head(3)# df_train.dtypes bidder_id payment_account address outcome 91a3c57b13234af24875c56fb7e2b2f4rb56a a3d2de7675556553a5f08e4c88d2c228754av a3d2de7675556553a5f08e4c88d2c228vt0u4 0.0 624f258b49e77713fc34034560f93fb3hu3jo a3d2de7675556553a5f08e4c88d2c228v1sga ae87054e5a97a8f840a3991d12611fdcrfbq3 0.0 1c5f4fc669099bfbfac515cd26997bd12ruaj a3d2de7675556553a5f08e4c88d2c2280cybl 92520288b50f03907041887884ba49c0cl0pd 0.0 异常数据检测1234# 查看各表格中是否存在空值print 'Is there any missing value in bids?',df_bids.isnull().any().any()print 'Is there any missing value in train?',df_train.isnull().any().any()print 'Is there any missing value in test?',df_test.isnull().any().any() Is there any missing value in bids? True Is there any missing value in train? False Is there any missing value in test? False 整个对三个数据集进行空值判断，发现用户数据训练集和测试集均无缺失数据，而在竞标行为数据集中存在缺失值的情况，下面便针对bids数据进一步寻找缺失值 123# nan_rows = df_bids[df_bids.isnull().T.any().T]# print nan_rowspd.isnull(df_bids).any() bid_id False bidder_id False auction False merchandise False device False time False country True ip False url False dtype: bool 1234missing_country = df_bids['country'].isnull().sum().sum()print 'No. of missing country: ', missing_countrynormal_country = df_bids['country'].notnull().sum().sum()print 'No. of normal country: ', normal_country No. of missing country: 8859 No. of normal country: 7647475 123456789import matplotlib.pyplot as pltlabels = ['unknown', 'normal']sizes = [missing_country, normal_country]explode = (0.1, 0)fig1, ax1 = plt.subplots()ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)ax1.axis('equal')plt.title('Distribution of missing countries vs. normal countries')plt.show() 综合上述的分析可以发现，在竞标行为用户的country一栏属性中存在很少一部分用户行为是没有country记录的，在预处理部分可以针对这部分缺失数据进行填充操作，有两种思路： 针对原始行为数据按照用户分组后，看看每个对应的用户竞标时经常所位于的国家信息，对缺失值填充常驻国家 针对原始行为数据按照用户分组后，按时间顺序对每组用户中的缺失值前向或后向填充相邻的国家信息 12345678# 查看各个数据的记录数# 看看数据的id是否是唯一标识print df_bids.shape[0]print len(df_bids['bid_id'].unique())print df_train.shape[0]print len(df_train['bidder_id'].unique())print df_test.shape[0]print len(df_test['bidder_id'].unique()) 7656334 7656334 2013 2013 4700 4700 12345678# 简单统计各项基本特征（类别特征）的数目（除去时间）print 'total bidder in bids: ', len(df_bids['bidder_id'].unique())print 'total auction in bids: ', len(df_bids['auction'].unique())print 'total merchandise in bids: ', len(df_bids['merchandise'].unique())print 'total device in bids: ', len(df_bids['device'].unique())print 'total country in bids: ', len(df_bids['country'].unique())print 'total ip in bids: ', len(df_bids['ip'].unique())print 'total url in bids: ', len(df_bids['url'].unique()) total bidder in bids: 6614 total auction in bids: 15051 total merchandise in bids: 10 total device in bids: 7351 total country in bids: 200 total ip in bids: 2303991 total url in bids: 1786351 由上述基本特征可以看到： 竞标行为中的用户总数少于训练集+测试集的用户数，也就是说并不是一一对应的，接下来验证下竞标行为数据中的用户是否完全来自训练集和测试集 商品类别和国家的种类相对其他特征较少，可以作为天然的类别特征提取出来进行处理，而其余的特征可能更多的进行计数统计 12345lst_all_users = (df_train['bidder_id'].unique()).tolist() + (df_test['bidder_id'].unique()).tolist()print 'total bidders of train and test set',len(lst_all_users)lst_bidder = (df_bids['bidder_id'].unique()).tolist()print 'total bidders in bids set',len(lst_bidder)print 'Is bidders in bids are all from train+test set? ',set(lst_bidder).issubset(set(lst_all_users)) total bidders of train and test set 6713 total bidders in bids set 6614 Is bidders in bids are all from train+test set? True 123456lst_nobids = [i for i in lst_all_users if i not in lst_bidder]print 'No. of bidders never bid: ',len(lst_nobids)lst_nobids_train = [i for i in lst_nobids if i in (df_train['bidder_id'].unique()).tolist()]lst_nobids_test = [i for i in lst_nobids if i in (df_test['bidder_id'].unique()).tolist()]print 'No. of bidders never bid in train set: ',len(lst_nobids_train)print 'No. of bidders never bid in test set: ',len(lst_nobids_test) No. of bidders never bid: 99 No. of bidders never bid in train set: 29 No. of bidders never bid in test set: 70 12345678data_source = ['train', 'test']y_pos = np.arange(len(data_source))num_never_bids = [len(lst_nobids_train), len(lst_nobids_test)]plt.bar(y_pos, num_never_bids, align='center', alpha=0.5)plt.xticks(y_pos, data_source)plt.ylabel('bidders no bids')plt.title('Source of no bids bidders')plt.show() 1print df_train[(df_train['bidder_id'].isin(lst_nobids_train)) &amp; (df_train['outcome']==1.0)] Empty DataFrame Columns: [bidder_id, payment_account, address, outcome] Index: [] 由上述计算可知存在99个竞标者无竞标记录，其中29位来自训练集，70位来自测试集，而且这29位来自训练集的竞标者未被标记为机器人用户，所以可以针对测试集中的这70位用户后续标记为人类或者取平均值处理 12# check the partition of bots in trainprint (df_train[df_train['outcome'] == 1].shape[0]*1.0) / df_train.shape[0] * 100,'%' 5.11674118231 % 训练集中的标记为机器人的用户占所有用户数目约5% 12df_train.groupby('outcome').size().plot(labels=['Human', 'Robot'], kind='pie', autopct='%.2f', figsize=(4, 4), title='Distribution of Human vs. Robots', legend=True) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f477135c5d0&gt; 由上述训练集中的正负例分布可以看到本数据集正负例比例失衡，所以后续考虑使用AUC（不受正负例比例影响）作为评价指标，此外尽量采用Gradient Boosting族模型来进行训练 数据预处理与特征工程12345import numpy as npimport pandas as pdimport pickle%matplotlib inlinefrom IPython.display import display 123bids = pd.read_csv('bids.csv')train = pd.read_csv('train.csv')test = pd.read_csv('test.csv') 处理缺失数据针对前面数据探索部分所发现的竞标行为数据中存在的国家属性缺失问题，考虑使用针对原始行为数据按照用户分组后，按时间顺序对每组用户中的缺失值前向或后向填充相邻的国家信息的方法来进行缺失值的填充处理 1display(bids.head(3)) bid_id bidder_id auction merchandise device time country ip url 0 8dac2b259fd1c6d1120e519fb1ac14fbqvax8 ewmzr jewelry phone0 9759243157894736 us 69.166.231.58 vasstdc27m7nks3 1 668d393e858e8126275433046bbd35c6tywop aeqok furniture phone1 9759243157894736 in 50.201.125.84 jmqlhflrzwuay9c 2 aa5f360084278b35d746fa6af3a7a1a5ra3xe wa00e home goods phone2 9759243157894736 py 112.54.208.157 vasstdc27m7nks3 12# pd.algos.is_monotonic_int64(bids.time.values, True)[0]print 'Is the time monotonically non-decreasing? ', pd.Index(bids['time']).is_monotonic Is the time monotonically non-decreasing? False 123# bidder_group = bids.sort_values(['bidder_id', 'time']).groupby('bidder_id')bids['country'] = bids.sort_values(['bidder_id', 'time']).groupby('bidder_id')['country'].ffill()bids['country'] = bids.sort_values(['bidder_id', 'time']).groupby('bidder_id')['country'].bfill() 1display(bids.head(3)) bid_id bidder_id auction merchandise device time country ip url 0 8dac2b259fd1c6d1120e519fb1ac14fbqvax8 ewmzr jewelry phone0 9759243157894736 us 69.166.231.58 vasstdc27m7nks3 1 668d393e858e8126275433046bbd35c6tywop aeqok furniture phone1 9759243157894736 in 50.201.125.84 jmqlhflrzwuay9c 2 aa5f360084278b35d746fa6af3a7a1a5ra3xe wa00e home goods phone2 9759243157894736 py 112.54.208.157 vasstdc27m7nks3 12print 'Is there any missing value in bids?',bids.isnull().any().any()# pickle.dump(bids, open('bids.pkl', 'w')) Is there any missing value in bids? True 1234missing_country = bids['country'].isnull().sum().sum()print 'No. of missing country: ', missing_countrynormal_country = bids['country'].notnull().sum().sum()print 'No. of normal country: ', normal_country No. of missing country: 5 No. of normal country: 7656329 12nan_rows = bids[bids.isnull().T.any().T]print nan_rows bid_id bidder_id auction \\ 1351177 1351177 f3ab8c9ecc0d021ebc81e89f20c8267bn812w jefix 2754184 2754184 88ef9cfdbec4c9e33f6c2e0b512e7a01dp2p2 cc5fs 2836631 2836631 29b8af2fea3881ef61911612372dac41vczqv jqx39 3125892 3125892 df20f216cbb0b0df5a7b2e94b16a7853iyw9g jqx39 5153748 5153748 5e05ec450e2dd64d7996a08bbbca4f126nzzk jqx39 merchandise device time country \\ 1351177 office equipment phone84 9767200789473684 NaN 2754184 mobile phone150 9633363947368421 NaN 2836631 jewelry phone72 9634034894736842 NaN 3125892 books and music phone106 9635755105263157 NaN 5153748 mobile phone267 9645270210526315 NaN ip url 1351177 80.211.119.111 g9pgdfci3yseml5 2754184 20.67.240.88 ctivbfq55rktail 2836631 149.210.107.205 vasstdc27m7nks3 3125892 26.23.62.59 ac9xlqtfg0cx5c5 5153748 145.7.194.40 0em0vg1f0zuxonw 1234# print bids[bids['bid_id']==1351177]nan_bidder = nan_rows['bidder_id'].values.tolist()# print nan_bidderprint bids[bids['bidder_id'].isin(nan_bidder)] bid_id bidder_id auction \\ 1351177 1351177 f3ab8c9ecc0d021ebc81e89f20c8267bn812w jefix 2754184 2754184 88ef9cfdbec4c9e33f6c2e0b512e7a01dp2p2 cc5fs 2836631 2836631 29b8af2fea3881ef61911612372dac41vczqv jqx39 3125892 3125892 df20f216cbb0b0df5a7b2e94b16a7853iyw9g jqx39 5153748 5153748 5e05ec450e2dd64d7996a08bbbca4f126nzzk jqx39 merchandise device time country \\ 1351177 office equipment phone84 9767200789473684 NaN 2754184 mobile phone150 9633363947368421 NaN 2836631 jewelry phone72 9634034894736842 NaN 3125892 books and music phone106 9635755105263157 NaN 5153748 mobile phone267 9645270210526315 NaN ip url 1351177 80.211.119.111 g9pgdfci3yseml5 2754184 20.67.240.88 ctivbfq55rktail 2836631 149.210.107.205 vasstdc27m7nks3 3125892 26.23.62.59 ac9xlqtfg0cx5c5 5153748 145.7.194.40 0em0vg1f0zuxonw 在对整体数据的部分用户缺失国家的按照各个用户分组后在时间上前向和后向填充后，仍然存在5个用户缺失了国家信息，结果发现这5个用户是仅有一次竞标行为，下面看看这5个用户还有什么特征 1234lst_nan_train = [i for i in nan_bidder if i in (train['bidder_id'].unique()).tolist()]lst_nan_test = [i for i in nan_bidder if i in (test['bidder_id'].unique()).tolist()]print 'No. of bidders 1 bid in train set: ',len(lst_nan_train)print 'No. of bidders 1 bid in test set: ',len(lst_nan_test) No. of bidders 1 bid in train set: 1 No. of bidders 1 bid in test set: 4 1print train[train['bidder_id']==lst_nan_train[0]]['outcome'] 546 0.0 Name: outcome, dtype: float64 由于这5个用户仅有一次竞标行为，而且其中1个用户来自训练集，4个来自测试集，由训练集用户的标记为人类，加上行为数太少，所以考虑对这5个用户的竞标行为数据予以舍弃，特别对测试集的4个用户后续操作类似之前对无竞标行为的用户，预测值填充最终模型的平均预测值 123bid_to_drop = nan_rows.index.values.tolist()# print bid_to_dropbids.drop(bids.index[bid_to_drop], inplace=True) 12print 'Is there any missing value in bids?',bids.isnull().any().any()pickle.dump(bids, open('bids.pkl', 'w')) Is there any missing value in bids? False 统计基本的计数特征根据前面的数据探索，由于数据集大部分由类别数据或者离散型数据构成，所以首先针对竞标行为数据按照竞标者分组统计其各项属性的数目，比如使用设备种类，参与竞标涉及国家，ip种类等等 123# group by bidder to do some statisticsbidders = bids.groupby('bidder_id')# pickle.dump(bids, open('bidders.pkl', 'w')) 1234567891011121314# print bidders['device'].count()def feature_count(group): dct_cnt = &#123;&#125; dct_cnt['devices_c'] = group['device'].unique().shape[0] dct_cnt['countries_c'] = group['country'].unique().shape[0] dct_cnt['ip_c'] = group['ip'].unique().shape[0] dct_cnt['url_c'] = group['url'].unique().shape[0] dct_cnt['auction_c'] = group['auction'].unique().shape[0] dct_cnt['auc_mean'] = np.mean(group['auction'].value_counts()) # bids_c/auction_c# dct_cnt['dev_mean'] = np.mean(group['device'].value_counts()) # bids_c/devices_c dct_cnt['merch_c'] = group['merchandise'].unique().shape[0] dct_cnt['bids_c'] = group.shape[0] dct_cnt = pd.Series(dct_cnt) return dct_cnt 1cnt_bidder = bidders.apply(feature_count) 123display(cnt_bidder.describe())# cnt_bidder.to_csv('cnt_bidder.csv')# print cnt_bidder[cnt_bidder['merch_c']==2] - auc_mean auction_c bids_c countries_c devices_c ip_c merch_c url_c count 6609.000000 6609.000000 6609.000000 6609.000000 6609.000000 6609.000000 6609.000000 6609.000000 mean 6.593493 57.850810 1158.470117 12.733848 73.492359 544.507187 1.000151 290.964140 std 30.009242 131.814053 9596.595169 22.556570 172.171106 3370.730666 0.012301 2225.912425 min 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 25% 1.000000 2.000000 3.000000 1.000000 2.000000 2.000000 1.000000 1.000000 50% 1.677419 10.000000 18.000000 3.000000 8.000000 12.000000 1.000000 5.000000 75% 4.142857 47.000000 187.000000 12.000000 57.000000 111.000000 1.000000 36.000000 max 1327.366667 1726.000000 515033.000000 178.000000 2618.000000 111918.000000 2.000000 81376.000000 特征相关性在对竞标行为数据按照用户分组后，对数据集中的每一个产品特征构建一个散布矩阵（scatter matrix），来看看各特征之间的相关性 12# 对于数据中的每一对特征构造一个散布矩阵pd.scatter_matrix(cnt_bidder, alpha = 0.3, figsize = (16,10), diagonal = 'kde'); 在针对竞标行为数据按照竞标用户进行分组基本统计后由上表可以看出，此时并未考虑时间戳的情形下，有以下基本结论： 由各项统计的最大值与中位值，75%值的比较可以看到除了商品类别一项，其他的几项多少都存在一些异常数值，或许可以作为异常行为进行观察 各特征的倾斜度很大，考虑对特征进行取对数的操作，并再次输出散布矩阵看看相关性。 商品类别计数这一特征的方差很小，而且从中位数乃至75%的统计来看，大多数用户仅对同一类别商品进行拍卖，而且因为前面数据探索部分发现商品类别本身适合作为类别数据，所以考虑分多个类别进行单独统计，而在计数特征中舍弃该特征。 1cnt_bidder.drop('merch_c', axis=1, inplace=True) 1cnt_bidder = np.log(cnt_bidder) 1pd.scatter_matrix(cnt_bidder, alpha = 0.3, figsize = (16,10), diagonal = 'kde'); 由上面的散布矩阵可以看到，个行为特征之间并没有表现出很强的相关性，虽然其中的ip计数和竞标计数，设备计数在进行对数操作处理之后表现出轻微的正相关性，但是由于是在做了对数操作之后才体现，而且从图中可以看到并非很强的相关性，所以保留这三个特征。 针对前述的异常行为，先从原train数据集中的机器人、人类中分别挑选几个样本进行追踪观察他们在按照bidders分组后的统计结果，对比看看 1cnt_bidder.to_csv('cnt_bidder.csv') 12345678# trace samples,first 2 bots, last 2 humenindices = ['9434778d2268f1fa2a8ede48c0cd05c097zey','aabc211b4cf4d29e4ac7e7e361371622pockb', 'd878560888b11447e73324a6e263fbd5iydo1','91a3c57b13234af24875c56fb7e2b2f4rb56a']# build a DataFrame for the choosed indicessamples = pd.DataFrame(cnt_bidder.loc[indices], columns = cnt_bidder.keys()).reset_index(drop = True)print \"Chosen samples of training dataset:(first 2 bots, last 2 humen)\"display(samples) Chosen samples of training dataset:(first 2 bots, last 2 humen) bidder_type auc_mean auction_c bids_c countries_c devices_c ip_c url_c robot1 3.190981 5.594711 8.785692 4.174387 6.011267 8.147578 7.557995 robot2 2.780432 4.844187 7.624619 2.639057 3.178054 5.880533 1.609438 human1 0.287682 1.098612 1.386294 1.098612 1.386294 1.386294 0.000000 human2 0.287682 2.890372 3.178054 1.791759 2.639057 2.995732 0.000000 使用seaborn来对上面四个例子的热力图进行可视化，看看percentile的情况 123456789101112import matplotlib.pyplot as pltimport seaborn as sns# look at percentile rankspcts = 100. * cnt_bidder.rank(axis=0, pct=True).loc[indices].round(decimals=3)print pcts# visualize percentiles with heatmapsns.heatmap(pcts, yticklabels=['robot 1', 'robot 2', 'human 1', 'human 2'], annot=True, linewidth=.1, vmax=99, fmt='.1f', cmap='YlGnBu')plt.title('Percentile ranks of\\nsamples\\' feature statistics')plt.xticks(rotation=45, ha='center'); auc_mean auction_c bids_c \\ bidder_id 9434778d2268f1fa2a8ede48c0cd05c097zey 94.9 94.6 97.0 aabc211b4cf4d29e4ac7e7e361371622pockb 92.4 87.2 92.3 d878560888b11447e73324a6e263fbd5iydo1 39.8 30.4 30.2 91a3c57b13234af24875c56fb7e2b2f4rb56a 39.8 60.2 53.0 countries_c devices_c ip_c url_c bidder_id 9434778d2268f1fa2a8ede48c0cd05c097zey 95.4 95.6 96.7 97.4 aabc211b4cf4d29e4ac7e7e361371622pockb 77.3 63.8 84.8 50.3 d878560888b11447e73324a6e263fbd5iydo1 48.8 38.7 34.2 13.4 91a3c57b13234af24875c56fb7e2b2f4rb56a 63.7 56.8 56.2 13.4 由上面的热力图对比可以看到，机器人的各项统计指标除去商品类别上的统计以外，均比人类用户要高，所以考虑据此设计基于基本统计指标规则的基准模型，其中最显著的特征差异应该是在auc_mean一项即用户在各个拍卖场的平均竞标次数，不妨先按照异常值处理的方法来找出上述基础统计中的异常情况 设计朴素分类器由于最终目的是从竞标者中寻找到机器人用户，而根据常识，机器人用户的各项竞标行为的操作应该比人类要频繁许多，所以可以从异常值检验的角度来设计朴素分类器，根据之前针对不同用户统计的基本特征计数情况，可以先针对每一个特征找出其中的疑似异常用户列表，最后整合各个特征生成的用户列表，认为超过多个特征异常的用户为机器人用户。 123456789101112# find the outliers for each featurelst_outlier = []for feature in cnt_bidder.keys(): # percentile 25th Q1 = np.percentile(cnt_bidder[feature], 25) # percentile 75th Q3 = np.percentile(cnt_bidder[feature], 75) step = 1.5 * (Q3 - Q1) # show outliers # print \"Data points considered outliers for the feature '&#123;&#125;':\".format(feature) display(cnt_bidder[~((cnt_bidder[feature] &gt;= Q1 - step) &amp; (cnt_bidder[feature] &lt;= Q3 + step))]) lst_outlier += cnt_bidder[~((cnt_bidder[feature] &gt;= Q1 - step) &amp; (cnt_bidder[feature] &lt;= Q3 + step))].index.values.tolist() 再找到各种特征的所有可能作为‘异常值’的用户id之后，可以对其做一个基本统计，进一步找出其中超过某几个特征值均异常的用户，经过测试，考虑到原始train集合里bots用户不到5%，所以最终确定以至少存在1个特征值异常的用户作为异常用户的一个假设，由此与test集合里的用户进行判断，可以得到一个用户子集，将这部分用户判定为朴素分类器的作弊用户判定结果。 12345# print len(lst_outlier)from collections import Counterfreq_outlier = dict(Counter(lst_outlier))perhaps_outlier = [i for i in freq_outlier if freq_outlier[i] &gt;= 1]print len(perhaps_outlier) 214 123# basic_pred = test[test['bidder_id'].isin(perhaps_outlier)]['bidder_id'].tolist()train_pred = train[train['bidder_id'].isin(perhaps_outlier)]['bidder_id'].tolist()print len(train_pred) 76 设计评价指标根据前面数据探索知本实验中的数据集的正负例比例约为19:1，有些失衡，所以考虑使用auc这种不受正负例比例影响的评价指标作为衡量标准，现针对所涉及的朴素分类器在原始训练集上的表现得到一个基准得分 1234567from sklearn.metrics import roc_auc_scorey_true = train['outcome']naive_pred = pd.DataFrame(columns=['bidder_id', 'prediction'])naive_pred['bidder_id'] = train['bidder_id']naive_pred['prediction'] = np.where(naive_pred['bidder_id'].isin(train_pred), 1.0, 0.0)basic_pred = naive_pred['prediction']print roc_auc_score(y_true, basic_pred) 0.54661464952 在经过上述对基本计数特征的统计之后，目前尚未针对非类别特征：时间戳进行处理，而在之前的数据探索过程中，针对商品类别和国家这两个类别属性，可以将原始的单一特征转换为多个特征分别统计，此外，在上述分析过程中，我们发现针对用户分组可以进一步对于拍卖场进行分组统计。 对时间戳进行处理 针对商品类别、国家转换为多个类别分别进行统计 按照用户-拍卖场进行分组进一步统计 对时间戳进行处理主要是分析各个竞标行为的时间间隔，即统计竞标行为表中在同一拍卖场的各个用户之间的竞标行为间隔 然后针对每个用户对其他用户的时间间隔计算 时间间隔均值 时间间隔最大值 时间间隔最小值 1234567891011121314151617181920212223from collections import defaultdictdef generate_timediff(): bids_grouped = bids.groupby('auction') bds = defaultdict(list) last_row = None for bids_auc in bids_grouped: for i, row in bids_auc[1].iterrows(): if last_row is None: last_row = row continue time_difference = row['time'] - last_row['time'] bds[row['bidder_id']].append(time_difference) last_row = row df = [] for key in bds.keys(): df.append(&#123;'bidder_id': key, 'mean': np.mean(bds[key]), 'min': np.min(bds[key]), 'max': np.max(bds[key])&#125;) pd.DataFrame(df).to_csv('tdiff.csv', index=False) 1generate_timediff() 后续内容请移步使用机器学习识别出拍卖场中作弊的机器人用户(二)","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"平坦世界","date":"2017-05-29T08:30:23.000Z","path":"2017/05/29/世界是平的/","text":"这次给大家推荐一本书——“The world is flat”（《世界是平的》），刚刚看完了一遍，感觉这本书的视角很广阔，让我看到自己受限于地理或者意识因素所看不到的一些其实就实实在在发生在身边的变化，私以为，作为即将毕业的大学生而言，是很有必要一读的。 【这本书面世其实快10年了，但是书中所述的一些变化我们或经历过，或忽略掉了，所以即使放在现在读，也是很有前瞻性的。特别由于作者本身是记者，写作风格还是颇为轻松地，所以，读起来感觉像是跟着作者开着上帝视角实现了所谓“世界那么大，我想去看看”的成就一般。】 此书整体大概是由世界变平坦的种种例证、现象到作者所认为使世界变平坦的一些因素再到各个“成员”大至国家公司、小至每一个个体在这平坦世界里的角色和相互影响，最后是平坦世界所带来的或即将造成的可能负面影响和威胁来逐层叙述的。我想先就宏观角度简单记叙下书中的一些观点，然后谈一下书中所说而我也感觉到的身边的一些变化，最后就自己而言随意谈一下想法。 就宏观而言，书中让我印象最深的是全球化的重中体现，首先是全球合作的一些案例，这个曾经在《互联网时代》里也看到过，比如现在的波音飞机的组装以及各零件的制造是分散到世界各个国家各个地区去做的，然后又井然有序的由各个地方汇聚到一个点组装成型，书中还举得一个例子是沃尔玛的生产线，和前者类似，也是一种像分布式的结构，这种做法极大地提高了效率并降低了成本，而其中最让我目眩神迷的是这么个巨大的整体居然能够这么一致的展开协同工作；另外一点是关于离岸外包，这里就不得不提到一个国家是印度，或者说印度的城市班加罗尔，看了这本书我感觉像亲身去看一下，所谓外包，就是将本国家的一些相对而言中低端而要耗费大量人力的基础工作交给其他国家去做，这里你也许会想到所谓的”Made in China”，诚然，中国也是外包的一个很好的例子，换做以前我会觉得嗤之以鼻，感觉我们国家就老是像给美国之类的国家打工一样，其实，分析一下，就目前而言，这个是双赢的，美国那边自不用说，花一分美国工人的钱在中国或者印度雇佣到6~8个同样水平的技术工人，极大地降低了成本，而且这里面不得不说有个很有意思的东西是——时差，因为有些工作在美国的白天又美国人做，晚上就交给印度人（印度正在白天）做，极大地提升了效率，这个我在电影《贫民窟的百万富翁》里看到过的一个场景就是印度人通过远程监控摄像头替美国人监控停车场让我印象深刻，那么对于印度和中国这些国家呢，好处在哪？其一就是增多了工作岗位，就印度而言的话，由于现在印度培养了大量的理工学生，而毕业后以前只能去国外谋出路，否则在国内一般只能成为出租车司机，而经过外包，他们可以在本土就进行软件开发的工作，虽然工资不如美国人，但是相对于自己本身而言，他们的生活已经得到不小的提升了，另外，通过这种形式，印度和中国还可以学习一些国外的技术，甚至是抢占国外的市场，书中后面有个例子是中国生产的埃及传统的灯几乎快要垄断埃及的市场，因为在技术上有所突破，而且价格也低廉。 当然，以上只是从书中看到的知识，也许我以后还需要实地考察一番看看实际情况是否真如作者所述，不过，对于我们国家，我还是希望抱有乐观的态度去看他的发展，相信他会越变越好。 就身边的所见所闻而言，我印象比较深的是一个 距离 的问题，和书中作者一次下飞机乘出租车的经历一般，在乘车那段时间里，司机一直在通过蓝牙耳机聊天，车上开着导航，播放着电影，而作者在自己的笔记本上整理文章以及收发e-mail，用作者的话说，在那一个小时里他们同时做了很多事，却几乎没有交流，作者甚至猜想，也许司机正和远在另一个国度的父母通话呢。这大概就是目前发生在我们身边的一个尴尬境地了，一方面，技术的发展拉近了我们的距离，而另一面却又使我们的距离变得遥远，它拉近了我们和处在不同空间的亲人之间的距离，却又在此时此地的就在你眼前的我面前树起了一道屏障。我想起了高中语文老师发的一篇文章里对动车、高铁上的年轻人乘客们的描述，确确实实的感受到这一真切的现象，大家一上车就戴上耳机，或插入ipad，或插入iphone，而彼此之间却没有交流，这和绿皮火车上的情景完全不同，也是值得我们反思的一件事。 最后我想简单谈一下自己的一些想法。首先关于教育和竞争方面，这里还得扯下印度，书中说道，班加罗尔的接线员晚上为美国（白天）的乘客们进行咨询和失物找回工作，晚上还会自己学习一些知识，攻读一些学位什么的，顿时感到一种压迫感。从前经常听到政治老师说美国家长告诫自己的孩子说快吃饭，不然中国和印度的孩子就把你的饭给吃了或者快努力学习，中国或印度的孩子快把你的饭碗抢走了之类的。而现在是大家都站在几乎同一个平台上竞争，前面我们需要追赶美国，而同时，印度的青年们却也在旁虎视眈眈。而身处计算机专业这一日新月异，竞争更加残酷的环境下，我一面感到威胁，一面感到兴奋，兴奋的是，试想一下你即将站在一个大舞台上，和来自不同城市乃至不同国家的人角逐，而威胁在于，你们本身的水平是不一样的，就我而言，在我将要踏上这个舞台的那一天，我不仅会和同班同学竞争，稍远一点还有来自全国各大高校的强者，再远一点还有世界其他角落的高手出没，而要想不被碾成炮灰，或者说不被这一变平的趋势所冲倒的话，我就得不断地去吸收和学习新的技术、新的知识，还要进一步强化自己的全局意识，做到让自己所做的事无可替代，也就不会被”外包”掉。另外一点就是之前实习时听到负责人说现在的联系方式太多了，又是电话、又是微信、QQ的，反而造成了联系上的障碍，以前我们只有电话时一般就通过电话联系，而现在常常是不同人有不同的习惯，你得把这些一股脑全开着，否则搞不好会错过重要讯息，我常常在想，世界是多元化的，但本意是方便我们交流的工具最后却慢慢成了束缚我们的枷锁，从这个角度来看，人类到底是进步了，还是退化了？","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"Economy","slug":"Economy","permalink":"http://yoursite.com/tags/Economy/"}]},{"title":"面壁者，我是你的破壁人","date":"2017-05-29T08:28:53.000Z","path":"2017/05/29/三体/","text":"对一个人的评判若不结合他所处的时代都是不公允的。 终于看完了《三体》全集，这部科幻小说给人的感觉真像作者自己命名的“地球往事”，像一本历史书，以宇宙为坐标，以光年为刻度，读罢，借用一句话“科幻小说虽然尽是对于未来的想象，但我们探索的一直是人的内心”。 情节上我就不剧透太多了，我想谈谈《三体》里的几个角色。 首先是罗辑，面壁者、执剑人、守墓人是他不同时期的身份，他是一个充满传奇色彩的人，用他自己的话是一个及时行乐的人，对其他的人、其他的事都不怎么上心的人。但是我看到的是他受人胁迫而终于接受面壁者身份时的无奈和挣扎；在冬眠复苏时被人当做笑话时的淡然、洒脱，再一次被推上历史转折点时的坚韧以及作为执剑人半个多世纪面壁时的狠厉。我得承认全书给我印象最深的是《黑暗森林》最高潮的部分，当罗辑站在自己挖掘的坟墓边对着虚空中的智子亮出最后底牌的那一刻；我真的有一种作为人类终于得救了、终于又有了希望的感觉。命运喜欢捉弄罗辑，在他吊儿郎当、无所事事时对他施以面壁者的诅咒，在他拯救了全人类时却让人类对他报以敌视；但罗辑不在乎这些，他只在乎自己所爱的那个女孩的幸福。 然后是《死神永生》里的女主角程心。之前有看一个《x分钟读完三体》的短片，当时发现弹幕上对于程心大多数是一片骂声，随处可见诸如“程圣母毁灭世界”的字眼。不得不说在看《死神永生》的中后段我也一直对于程心是恨得咬牙切齿的，实话说，我讨厌她的不作为和自以为是爱与善的化身所做的所有决定，讨厌她毁灭了地球、害人类灭亡，但后来一想，我其实是开了上帝视角在看这个角色，其实回过头来看，程心是当前普通人类的典型，甚至，是大众中向善的群体的典型，她一直秉持着自己的责任，逆来顺受；她是勇敢的，敢于牺牲自己，但是，她的能力是不够的。整部小说看下来程心大部分时间处于冬眠状态，而经常是其他人帮她做足了准备，然后突然一下把她推到全人类命运决策的位置，可是却没有想过她准备好了没有，结果她只能依据自己当前的认识做出最佳选择，然后成了公众的替罪羊，其实换位思考，我们估计在那个节点做的更差；不信你把程心放到不同的时间段看公众对于她作为执剑人的那几分钟的态度的不停反复就可以看出来了，大众只是需要一个为自己顶包的人，然后好像自己就可以置身事外了。 如果说程心更多的依靠感性来做判断，那么就不得不提到维德了，这个极端理性到冷酷的男人。最开始他对于程心的捉弄确实让我很不爽，从发射云天明的大脑一直到后面他不择手段的想要暗杀程心竞争执剑人。但读到后面你会发现维德其实是个表里如一的人，对于自己认定的事他能坚持到底，不择手段的坚持到底。 所以不存在这么一个人，把他放到任何一个时空他的判断都是对的，或者说大众对于个体的判断其实也不能代表什么，一个人只需坚持自己所相信的，在乎自己所爱的，到最后一刻，就够了。 各位面壁者们，准备好做自己的破壁人了么？","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"Novel","slug":"Novel","permalink":"http://yoursite.com/tags/Novel/"}]},{"title":"平凡的世界，不平凡的凡人","date":"2017-05-29T08:27:42.000Z","path":"2017/05/29/平凡的世界/","text":"一直想读这套书，在临近毕业时还专门从朋友那收藏了一套，借着在MEB的机会终于是忙里偷闲的看完了，虽意犹未尽，却也感到一种平和和满足。 如果要用一句话概括，《平凡的世界》写的是爱情和面包。 书中描述的爱情种类比较多，我就简单挑给我印象最深的三个来说说吧。首先是润叶和少安的爱情，那句诗里写的“郎骑竹马来，绕床弄青梅”大抵描述的就是这么一种爱情。少安和润叶是两小无猜的一对童年玩伴，随着两家各自发展出的家庭背景的差异却也慢慢在双方之间产生了巨大的鸿沟，一个留校任教成了城里人，另一个却回家务农，给家里分担压力。诚然，你可以说是少安主观上的懦弱导致了最终两人的分离和三个家庭的阴影和痛苦，但是个人认为，在那么一种背景下，就一个男人而言是不得不考虑这些东西的。书中虽说如果两人身份对调也许就没什么问题，但我觉得即使两人身份对调而彼此相爱也还是要考虑双方的家庭背景，这里我倒不是在推崇门当户对的老观念，只是客观的分析一下，毕竟，我一直认为，爱情是建立在一定的物质基础之上，不是简单的彼此相爱，毕竟，爱情可能产生在一瞬间，但是维系这份感情却需要长期的努力，也就需要一定的物质作为基础，而若双方各自条件不对等，时间长了或多或少会产生并积累一些问题。由此看来，古人所推崇的门当户对确有一定道理。 当然，我不否认灰姑娘和王子的童话故事，但是即使在灰姑娘的故事里，灰姑娘也是有仙子为她做后盾的而且她本身有相对应的素质作为支撑。 扯远了，其实书中倒也有这么一对，就是少平和晓霞。我对这一对的定义是充满理想和浪漫主义色彩的情侣，两人由最初的共同爱好或说理想发展出深厚的友谊，继而在不同的人生道路上产生的共鸣引导出爱情，说实话，看到这里我已经觉得有点过于理想化了，到后来晓霞拒绝高富帅依然坚定地爱着少平时，我也深深被打动，从心理上我是希望看到这一对能有美好的结局的，我原希望会是少平从煤炭场最后走出来完成逆袭然后迎娶白富美的励志故事（原谅程序员的屌丝气息），不过感觉依作者的套路势必会再给这对有情人制作一些波折，可是，万万没想到，情节发展居然是晓霞的牺牲，当时感觉很难接受这一事实，毕竟这朴实的书中的一朵充满浪漫主义色彩的绚丽花朵竟以这种方式枯萎。 最后一对就是润叶和向前，很难用爱情定义这一对，因为故事前段一直是向前的单恋，即使两人的结合也是润叶的自我牺牲和自我放逐，结果是长期的互相折磨到最后，然后转折点在向前的事故所带来的残疾，润叶因为内疚而突然回头，两人复合（个人觉得此处作者不够深入），然后两人开始真正的婚姻生活，不过也很难说是爱情，连文中也提及此时的润叶更像是一个虔诚的修女，给予向前的除了妻子的责任另外确是一种变相的母性关怀，姑且称之为——怜悯或者同情吧，其实我是很喜欢润叶这么个姑娘的，敢爱敢恨，在和少安的爱情中表现出勇敢，却又太懂事，这样矛盾的性格造成她最终的屈服，与其说她是嫁给了一个自己不爱的男人，倒不如说她是嫁给了生活。但我又不能怪罪向前，爱情这东西说不清道不明的，很多时候其实就是，我喜欢你然而我并不能理解你为何不喜欢我，其结果至少有一人受伤。 以上是对书中描述的几种爱情的粗浅理解，当然，作为一个纯理论家，请相信，我说的每一句话都是谎话。 还是来谈谈另一条线——面包吧。 全书主要描述了双水村这么个平凡的小天地里一个又一个像你像他像那路边野花的平凡人的平凡而又不平凡的生活。总体来说，大的背景是变化莫测的，从文革末的动乱年代到改革开放的小康生活，立足在这么一个大环境下，每个人的命运都不能孤立的来看，刚开始看这本书的时候我常常随着故事的情节发展给故事中的人下定义是好是坏是对是错，后来随着作者时不时的客观分析才发现自己的浅薄，其实就像我们所生活中的生活，它不像我们做的卷子上的题目，没有什么绝对的对错，每个人只是在给他演出的那么一段镜头里或优雅谢幕或落荒而逃，导演却是生活。也许我仍不能从简单的评判一个人是好人坏人，他做的一件事是对的还是错的这么一种观念里走出来，但是从这本书里我看到了生活这辆无缰马车上的形形色色人物的身不由己，同时，我也看到他们的奋斗，一次一次被生活喊cut却又一次一次带着微笑或者含着泪水伤痕累累的跑龙套，在这场戏里，没有主角，或者说每个人都是主角，轮番上场。 每一个人物都有其鲜明特色又有其局限性，之前有一朋友说他觉得书中把任务刻画得太脸谱化了，我觉得是有这么一点，但是大体而言路遥对人物的刻画还是很鲜明，很丰满而有血有肉的。我欣赏少安在物质生活追求上的不屈不挠精神，却又为他有时的偏执扼腕，对于他在爱情上的胆怯以及一意孤行而叹息；我喜欢少平这么一个脚踏实地的为生活战斗，为自己为他人着想的‘精神贵族’。他太可爱了，看这本书真的就是看着这个少年的成长，看着他咀嚼生活，消化生活，不过我希望他能更勇敢，有时能多为自己考虑一点，勇敢的追逐自己的幸福；至于晓霞，自不用说，感觉书中几乎刻画成女神一般的存在了，可惜也由于她的冒险精神和舍己为人的精神而香消玉殒；润叶，真是一个让人忍不住心疼却又忍不住尊敬的女性，她是那么为大家着想，甚至于可以牺牲自己；还有孙玉厚，这个伟大的老父亲，虽然书中没太多专门的笔墨，但是仍可以看见少安、少平甚至兰花从他身上传承的坚韧的基因，也许从早期发狠供玉亭念书也能窥见一二，另外，这位老人在儿子追求自己目标时默默支持，在孩子们遭受不幸时却总是挺身而出，实在令人感动；其他还有用情专一的向前，逛鬼王满银，懂事的兰香等等，像你我身边的每一个人一样在这个平凡的世界里献上了这精彩的演出，然后帷幕缓缓落下，他们又消失在人群中，从此你我看到的身边的每一个人都似曾相识。 以上大概就是这粗略的第一次阅读《平凡的世界》的一点不吐不快的想法，虽然书有一点点小瑕疵比如各个主角的结局实在难以让我这么一个习惯了happy ending的人满意，比如中间外星人的情节有点乱入的感觉，不过还是强力推荐这本书，怎么说呢，在看这本书的过程中，我常常有一种踏实感，活着的踏实感，在我合上书页，回想起书中一些情节时情不自禁的感受到生活，从某种程度上，它减轻了我若有若无的虚无感。","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"Novel","slug":"Novel","permalink":"http://yoursite.com/tags/Novel/"}]},{"title":"韶华倾负，难舍皮囊","date":"2017-05-29T08:22:21.000Z","path":"2017/05/29/韶华倾负，难舍皮囊/","text":"利用在动车上的四个小时看完了《皮囊》（除去中间15分钟和邻座美女搭讪的时间），感觉是很不错的一本书，不同的章节写的不同的故事，如果你有时间不妨通篇阅读一下，如果没有时间不妨听我说一说。 个人觉得，前五章的主题关于生离，关于死别。 第一章和书同名是为《皮囊》，讲述的是99岁的阿太。 我们的生命本来多轻盈，都是被这肉体和各种欲望的污浊给拖住。阿太，我记住了。“肉体是拿来用的，不是拿来伺候的。” 没文化的神婆阿太穷极一生都在利用自己的那副皮囊，甚至要求周围的人也学会去利用好这幅皮囊，所以她把年幼的孩子扔进海里让他学游泳，所以她即使白发人送黑发人也保持一副让人不解的嘲弄表情，或许在她看来，失掉这皮囊，灵魂最终的归宿反而是自由吧。 但这并不意味着阿太是那种不在乎生死的人，相反，我觉得她是站在更高的角度看待生死这个问题的人，而这也许隐隐约约影响着身边的人。所以作者后来写下这么一句话： 从小我就喜欢闻泥土的味道，也因此其实从小我不怕死，一直觉得死是回家，是入土。我反而觉得生才是问题，人学会站立，是任性地想脱离这土地，因此不断向上攀爬，不断抓取任何理由——欲望、理想、追求。然而，我们终究需要脚踏着黄土。在我看来，生是更激烈的索取，或许太激烈的生活本身就是一种任性。 然后是第二章——《母亲的房子》写的是母亲，房子还有爱情。 每个人都会有自己的执着，而那种执着最后就物化成某种具体的事物，然后进一步，有的化成妥协后的一道疤痕，有的成为穷极一生的执念。而文中的母亲显然是后者，而她所执着的是那所房子，是年轻时的父亲曾允诺的房子，以一种执着的近乎任性的方式。从最开始的一修再修，这所母亲执意要修建的房子联系着一家人的生活和命运，从拮据的生活到后来周围人的不解最后甚至家人的埋怨，母亲一度支撑不下去却又不甘心，最难过的日子里甚至一家人想要一起求死，直到后来身为“不合格的一家之主”的我终于理解了母亲： 事实上，知道母亲坚持要建好这房子的那一刻。我才明白过来，前两次建房，为的不是她或者我的脸面，而是父亲的脸面——她想让父亲发起的这个家庭看上去是那么健全和完整。 这是母亲从老没表达过，也不可能说出口的爱情。 另外，关于母亲的故事在《我的神明朋友》里会继续讲述，你会看到更多母亲身上的坚韧。 然后《残疾》讲述的是父亲中风后的家里的种种艰辛和一家人和生活的抗争。 不同于以前语文课本里父亲那种高大、沉默的形象，《皮囊》里的父亲首先是一个中风而偏瘫了的父亲，而后是一个曾经混黑社会呼风唤雨的混混头子，中风前后的心理落差造成了父亲接下来的一系列变化，他从假装坚强，希望靠对时间的严苛要求和每天的活动恢复到以前的健壮身体，于是一家人默契的相互演着戏，却也过了一段幸福的时间，直到被一场意料之中的台风摧毁。 虽然知道根本不是台风的错。那结局是注定的，生活中很多事情，该来的会来，不以这个形式，就会以那样的形式。但把事情简单归咎于我们无能为力的某个点，会让我们的内心可以稍微自我安慰一下，所以，我至今仍愿意诅咒那次台风。 那场台风刮倒的不仅仅是父亲，更打碎了父亲伪装的坚强和一家人匆忙写就的生活的剧本。自此，父亲进入了第二个状态，先是放弃了坚强，呈现出一副自暴自弃的样子期盼着死亡。而后又进入下一阶段，甚至于舍弃了父亲的身份，像个不懂事的孩子一样撒娇、任性，却不再期盼死亡。 但是父亲的故事并没有完，在《重症病房里的圣诞节》，作者以在重症病房里的看护家属的视角半写实半轻描淡写地描述了退去浮华身份后的种种生离死别。 疾病在不同的地方找到了他们，即使他们当时身处不同的生活，但疾病一眼看出他们共同的地方，统一把他们赶到这么一个地方圈养。 在这一章里，作者刻画了几个不同的人物，有乐观的病人，有刻意显得刻薄的医护人员，有早熟的同龄孩子。在医院这么一个特殊的小世界里，赤裸的生长着。有几个事件的刻画让我印象深刻，比如‘我’对于医院里电梯的描写，对于走过一间间病房核查之前的病人是否还在；比如和同龄的孩子交流，发现相同点，大家拥有相同的眼睛，那是经历过生命消逝后才看得到的眼睛，所以会被一眼看透，而无法交到同龄的朋友，因为他们只有一次也只能有一次痛彻心扉的谈话；比如乐观的病友最后还是被夺走生命后‘我’心态上的变化，印象最深的是那个为父亲在圣诞节违禁燃放烟花的年轻人，只因他父亲要进行手术了，然后他被三个保安团团围住…… 比如，在帮父亲换输液瓶时，会发觉他手上密密麻麻的针孔，找不到哪一寸可以用来插针；比如医生会时常拿着两种药让我选择，这个是进口的贵点的，这个是国产的便宜的，你要哪种？我问了问进口的价钱，想了很久。“国产的会有副作用吗？”“会，吃完后会有疼痛，进口的就不会。”我算了算剩下的钱和可能要住院的时间，“还是国产的吧。” 然后看着父亲疼痛了一个晚上，怎么都睡不着。 在这里，你一不小心留出空当，就会被悲伤占领——这是疾病最廉价、最恼人的雇佣兵。 这种笔触没在重症病房待过的人是写不出来的，那种切实的压抑感和空闲下来被对未来的恐惧和迷茫逼到绝路的走投无路感。 接下来的几章个人觉得是关于青春和梦想的。 首先《张美丽》这一章从这么一个略显俗气的名字开始，从迷信的桃色传说开始记叙了传说中自杀的殉情张美丽，而后记叙了现实中被夹在理想和世俗之间压垮的张美丽，同样是一副让荷尔蒙萌动的青春期少年燥起来的皮囊，演绎了不同的故事。这故事一面让我想起了《搜索》这部电影里舆论的可怕，人们对于未知的事物的不理解演化成的嫉妒和驱逐和以讹传讹造成的悲剧上演。另一面看到的是被遗忘的先驱们，为了自己的理想而头破血流，然后若干年后在残次不齐的字里行间被遗忘或变成谈资。只是人们一同忘掉的，是现在所呈现在眼前的似曾相识。 接下来《阿小和阿小》、《天才文展》记叙的算是童年和成长吧。 关于城市，那是不在城市长大的孩子们小时候的天堂，是他们在电视屏幕上看到的模样，而两个阿小，一个土生土长在小城镇；另一个为即将开始的大城市生活过度而寄居在小城镇。同样的名字，也沉浸在同样的幻想里，只是他们误读了城市，他们以为的城市就真的是电视屏幕上的那样，数不过来的高楼，四六分的香港发饰，衬衫，洁白的牙齿……于是，他们开始注重外表上的模仿和行为上的接近，在我看来，这正是一种青春期的迷茫，对于寻找自我定位时的种种探索，追求那些很酷很与众不同的感觉。但是结局却不尽如意： 大部分人都困倦到睡着了——他们都是一早七点准时在家门口等着这车到市区，他们出发前各自化妆、精心穿着，等着到这城市的各个角落，扮演起维修工、洗碗工、电器行销售、美发店小弟……时间一到，又仓皇地一路小跑赶这趟车，搭一两个小时回所谓的家，准备第二天的演出。 他们都是这城市的组成部分。而这城市，曾经是我们在小镇以为的，最美的天堂。他们是我们曾经认为的，活在天堂里的人。 而《天才文展》里的文展则代表着一种有远大抱负却囿于家庭环境的早熟的年轻人，也许我们都碰到过像这样的人，以一种居高临下的姿态看着你，对于自我有着严苛的要求，希望通过一项项计划通来向别人证明自己，来狠狠地扇曾经看不起自己的人一耳光。 我才明白，那封信里，我向文展说的“小时候的玩伴真该一起聚聚了”，真是个天真的提议。每个人都已经过上不同的生活，不同的生活让许多人在这个时空里没法相处在共同的状态中，除非等彼此都老了，年迈再次抹去其他，构成我们每个人最重要的标志，或许那时候的聚会才能成真。 然而结局却是悲剧的，‘我’最终过上了文展所希冀的生活而遭到文展的嫉妒和排斥，令人吃惊的是‘我’却也产生了一些负罪感。就像电视剧里经常碰到的桥段，“如果不是因为当初XXX，你现在所有的一切本该是我的。”其实这何尝不是自我欺骗呢，就算自我假设的前提条件不成立，结局却不见得会不一样，失败者把失败原因归结到自己以外的事物上注定还是会重蹈覆辙。 然后是《厚朴》描述的这么一个可爱的男生，英文名又是hope，伴着激情和叛逆的形象出场，不得不说让我想起身边一些充满朝气和正能量的人，平时还是很羡慕他们的，不过厚朴是不同的，开始我也很希望看看他所谓的“务虚”能走出一片天地来，俗话说就是理想主义者，因为我感觉自己更类似于作者。不过可惜的是最后结局让人痛心，像作者所形容的 不清楚真实的标准时，越用力就越让人觉得可笑。 不合时宜的东西，如果自己虚弱，终究会成为人们嘲笑的对象，但有力量了，或坚持久了，或许反而能成为众人追捧的魅力和个性——让我修正自己想法，产生这个判断的，是厚朴。 他以为自己做着摧毁一切规矩的事情，但其实一直活在规矩里。我以为自己战战兢兢地以活在规矩里为生活方式，但其实却对规矩有着将其彻底摧毁的欲望。 所以厚朴所表现出来的其实是一种沉浸在自己构造的幻想世界里而最终难以回到现实，他不愿接受自己失败了的现实，只是表现的叛逆。但话说回来我是希望能看到一种理想主义者最终实现理想的故事，我相信如果有那么一定会是精彩的。 说到这里我甚至有一种奇怪的感觉，是否厚朴其实就是作者的另一面，只是最终发现自己其实是在规则里画着圈而随着屈服而死掉的那部分。 最后几章主要简单谈了些关于城市、理想之类的话题，印象较之之前的几章倒是没那么深了，截取一段如下 只是我觉得城市不好。特别是中国的城市不好。厦门和中国大部分城市的建设都有个基础——人家国外的城市是怎么样的，以及人们该怎么被组织的，然后再依据这样的标准建设。中国近代的城市不是长出来的，不是培植出来的，不是催生出来的，而是一种安排。因为初期必然要混乱，所以中国的城市也表现出强大的秩序意识，人要干吗，路要怎么样。生长在这样环境里的人，除了维护秩序或者反抗秩序，似乎也难接受第二层次的思维了。 回头来看，几篇文章倒确实很契合《皮囊》这么一个主题，就像开头所述的，灵魂离不开皮囊，无论你如何讨厌自己的这幅皮囊，你的灵魂也得附庸其下。皮囊往小的说就是阿太所谓的驱壳和父亲半瘫的残疾，往大了说是一个人所处的位置，他背后的故事，就像母亲的那所房子，阿小的香港梦，厚朴的架子鼓。过去我常常看不懂电视剧里有些人的无可奈何，一厢情愿的认为这么容易的事换做是我直接三下五除二就解决了，实际情况却是不要以好坏善恶对错来划分人群，你看到的只是他当前展现给你的，你看不到的是他这一举动后面的挣扎，人是不能孤立和剥离来看的，当他站在你面前时，你得看到他的背景。 我常对朋友说，理解是对他人最大的善举。当你坐在一个人面前，听他开口说话，看得到各种复杂、精密的境况和命运，如何最终雕刻出这样的性格、思想、做法、长相，这才是理解。而有了这样的眼睛，你才算真正“看见”那个人，也才会发觉，这世界最美的风景，是一个个活出各自模样和体系的人。 如果每个人都是一段程序的话，那么我们在和不同的人交往的过程中就有意无意的修改了他们的代码，从此也许他们身上就带着你的痕迹；而身边亲近的人，更像是写进你基因里的代码段，在某个特定的时间里被激活，让你似曾相识，让你视线模糊。 无论如何，请带着这副皮囊好好生活。","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"Novel","slug":"Novel","permalink":"http://yoursite.com/tags/Novel/"}]}]