[{"title":"统计学习方法笔记(七) —— 支持向量机","date":"2017-07-12T09:11:48.000Z","path":"2017/07/12/svm/","text":"书中例题7.1的凸二次优化的Python解法，参照凸优化 · 如何在 Python 中利用 CVXOPT 求解二次规划问题,针对该题可见我的github","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"SVM","slug":"SVM","permalink":"http://yoursite.com/tags/SVM/"}]},{"title":"最优化算法小记","date":"2017-07-11T06:14:02.000Z","path":"2017/07/11/optimization/","text":"最优化： 构造一个合适的目标函数（策略），使得这个目标函数取到极值的解就是你所要求的东西； 找到一个能让这个目标函数取到极值的方法（算法） 这里主要讨论几种常用的求解最优化问题的算法 梯度下降法（最速下降法）梯度 在微积分里面，对多元函数的参数求偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度;从几何意义上讲，就是函数变化增加最快的地方。 梯度下降法 梯度下降法是求解无约束最优化问题的一种常用的一阶优化方法，是一种迭代算法，每一步需要求解目标函数的梯度向量。 缘由 对于无约束优化问题$min_xf(x)$，其中$f(x)$为连续可微函数，若能构造一个序列$x^0,x^1,…$,满足$$f(x^{t+1})&lt;f(x^t),t=0,1,2,..$$，则不断执行该过程即可收敛到局部极小点； 而为了达到这一目的，根据泰勒展开式有$$f(x+\\Delta x)\\approx f(x)+\\Delta x^T\\nabla f(x)$$,而为了满足$f(x+\\Delta x)&lt;f(x)$，可选择$$\\Delta x=-\\gamma \\nabla f(x)$$即负梯度,其中步长（即学习率）$\\gamma$是一个小常数（每步的步长可以不同）； 对于步长，可通过一维检索确定，即$$f(x^{(k)}-\\gamma \\nabla f(x))=min_{\\gamma\\geq 0}f(x^{(k)}-\\gamma \\nabla f(x))$$,但是这种方法会增加额外的计算开销，不过如果固定步长，也有可能导致糟糕的收敛，所以需要根据实际情况选择处理方法。 算法 输入：目标函数$f(x)$,梯度函数$g(x)=\\nabla f(x)$，计算精度$\\varepsilon$ 输出：$f(x)$的极小点$x^*$ (1)取初始值$x^{(0)}\\in R^n$,置$k=0$ (2)计算$f(x^{(k)})$ (3)计算梯度$g_k=g(x^{(k)})$,当$||g_k||&lt;\\varepsilon$时，停止迭代，令极小点$x^*=x^{(k)}$,否则令负梯度$p_k=-g(x^{(k)})$,求步长$\\lambda_k$，使$$f(x^{(k)}+\\lambda_kp_k)=min_{\\lambda\\geq 0}f(x^{(k)}+\\lambda p_k)$$ (4)迭代更新，置$x^{(k+1)}=x^{(k)}+\\lambda_kp_k$,计算$f(x^{(k+1)})$,当$||f(x^{(k+1)})-f(x^{(k)})||&lt;\\varepsilon$或$||x^{(k+1)}-x^{(k)}||&lt;\\varepsilon$时，停止迭代，令$x^*=x^{(k)}$ (5)否则,置$k=k+1$,转第(3)步 Notes: 一般情况下，步长可以取一个较小的值，而不必每次都计算当前的最佳步长 另外，该算法中停止迭代的条件出现了两次，第一次是刚计算出梯度时就与计算精度进行比较，第二次是分别把目标函数值以及两次迭代的输入$x$的差值与计算精度进行了比较，但其他的一些参考资料似乎只是检查了梯度下降的距离与计算精度的关系，即$||f(x^{(k+1)})-f(x^{(k)})||&lt;\\varepsilon$，比如博客 特点 梯度下降法靠近极小值时速度减慢（所以不必每次迭代刻意减小步长） 对某些目标函数可能会“之字型”地下降 若目标函数$f(x)$满足一些条件，则通过选取适当的步长能确保通过梯度下降收敛到局部极小点，比如若$f(x)$满足L-Lipschitz条件（对任意$x$存在常数$L$使得$||\\nabla f(x)||\\leq L$成立），则将步长设置为$1/2L$可确保收敛到局部极小点； 当目标函数是凸函数时，局部极小点即对应着函数的全局最小点 调优 梯度下降算法的步长会影响其收敛，步长太大可能难以收敛到局部最优值，步长太小则会导致收敛速度较慢； 初始值不同可能导致梯度下降算法收敛到不同的局部最优解，所以一般可以采取多次训练的方法求解 由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化$$\\frac{x-\\overline{x}}{std(x)}$$从而加快收敛速度 分类批量梯度下降法 Batch Gradient Descent(BGD):在更新参数时使用所有的样本来进行更新 $$\\theta_i=\\theta_i-\\alpha\\sum_{j=0}^{m}(h_\\theta(x_0^{j}, x_1^{j}, …x_n^{j}) - y_j)x_i^{j}$$其中$\\theta_i$为需要估计的达到局部最优时的参数值，步长为$\\alpha$，一共$m$个样本$n$个特征,求梯度的时候用了所有样本的梯度数据 随机梯度下降法 Stochastic Gradient Descent(SGD):计算梯度时仅选取一个样本来求梯度，适用于训练数据集数据量较大的情形 $$\\theta_i=\\theta_i-\\alpha(h_\\theta(x_0^{j}, x_1^{j}, …x_n^{j}) - y_j)x_i^{j}$$ 小批量梯度下降法 Mini-batch Gradient Descent(MBGD):批量梯度下降法和随机梯度下降法的折衷，也就是对于$m$个样本，我们采用$x$个样本来迭代，$1&lt;x&lt;m$ $$\\theta_i=\\theta_i-\\alpha\\sum_{j=t}^{t+x-1}(h_\\theta(x_0^{j}, x_1^{j}, …x_n^{j}) - y_j)x_i^{j}$$ Notes: 目标函数收敛速度来说：随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。 对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。 小批量梯度下降法则是批量梯度下降法和随机梯度下降法的折衷 最小二乘法严格来讲，最小二乘法其实并不能归类到最优化算法这里面来，主要是在涉及线性回归模型的目标函数求解时，不少资料里都提到过可以用最小二乘法或者梯度下降法求解，所以放到这里进行比较讨论，具体的一些讨论可以参照参考资料里的知乎的一系列问题 牛顿法拟牛顿法小结 对于梯度下降法，当目标函数$f(x)$二阶可微时，可将一阶泰勒展开式$f(x+\\Delta x)\\approx f(x)+\\Delta x^T\\nabla f(x)$替换为更精确的二阶泰勒展开式，这样就得到了牛顿法，牛顿法是典型的二阶方法，其迭代轮数远小于梯度下降法；但另一方面由于牛顿法使用了二阶导数，所以其每轮迭代都涉及了海森矩阵的求逆，计算复杂度较高，尤其在高维问题中几乎不可行；若能以较低的计算代价寻找海森矩阵的近似逆矩阵，则可显著降低计算开销，这就是拟牛顿法 参考资料 刘建平Pinard的博客 知乎-最优化问题的简洁介绍是什么？ 知乎-最小二乘法和梯度下降法有哪些区别？ 李航《统计学习方法》 周志华《机器学习》 Andrew Ng 机器学习课程（Coursera） 梯度下降法维基百科","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"最优化问题","slug":"最优化问题","permalink":"http://yoursite.com/tags/最优化问题/"}]},{"title":"机器学习笔记之线性模型","date":"2017-07-09T07:09:53.000Z","path":"2017/07/09/linear-model/","text":"基本形式 线性模型试图学得一个通过属性的线性组合来进行预测的函数，即$$f(x)=w_1x_1+w_2x_2+…w_dx_d+b$$ 线性模型形式简单、易于建模，具有很好的可解释性 很多功能强大的非线性模型可在线性模型的基础上通过引入层级结构或高维映射而得 线性回归线性回归 模型线性回归试图学得$f(x_i)=wx_i+b$,使得$f(x_i)\\approx y_i$ 均方误差如何衡量$f(x)$与$y$之间的差别？均方误差是回归任务中最常用的性能度量，所以关键在于使均方误差最小化，即$$(w^*,b^*)=argmin_{(w,b)}\\sum_{i=1}^m(f(x_i)-y_i)^2$$$$(w^*,b^*)=argmin_{(w,b)}\\sum_{i=1}^m(y_i-wx_i-b)^2$$基于均方误差来进行模型求解的方法称为最小二乘法，在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小 参数估计线性回归模型的最小二乘参数估计，即求解$w,b$使$E_{(w,b)}=\\sum_{i=1}^m(y_i-wx_i-b)^2$最小化的过程 对w,b分别求导，得 $$\\frac{dE_{(w,b)}}{dw}=2\\left(w\\sum_{i=1}^mx_i^2-\\sum_{i=1}^m(y_i-b)x_i\\right)$$ $$\\frac{dE_{(w,b)}}{db}=2\\left(mb-\\sum_{i=1}^m(y_i-wx_i)\\right)$$ 令导数为0可求得 $$w=\\frac{\\sum_{i=1}^my_i(x_i-\\overline{x})}{\\sum_{i=1}^mx_i^2-\\frac{1}{m}(\\sum_{i=1}^mx_i)^2}$$ $$b=\\frac{1}{m}\\sum_{i=1}^m(y_i-wx_i)$$其中$\\overline{x}=\\frac{1}{m}\\sum_{i=1}^mx_i$为$x$的均值 广义线性模型思想线性模型可以有丰富的变化，对于原始的线性模型，我们希望其预测值逼近真实标记$y$，于是得到了线性回归模型，而如果我们令模型预测值逼近y的衍生物，这样一来就可以推广到广义的线性模型 考虑单调可微函数$g(\\cdot)$,令$$y=g^{-1}(w \\cdot x+b)$$,这样得到的模型称为广义线性模型，其中函数$g(\\cdot)$称为联系函数。 实质上是在求取输入空间到输出空间的非线性函数映射 例子——对数线性回归假设认为示例所对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标，即$$ln y=w \\cdot x+b$$即对数线性回归，实际上是在试图让$e^{w\\cdot x+b}$逼近$y$,这里的对数函数就起到了将线性回归模型的预测值与真实标记联系起来的作用 Notes:此外，Logistic Regression模型可以看作是广义线性模型在分类问题上的延伸","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"LR","slug":"LR","permalink":"http://yoursite.com/tags/LR/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"机器学习校招常考知识点小记","date":"2017-07-08T10:47:00.000Z","path":"2017/07/08/ml-job/","text":"概念&amp;原理 高频话题是 SVM、LR、决策树（决策森林）和聚类算法，要重点准备； 算法要从以下几个方面来掌握 产生背景 适用场合（数据规模，特征维度，是否有 Online 算法，离散/连续特征处理等角度）； 原理推导（最大间隔，软间隔，对偶）； 求解方法（随机梯度下降、拟牛顿法等优化算法）； 优缺点，相关改进； 和其他基本方法的对比； 不能停留在能看懂的程度，还要对知识进行结构化整理，比如撰写自己的 cheet sheet，面试是在有限时间内向面试官输出自己知识的过程，如果仅仅是在面试现场才开始调动知识、组织表达，总还是不如系统的梳理准备；从面试官的角度多问自己一些问题，通过查找资料总结出全面的解答，比如如何预防或克服过拟合。 LDA的原理和推导；SVD、LDA 无监督和有监督算法的区别？ 多分类怎么处理？（参见《机器学习》P64,通常切分为多个二分类，最后再集成） OVO OVR MVM–&gt;ECOC(纠错输出码) 为什么会产生过拟合，有哪些方法可以预防或克服过拟合？ 什么情况下一定会发生过拟合？ 训练集和测试集分布不一致的时候 采用EM算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？ 有哪些聚类方法？ K-均值聚类算法、K-中心点聚类算法、CLARANS、 BIRCH、CLIQUE、DBSCAN等 聚类算法中的距离度量有哪些？ 如何进行实体识别？ 聚类和分类有什么区别？分类是事先知道类标的，而聚类事先不知道类标。 L1和L2的区别 生成与判别模型 ROC-AUC 链接 深度学习和机器学习的区别 数据挖掘和人工智能的区别 测试集和训练集的区别 PCA 什么是模糊聚类，还有划分聚类，层次聚类等 http://blog.csdn.net/xiahouzuoxin/article/details/7748823 http://www.cnblogs.com/guolei/p/3899509.html 线性分类器与非线性分类器的区别及优劣； 特征比数据量还大时，选择什么样的分类器？ 对于维度很高的特征，你是选择线性还是非线性分类器？ 对于维度极低的特征，你是选择线性还是非线性分类器？ 如何解决过拟合问题？ L1和L2正则的区别，如何选择L1和L2正则？ 线性回归的梯度下降和牛顿法求解公式的推导 如何处理类别不平衡问题？（参见《机器学习》P66） 欠采样–&gt;EasyEnsemble 过采样–&gt;SMOTRE 阈值移动/代价敏感学习–&gt;原始数据嵌入“负正比”，具体的比如在xgboost中可以调整scale_pos_weight参数为数据集的负正比 常见的最优化方法 牛顿法的原理 L1正则化的优化 0/1损失函数的常用替代损失函数（《机器学习》P130） hinge损失（合页损失）–&gt; SVM 指数损失 –&gt; AdaBoost 对率损失 –&gt; LR、最大熵 模型简介 原理、推导、特性、优缺点、调参 统计学习的核心步骤：模型、策略、算法，你应当对logistic、SVM、决策树、KNN及各种聚类方法有深刻的理解。能够随手写出这些算法的核心递归步的伪代码以及他们优化的函数表达式和对偶问题形式。 SVM的原理、推导、特性，SVM里面的核，SVM非线性分类，核函数的作用；SVM详细过程，支持向量，几何间隔概念，拉格朗日函数如何求取超平面，非线性分类；写写SVM的优化形式，用拉格朗日公式推导SVM kernel变换；SVM：中文分词；RBF核与高斯核的区别（没区别，高斯核就是rbf核）;SVM的支持向量的数学表示 LR 的推导，特性？；为什么可以使用logistic回归;logistic regression为什么使用sigmoid函数？;linear regression 如何处理离散值的情况，如特征为：男1，女0 决策树的特性？;决策树分裂 用EM算法推导解释 Kmeans；K-means起始点；K-means的具体流程；如何优化K-means 解释密度聚类算法 解释贝叶斯公式和朴素贝叶斯分类；贝叶斯分类器的优化和特殊情况的处理 朴素贝叶斯核心思想利用先验概率得到后验概率，并且最终由期望风险最小化得出后验概率最大化，从而输出让后验概率最大化的值（具体概率与先验概率由加入拉普拉斯平滑的极大似然估计而成的贝叶斯估计得到），特征必须相互独立。 KNN（分类与回归） CART（回归树用平方误差最小化准则，分类树用基尼指数最小化准则） GBDT（利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值，拟合一个回归树） 随机森林（Bagging+CART） 神经网络,plsi的推导 关联分析、apriori em算法推导 随机森林的学习过程；随机森林中的每一棵树是如何学习的；随机森林学习算法中CART树的基尼指数是什么？ 谱聚类处理同心圆的情况如何处理 spectral clustering的实现 推导softmax regression模型 模型比较 naive bayes和logistic regression的区别 SVM、LR、决策树的对比？ LR vs. SVM（源于《机器学习》P132） 同：优化目标相近，通常情形下性能也相当 异： LR的优势在于其输出具有自然的概率意义，给出预测标记的同时也给出了概率，而SVM要想得到概率输出需特殊处理 LR能直接用于多分类任务，而SVM则需要进行推广 SVM的解具有稀疏性（仅依赖于支持向量），LR对应的对率损失则是光滑的单调递减函数，因此LR的解依赖于更多的训练样本，其预测开销更大 GBDT和随机森林的区别？ 哪些模型容易过拟合，模型怎么选择 改变随机森林的训练样本数据量，是否会影响到随机森林学习到的模型的复杂度 lr的正则化，与SVM的区别 LR与最大熵的关系 LR与最大熵模型都属于对数线性模型，形式类似 两个模型的学习一般采用极大似然估计，或正则化的极大似然估计，可以形式化为无约束最优化问题（意思然函数为目标函数的最优化问题，此时的目标函数是光滑的凸函数，可以保证找到全局最优解），求解该最优化问题的算法有改进的迭代尺度法、梯度下降法、拟牛顿法等 项目相关 用了什么算法？为什么用？有什么优缺点？ * [实际比赛中为什么tree-ensemble的机器学习方法更好](https://www.zhihu.com/question/51818176/answer/127637712) * 理论（VC维）：模型可控性更好，不易过拟合（用一些弱模型去提升） * 数据：一般基于树的算法的抗噪能力更强 * 树模型中易对缺失值处理 * 树模型对类别特征更友好 * 特征的多样性导致很少使用svm，因为因为 svm 本质上是属于一个几何模型，这个模型需要去定义 instance 之间的 kernel 或者 similarity（线性svm中的内积），而我们无法预先设定一个很好的similarity。这样的数学模型使得 svm 更适合去处理 “同性质”的特征 * 系统实现：——xgboost * 正确高效，C++实现 * 灵活（深度定制不同的分类器、选择不同的损失函数、支持各种语言）、可扩展性好，可以推及到大数据、分布式训练（跨平台） * 简单易用（early_stop,自带cv） * 自适应非线性：随着决策树的生长，能够产生高度非线性的模型，而SVM等线性模型的非线形化需要基于核函数等方法，要求在学习之前就要定好核函数，然而确定合适的核函数并非易事。 * 多分类器隐含正则：高度非线性的模型容易过拟合，因此几乎没有人会用单颗决策树。boosting和random forest等集成学习的方法都要训练多个有差异的学习器，近来有工作表明，这些有差异的学习器的组合，能够起到正则化的作用，从而使得总体复杂度降低，提高泛化能力。尤其对于random forest这样的“并行”集成方法，即使每一颗树都过拟合，直观的来讲，由于过拟合到不同的地方，总体投票或平均后并不会过拟合。 你在研究/项目/实习经历中主要用过哪些机器学习/数据挖掘的算法？ 最好是在项目/实习的大数据场景里用过，比如推荐里用过 CF、LR，分类里用过 SVM、GBDT； 一般用法是什么，是不是自己实现的，有什么比较知名的实现，使用过程中踩过哪些坑； 优缺点分析。 你熟悉的机器学习/数据挖掘算法主要有哪些？ 基础算法要多说，其它算法要挑熟悉程度高的说，不光列举算法，也适当说说应用场合； 你用过哪些机器学习/数据挖掘工具或框架？ 如何进行特征选择？ 用过哪些聚类算法？ 项目中的数据是否会归一化处理，哪个机器学习算法不需要归一化处理 量纲问题：归一化有利于优化迭代速度（梯度下降），提高精度（KNN） 各类算法优缺点、模型调优细节 特征提取的方法（无关键词也是一个特征） 自己实现过什么机器学习算法? 从项目中在哪一方面体会最深 自己项目中有哪些可以迁移到其他领域的东西 项目相比别人有什么优劣 项目的数据从哪里来 项目的特征向量的归一化与异常处理 目前在研究什么 业务 做广告点击率预测，用哪些数据什么算法 http://bbs.pinggu.org/thread-3182029-1-1.html 推荐系统的算法中最近邻和矩阵分解各自适用场景 http://www.doc88.com/p-3961053026557.html 今日头条的个性化推荐是如何实现的？ 推荐算法（基于用户的协同过滤，基于内容的协同过滤） 如何做一个新闻推荐 NLP Q1：给定一个1T的单词文件，文件中每一行为一个单词，单词无序且有重复，当前有5台计算机。请问如何高效地利用5台计算机完成文件词频统计工作？ Ans（有问题的）：将1T文件切分为5份，分配给5台计算机。每台计算机进行词频统计工作，输出一个结果为{单词：频数}的字典结果文件。将5台计算机生成的5个结果文件合并。 Q2：每台计算机需要计算200G左右的文件，内存无法存放200G内容，那么如何统计这些文件的词频？ Ans（不是最优）：首先将文件排序，然后遍历利用list存储结果即可。（不能用字典，因为200G统计出来的结果会很大，没有那么大的内存存放字典。由于经过排序操作，遍历存储并不会使结果丢失，所以用list存储结果即可，每当一个list即将占满内存，则将其写入文件，然后清空list继续存储结果。） Q3：如何将1T的文件均匀地分配给5台机器，且每台机器统计完词频生成的文件只需要拼接起来即可（即每台机器统计的单词不出现在其他机器中） Ans1（不是很好）：对1T文件中的单词进行抽样，获得其概率分布，遍历文件，然后根据首字母的概率均匀分配至5台计算机，如a到e的概率均为0.04, 0.04*5=0.2，则将所有以a-e的单词放入第1台计算机，若z的概率为0.2，则把所有以z开头的单词放入第5台计算机。缺点：不具有可扩展性，如果有100台计算机，那么可能就需要2个字母计算了，则程序就要改变。还有可能出现2台机器中有相同的单词。 Ans2（不是最优）：遍历文件，对于每一个单词，获得单词中各字母的ASCII码值，然后将ASCII值之和取余。则每台机器中的单词必定是不一样。 开放性问题 先不要考虑完善性或可实现性，调动你的一切知识储备和经验储备去设计，有多少说多少，想到什么说什么，方案都是在你和面试官讨论的过程里逐步完善的，不过面试官有两种风格：引导你思考考虑不周之处 or 指责你没有考虑到某些情况，遇到后者的话还请注意灵活调整答题策略; 和同学朋友开展讨论 用户流失率预测怎么做（游戏公司的数据挖掘都喜欢问这个）（涉及数据不平衡问题和时间序列分析） http://www.docin.com/p-1204742211.html 一个游戏的设计过程中该收集什么数据 如何从登陆日志中挖掘尽可能多的信息 http://www.docin.com/p-118297971.html 给你公司内部群组的聊天记录，怎样区分出主管和员工？ 如何评估网站内容的真实性（针对代刷、作弊类）？ 深度学习在推荐系统上可能有怎样的发挥？ 路段平均车速反映了路况，在道路上布控采集车辆速度，如何对路况做出合理估计？采集数据中的异常值如何处理？ 如何根据语料计算两个词词义的相似度？ 在百度贴吧里发布 APP 广告，问推荐策略？ 如何判断自己实现的 LR、Kmeans 算法是否正确？ 100亿数字，怎么统计前100大的？ 每个实体有不同属性，现在有很多实体的各种属性数据，如何判断两个实体是否是同一种东西 重写equals方法，对类里面的对象进行属性比较 学校食堂如何应用数据挖掘的知识 有用户的搜索历史数据，如何判断用户此时的各种状态（准备买车、已经买车等） 给定年龄、性别、学历等信息，对用户的信用进行评估 数学 如何判断函数凸或非凸？ 凸函数定义：对区间$[a,b]$上定义的函数$f$，若他对区间中任意两点$x_1,x_2$均有$f(\\frac{x_1+x_2}{2})\\leq \\frac{f(x_1)+f(x_2)}{2}$,则称$f$为区间$[a,b]$上的凸函数。 U形曲线的函数通常是凸函数 对实数集上的函数通常可以通过求二阶导数来判别，而二阶导数在区间上非负则称为凸函数，若二阶导数在区间上恒大于0则称严格凸函数 解释对偶的概念 一个概率题目： 6个LED灯管，找整体旋转180’后仍然是一个正常输入的情况（考虑全即可） 一个袋子里有很多种颜色的球，其中抽红球的概率为1/4，现在有放回地抽10个球，其中7个球为红球的概率是多少？（伯努利试验） 给定一个分类器p，它有0.5的概率输出1，0.5的概率输出0。 Q1：如何生成一个分类器使该分类器输出1的概率为0.25，输出0的概率为0.75？ Ans：连续进行两次分类，两次结果均为1则输出1，其余情况（10,01,00）均输出0。 Q2：如何生成一个分类器使该分类器输出1的概率为0.3，输出0的概率为0.7？ Tip：小明正在做一道选择题，问题只有A、B和C三个选项，通过抛一个硬币来使选择3个选项的概率相同。小明只需抛连续抛两次硬币，结果正正为A，正负为B，负正为C，负负则重新抛硬币。 Ans：连续进行4次分类（2^4=16 &gt; 10），结果前3种情况则输出1，结果接下来7种情况则输出0，其余情况重新进行分类。 概率题：一个游戏，升一级的概率为p1，等级保持不变的概率为p2，等级下降一级的概率为p3。一个用户经过n个回合，等级为m的概率 大数据相关 如何用hadoop实现k-means？ spark工作原理 spark运行原理 map-reduce mapreduce常用的接口 mapreduce的工作流程 MR优化方式 什么样的情况下不能用mapreduce mapreduce怎么实现join连接 如何解决mapreduce的数据倾斜 mapreduce怎么实现把移动数据到移动计算的 10亿个整数，1G内存，O(n)算法，统计只出现一次的数。 方案一：分拆然后分布式，方案二：对应每个数有三个状态，01代表出现一次，统计10亿以内数据，然后看最终哪些是01状态 海量数据排序； bit位操作 海量数据中求取出现次数最大的100个数 链接 分而治之/hash映射 + hash统计 + 堆/快速/归并排序； 双层桶划分 Bloom filter/Bitmap； Trie树/数据库/倒排索引； 外排序； 分布式处理之Hadoop/Mapreduce。 实现一个分布式的矩阵向量乘的算法 实现一个分布式的topN算法 深度学习 介绍卷积神经网络，和DBN有什么区别？ Deep CNN, Deep RNN, RBM的典型应用与局限，看Hinton讲义和Paper去吧 语言 你觉得Python和Java在使用起来，有什么区别？ Java 数组与链表的区别是什么？ 给你两张表，表A和表B，其中表A有3条数据，表B有5条数据，问：表A left join 表B后有几条？ ArryList、LinkedList、vector的区别 hashMap HashTable的区别 垃圾回收机制 JVM的工作原理 for循环LinkedList 遍历HashMap的并且把某一个值删除 线程 进程 Java中Runnable和Thread的区别Callable Callable与Future的介绍 sleep wait区别 java的数据类型 JAVA如何实现序列化 反序列化是什么？ 序列化是将（内存中的）结构化的数据数据，序列化成2进制 python读取文件，写代码 python计算一个文件中有N行，每行一列的数的平均值，方差，写代码 main(argc,argv[])里面两个参数什么意思 args是Java命令行参数，我们在DOS中执行Java程序的时候使用“java 文件名 args参数”。args这个数组可以接收到这些参数。当然也可以在一个类中main方法中直接调用另一个类里的main方法，因为main方法都是static修饰的静态方法，因此可以通过类名.main来调用，这时就可在调用处main方法中传入String[]类型的字符串数组，达到调用的目的，也可不传入参数 Python list有哪几种添加元素的方法，能否从表头插入元素？(append, extend和insert, insert能从表头插入元素, 但是时间复杂度为O(n).) 如何提高Python的运行效率 写一个简单的正则匹配表达式(将文本中的123.4匹配出来)（Python） a = [1, 2, 3, 4], b = a, b[0] = 100, 请问print(a)结果是什么 list是怎样实现的 常用的数据结构及应用场景（list，dict，tuple） Python装饰器、yield等 给定一个文件，包含了data, ciyt, count等信息，写代码实现给定city和date1-date2日期内，count最高的日期 数据结构与算法 两个数组，求差集 写程序实现二分查找算法，给出递归和非递归实现，并分析算法的时间复杂度。 实现单链表的反转 求两个一维数组的余弦相似度，写代码 字符串翻转 快排 非递归的二叉前/中/后序遍历 手写二叉树前/中/后序递归遍历算法（千万不要忘记异常处理！） 两个字符串的复制（除了字符串地址重叠的情况，也要注意判断字符串本身的空间足够不足够，对于异常情况要考虑全面） 一个数组，如果存在两个数之和等于第三个数，找出满足这一条件的最大的第三个数（设为x+y =c） 怎样将二叉排序树变成双向链表，且效率最高 从栈里找最小的元素，且时间复杂度为常数级 float转string 判断一棵树是否是另一棵的子树 在一个n*n的矩阵中填数的问题，那种转圈填数 链表存在环问题，环的第一个节点在哪里？ 几个排序算法，时间复杂度&amp;空间复杂度 数据结构当中的树，都有哪些？ 二叉查找树（二叉排序树）、平衡二叉树（AVL树）、红黑树、B-树、B+树、字典树（trie树）、后缀树、广义后缀树 链接 输出一个循环矩阵 翻转字符串（《剑指offer》原题） N个数找K大数那个题,堆解释了一遍,比较满意,问还能怎么优化O(Nlogk)的方法，并行方面想 一个班60个人怎么保证有两个人生日相同,听完后有点奇怪,①为什么是60个人?②为什么是保证?,反正没管这么多就是概率嘛,算就完了. 1减去50个人生日不同的概率≈100% 问一个字符串怎么判断是邮箱比如:vzcxn@sdf.gre.有限状态自动机,然后要我画状态转移图. 链接 给10^10个64位数,100M内存的空间排序,一个求中位数的方法.用文件操作来做了,像快排一样,二分选个数统计大于那个数的数量和小于那个数的数量,如果能用100M的空间排序就把那些数排了,如果不能继续.直到能排为止. 链接 kmp算法 求最大字段和，用动态规划和分治法两个方法，时间复杂度怎么算 统计字符串中出现的字符个数，忽略大小写，其中可能有其他字符。 寻找二叉树的公共父节点 b+ b-树、红黑树 判断两条链表是否交叉 树的广度、深度遍历 稳定与不稳定排序 并行计算、压缩算法 最长上升子序列，两个大小相同的有序数组找公共中位数 介绍大顶堆和小顶堆 反转链表 单链表转二叉树，二叉树转单链表（要求原地） 参考 知乎-如何准备机器学习工程师的面试 ？ by 刘志权 机器学习及大数据相关面试的职责和面试问题(转) by 姚凯飞","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"offer","slug":"offer","permalink":"http://yoursite.com/tags/offer/"}]},{"title":"统计学习方法笔记（六）——Logistic Regression","date":"2017-07-08T05:49:08.000Z","path":"2017/07/08/lr/","text":"Logistic Regression与最大熵模型均属于对数线性模型 Logistic Regression的理解角度一：输入变量$X$服从Logistic分布的模型Logistic分布 定义：设$X$是连续随机变量，$X$服从Logistic分布是指$X$具有下列分布函数和密度函数 分布函数$$F(X)=P(X \\leq x)=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}$$ 密度函数$$f(x)=F’(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma(1+e^{-(x-\\mu)/\\gamma})^2}$$其中$\\mu$为位置参数，$\\gamma&gt;0$为形状参数 分布函数属于Logistic函数，对应的图行为一条S形曲线（sigmoid curve），以点$\\left(\\mu,\\frac{1}{2}\\right)$为中心对称，如图，曲线在中心附近增长较快 二项LR模型 二项LR模型是一种分类模型，由条件概率分布$P(Y|X)$表示，形式为参数化的Logistic分布，其中随机变量$X$的取值为实数，随机变量$Y$的取值为1或0 定义：二项LR模型是如下的条件概率分布：$$P(Y=1|x)=\\frac{exp(w \\cdot x+b)}{1+exp(w \\cdot x+b)}$$$$P(Y=0|x)=\\frac{1}{1+exp(w \\cdot x+b)}$$其中$w \\in R^n$称为权值向量，$b$称为偏置，两者为需要估计（学习）的参数,$w \\cdot x$为$w$和$x$的内积 对于给定的输入实例$x$，按照上式可求得对应的条件概率，比较两个条件概率的大小，将实例划分到较大的一类 有时为了方便，将偏置扩充到权值向量中，即$w=(w^{(1)},w^{(2)},…,w^{(n)},b)^T, x=(x^{(1)},x^{(2)},…,x^{(n)},1)^T$,此时模型表示如下$$P(Y=1|x)=\\frac{exp(w \\cdot x)}{1+exp(w \\cdot x)}$$$$P(Y=0|x)=\\frac{1}{1+exp(w \\cdot x)}$$ 角度二：LR模型将线性分类函数转换为（条件）概率 几率：一个事件的几率是指该事件发生的概率与该事件不发生的概率的比值：若事件发生的概率为$p$则该事件的几率为$\\frac{p}{1-p}$，进一步地该事件的对数几率为$$logit(p)=log\\frac{p}{1-p}$$ 而观察前面的LR模型可以发现，$$log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=log\\frac{P(Y=1|x)}{P(Y=0|x)}=w \\cdot x$$，也就是说，在LR模型中，输出$Y=1$的对数几率是输入$x$的线性函数，或者说输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型，即LR模型（所以说Logistic Regression属于对数线性模型） 因此，针对对输入$x$进行分类的线性函数，通过LR模型可以将其转换为（条件）概率：$$P(Y=1|x)=\\frac{exp(w \\cdot x+b)}{1+exp(w \\cdot x+b)}$$，其中线性函数的值越接近正无穷，概率值就越接近1，线性函数的值越接近负无穷，概率值就越接近0，这样的模型就是LR模型。 角度三：广义线性模型在分类问题上的推广（参考《机器学习》） 用线性回归模型的预测结果去逼近真实标记的对数几率 对于二分类任务：其输出标记$y \\in \\lbrace 0,1 \\rbrace$,而线性回归模型产生的预测值$f(x)=w\\cdot x+b$是实值 而根据广义线性模型的思想，可以通过寻找一个联系函数来将分类任务的真实标记与线性回归模型的预测值联系起来 最直接的想法是使用单位阶跃函数，即即若预测值$z$大于0就判为正例，小于0则判为反例 但是单位阶跃函数不连续，所以我们希望找到一个替代函数来在一定程度上近似单位阶跃函数，并希望该函数单调可微，于是就引入了对数几率函数$$y=\\frac{1}{1+e^{-z}}$$ 将对数几率函数作为$g^-(\\cdot)$带入广义线性模型则有$$y=\\frac{1}{1+e^{-(w \\cdot x+b)}}$$ 其变形形式为$$ln\\frac{y}{1-y}=w\\cdot x+b$$,若将$y$视作样本$x$作为正例的可能性，则$1-y$是其作为反例的可能性，两者的比值$\\frac{y}{1-y}$称为几率（联系角度二），反映了$x$作为正例的相对可能性 LR模型的优点 LR模型直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题 LR模型不是仅预测出类别，而是可得到近似概率预测，这对于许多需利用概率辅助决策的任务很有用 对数几率函数是任意阶可导的凸函数，有很好的数学性质，现有很多数值优化算法均可直接用于求取最优解 模型参数估计 基本思想：应用极大似然估计法估计模型参数，从而将问题转化为以对数似然函数为目标函数的最优化问题，然后采用梯度下降法或者拟牛顿法进行求解 此处的目标函数为考点 为什么LR模型的损失函数是交叉熵？而线性回归模型的损失函数却是最小二乘呢？能否随意确定一个损失函数作为目标呢？ 答案是模型的损失函数由各自的响应变量$y$的概率分布决定，比如对于线性回归模型，针对的问题是预测某个结果，属于回归问题，其输出是连续值，所以我们对于该问题假设$y$服从正态分布；相对的，LR模型一般用来解决二分类问题，所以其输出是0/1，故而我们假设其输出服从伯努利分布；而进一步地，两者的损失函数都是通过极大似然估计推导的来的，所以模型的损失函数并非随意确定。 另外，对于线性回归问题，它的模型是 $p(y|x) = N(w^Tx, \\sigma^2)$,我们采用最大似然来构造一个目标函数，最后用梯度下降来找到目标函数的最值。当然，对于这个问题，我们也可以不用梯度下降，直接用向量的投影来直接算出最优解的表达式,即“最小二乘法”；而LR模型是$p(y|x)=Ber(y|sigmoid(w^Tx))$，Ber是伯努利分布，sigmoid是logistic sigmoid函数，我们采用最大似然来构造一个目标函数，与之前的问题不同，这个目标函数比较复杂，是无法像线性回归那样一步直接算出最终的解的，但是，这个目标函数是凸的，所以我们依然能用梯度下降或者牛顿法来来找到它的最优解。 多项LR模型——多分类假设离散型随机变量$Y$的取值集合为$\\lbrace 1,2,…,K \\rbrace$，则多项LR模型是$$P(Y=k|x)=\\frac{exp(w_k \\cdot x)}{1+\\sum_{k=1}^{K-1}exp(w_k \\cdot x)}$$$$P(Y=K|x)=\\frac{1}{1+\\sum_{k=1}^{K-1}exp(w_k \\cdot x)}$$注意$k=1,2,…,K-1$,其中$x\\in R^{n+1}, w_k \\in R^{n+1}$ 参考资料 李航《统计学习方法》 周志华《机器学习》 知乎-最小二乘、极大似然、梯度下降有何区别？","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"LR","slug":"LR","permalink":"http://yoursite.com/tags/LR/"}]},{"title":"GBDT","date":"2017-07-07T06:50:00.000Z","path":"2017/07/07/GBDT/","text":"参考资料:A Gentle Introduction to Gradient Boosting gradient boosting = gradient descent + boosting 附：回顾AdaBoost AdaBoost vs. Gradient Boosting AdaBoost Gradient Boosting 相同点 1、加性模型+前向分步算法 2、每一步训练一个弱学习器以弥补前面模型的不足 不同点 1、AdaBoost中当前学习器的“不足”由样本权重来决定 2、GBDT中当前学习器的“不足”由求梯度决定 Boosting历史 发明AdaBoost。第一个成功的提升算法 规整AdaBoost为使用特殊损失函数的梯度下降方法 将AdaBoost泛化为Gradient Boosting（能处理一般的损失函数） 简单例子理解给定数据集$(x_1,y_1),(x_2,y_2),…,(x_n,y_n)$，现在需要拟和模型$F(x)$来使平均误差最小化，假设你的朋友给你提供了一个模型$F$，你发现该模型还可以进一步提升，对于上述数据集，该模型存在少量错误，比如$F(x_1)=0.8$但$y_1=0.9; F(x_2)=1.4$但$y_2=1.3$，如何改进这个模型？ 一种可行的方法就是在原始的模型$F$的基础上添加一个模型，h（比如回归树），从而得到模型$F(x):=F(x)+h(x)$ 通过拟合来改进该模型$$F(x_1)+h(x_1)=y_1$$$$F(x_2)+h(x_2)=y_2$$$$…$$$$F(x_n)+h(x_n)=y_n$$于是我们可以得到$$h(x_i)=y_i-F(x_i)$$,也就是在$F$的基础上我们针对数据集$(x_1,y_1-F(x_1)),(x_2,y_2-F(x_2)),…,(x_n,y_n-F(x_n))$拟和一个模型$h$，其中$y_i-F(x_i)$被称做是残差 与梯度下降法的关联（回归树）梯度下降$$\\theta_i=\\theta_i-\\rho \\frac{\\partial J}{\\partial \\theta_i}$$通过朝负梯度方向移动来最小化函数 损失函数残差是如何与梯度联系起来的？那么我们为何还需要其他的损失函数？是因为平方损失函数不够好么？ 平方损失函数 优点：易于计算 缺点：对于离群值非常敏感（容易为了拟合离群值而导致整体性能下降） 而其他的一些损失函数比如绝对损失函数、huber loss（丘陵损失）等对于离群值相对没有那么敏感 一般过程（算法） 以$L$为损失函数的回归问题的一般过程 初始化模型如$F(x)=\\frac{\\sum_{i=1}^ny_i}{n}$ 循环直至收敛 计算负梯度$-g(x_i)=-\\frac{dL(y_i,F(x_i))}{dF(x_i)}$ 用一个回归树$h$来拟合负梯度 更新模型$F:=F+\\rho h$ Notes:相较之下，负梯度关注离群点较少。 GBDT用于分类——识别手写字母（多分类）基本思路 训练26个模型：$F_A ~ F_Z$ 针对每一个样本实例，分别使用每一个模型打分，计算样本属于某个类别的概率 将概率最高的类标记为该样本的预测标记 一般过程 将样本集的实际标记转换为概率分布的形式（0-1） 计算各个模型预测样本属于各自分类的概率 计算真实概率与预测概率的差值，目的是通过调整模型减少整体误差（使预测点概率分布尽可能接近真实概率分布） Notes:优化参数矩阵、计算梯度矩阵","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"GBDT","slug":"GBDT","permalink":"http://yoursite.com/tags/GBDT/"}]},{"title":"模型集成小记","date":"2017-07-03T12:08:51.000Z","path":"2017/07/03/ensemble/","text":"本文主要基于周志华《机器学习》一书第八章 集成学习内容做的整理笔记，此外查阅了网上的一些博客和问答网站。 概念 集成学习通过构建并结合多个学习起来完成学习任务。 如何使得集成学习性能比最好的单一学习器更好？ 准确性 多样性 好而不同 如何产生并结合“好而不同”的个体学习器？集成学习研究的核心当前按照个体学习器的生成方式划分 bagging（及其变体随机森林）——个体学习器间不存在强依赖关系，可同时生成的并行化方法 boosting——个体学习器间存在强依赖关系，必须串行生成的序列化方法 boosting工作机制 首先从初始训练集训练出一个基学习器，在根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T,最终将这T个基学习器进行加权组合。 AdaBoost 推导方式，基于“加性模型”，即基于基学习器的线性组合， 如何在每一轮修改样本分布 重赋权法：在每一轮训练中，根据样本分布为每一个训练样本重新赋予一个权重（比如《机器学习实战》一书中就是利用这种方法构建提升树算法，通过修改权重来计算每一轮的损失） 重采样法：利用重采样的训练集来对基学习器进行训练–&gt;重启动：避免训练过程过早停止 Notes: 西瓜书上的算法还提到训练的每一轮开始都要检查当前学习器是否比随机猜测好，若条件不满足则抛弃当前学习器，这种情形可能会导致学习过程未达到T轮即停止，所以有重采样的方法来进行重启动避免出现此种情况；但是另一方面《统计学习方法》以及《机器学习实战》中的算法并未有这条判断语句； 《机器学习》一书中提到，从统计学的出发认为AdaBoost实质上是基于加性模型（后续指出这一视角阐释的是一个与AdaBoost很相似的过程而非其本身），以类似牛顿迭代法来优化指数损失函数，通过将迭代优化过程替换为其他优化方法产生了GradientBoosting、LPBoost等；而这里也提到每一种变体针对不同的问题（噪声、数据不平衡等）而拥有不同的权重更新规则。 特点 从偏差-方差分解的角度来看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成（与bagging不同）。 Notes: boosting对于噪音数据较为敏感 bagging Bootstrap AGGregatING的缩写 出发点依然是“好而不同” “不同”——不同基学习器基于不同的样本子集 “好”——使用相互有交叠的采样子集 工作机制基于自助采样法（bootstrap sampling） 给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样经过m次随机采样操作，我们得到m含m个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现，照这样我们可采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合；在对预测输出进行结合时，通常对分类任务使用简单投票法，对回归任务使用简单平均法 优点（相对于boosting） 高效——训练一个bagging集成与直接使用基学习器算法训练一个学习器的复杂度同阶 baggign能不经修改地用于多分类、回归任务（标准AdaBoosting只能适用于二分类任务） 包外估计——自助采样过程中剩余的样本可以作为验证集来对泛化性能进行“包外估计” Notes:包外样本的其他好处（针对基学习器为决策树或神经网络时） 可使用包外样本来辅助剪枝 用于估计决策树中各结点的后验概率以辅助对零训练样本节点的处理 当基学习器为神经网络时，可使用包外样本来辅助早期停止以减小过拟和风险 特点 从偏差-方差角度看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。 随机森林概述 Bagging的一个扩展变体 以决策树维基学习器构建Bagging集成 在决策树的训练过程中引入了随机属性选择 传统决策树在选择划分属性时是在当前结点的属性集合中选择一个最优属性；而在随机森林中，对基决策树的每个节点，先从该节点属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分，其中k控制了随机性的引入程度 特点 简单、易于实现、计算开销小 样本扰动+属性扰动 bagging vs. 随机森林 两者收敛性相似，随机森林的起始性能往往相对较差，但会收敛到更低的泛化误差 随机森林的训练效率常优于Bagging，主要是决策树划分属性时，原始baggin需要对属性全集进行考虑，而rf是针对一个子集 结合策略学习器结合的好处 统计的角度：假设空间很大时，可能存在多个假设在训练集上达到同等性能，但学习其可能误选导致泛化性能不佳，结合多个学习其可以减小该风险 计算的角度：降低陷入糟糕局部极小点的风险 表示的角度：结合多个学习器可扩大假设空间，对于真实假设在假设空间之外的情形可能学得更好的近似 策略 平均法(Averaging) 简单平均法 加权平均法——BMA(贝叶斯模型平均：基于后验概率来为不同模型赋予权重) 投票法(Voting) 多数投票法 绝对多数投票法：若某标记得票过半数则预测为该标记，否则拒绝预测（可靠性） 相对多数投票法：预测为得票最多的标记 加权投票法 若按个体学习器输出值类型划分 硬投票：预测为0/1 软投票：相当于对后验概率的一个估计 学习法 Stacking:先从初始数据集训练出初级学习器，然后生成一个新的数据集用于训练元学习器，在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记；一般使用交叉验证法或留一法来用训练初级学习器未使用的样本来产生元学习器的训练样本 Notes:关于Stacking 《机器学习》作者也指出Stacking本身是一种著名的集成方法，且有不少变体和特例，但他这里是作为一种特殊的结合策略看待 关于Stacking的细节详述，个人觉得“如何在 Kaggle 首战中进入前 10%”一文中阐述的比较透彻，以一幅图来说说明5折Stacking的过程 推荐一个Python的实现了Stacking集成的库mlxtend 原作者举了一个5折stacking的例子，基本方法是， 每一折取训练集80%的数据训练一个基模型并对剩下的20%的数据进行预测，同时将该模型对测试集做出预测，保留训练子集的预测结果和测试集的预测结果 将5折的训练子集预测结果结合起来构成第二层元模型的输入特征进行训练得到元分类器 将前面每一折在测试机集预测得到的结果取均值作为最终元分类器的预测输入，并使用训练好的元分类器在该数据上作出最终预测 此外知乎上的一篇文章还提到 可以将K个模型对Test Data的预测结果求平均，也可以用所有的Train Data重新训练一个新模型来预测Test Data 多样性误差-分歧分解 集成学习“好而不同”的理论分析，见《机器学习》P185~186 寻找集成泛化误差、个体学习器泛化误差、个体学习器间的分歧三者之间的关系 多样性度量 多样性度量是用于度量集成中个体分类器的多样性，即估算个体学习器的多样化程度，典型做法是考虑个体分类器的两两相似/不相似性 度量方法 不合度量 相关系数 Q-统计量 k-统计量 多样性增强如何增强多样性？——在学习过程中引入随机性 类别 数据样本扰动 输入属性扰动 输出表示扰动 算法参数扰动 思路 给定初始数据集，可从中产生不同的数据子集，再利用不同的数据子集训练出不同的个体学习器，通常基于采样法 从初始属性集中抽取若干个属性子集、基于每个属性子集训练一个基学习器（如随机子空间算法），最后结合 对输出表示进行操纵，比如 1、对训练样本的类标记稍作变动（翻转法） 2、对输出表示进行转化，如输出调制法 3、将原任务拆解成多个可同时求解的子任务，如Ecoc法 随机设置不同的参数，比如1、负相关法 2、对参数较少的算法，可将其学习过程中某些环节用其他类似方式替代 适用 不稳定学习器：决策树、神经网络 包含大量冗余属性的数据 - - Notes: 数据样本扰动中相对的稳定基学习器包括：线性学习器、支持向量机、朴素贝叶斯、k近邻学习器等 对于算法参数扰动，与交叉验证做比较，交叉验证常常是在不同参数组合模型里选择最优参数组合模型，而集成则是将这些不同参数组合的模型结合起来，所以集成学习技术的实际计算开销并不比使用单一学习器大很多 参考 kaggle-ensembling-guide Bagging, Boosting &amp; Stacking stackexchange及评论区 如何在 Kaggle 首战中进入前 10% 分分钟带你杀入Kaggle Top 1%","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"ensemble","slug":"ensemble","permalink":"http://yoursite.com/tags/ensemble/"}]},{"title":"统计学习方法笔记(八) —— 提升方法","date":"2017-06-29T05:45:11.000Z","path":"2017/06/29/boosting/","text":"在分类问题中，Boosting通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。 “三个臭皮匠，顶个诸葛亮” 关键词：强可学习、弱可学习、PAC(probably approximately correct) 由于发现弱学习算法通常比发现强学习算法容易，所以Boosting试图解决这么一个问题：“如果已经发现了‘弱学习算法’，那么能否将它提升为‘强学习算法’？” 提升方法就是从弱学习算法出发，反复学习(改变训练数据的权值分布)，得到一系列弱分类器，然后组合这些弱分类器，构成一个强分类器； 提升方法需要解决的两个问题： 在每一轮训练中，如何改变训练数据的权值/概率分布？ 在得到一系列弱分类器后，如何将弱分类器组合成一个强分类器？ AdaBoost——Adaptive Boosting基本思路针对提升方法要解决的两个问题，AdaBoost的做法是这样的（两个‘权值’） 提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。（让被错误分类的样本得到当前弱分类器更多的‘关注’） AdaBoost采取加权多数表决的方法来组合弱分类器：加大分类误差率小的弱分类器的权值 Notes:由上述基本思路可以发现，对于AdaBoost算法而言，其中最核心的部分是两个权值的求解，一个是每一轮用于训练基分类器的训练数据的样本的权值分布，而另一个则是最终组合各个基分类器时代表各个基分类器的‘话语权’的分类器的权重，而这两者都跟每一轮的基分类器的分类错误、训练误差相关，样本的权值分布根据前一轮基分类器的错误来进行调整，是一种‘弥补前人错误’的体现，而基分类器的权重则是受到每一轮训练的基分类器在该轮的分类误差影响，分类误差越小的基分类器自然可信度越高，所以最终我们组合基分类器时给它赋予的‘话语权’越大。 算法-AdaBoost 输入： 训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $，其中$x_i \\in R^n, y_i \\in \\lbrace -1, +1 \\rbrace$; 弱学习算法 输出：最终分类器$G(x)$ (1) 初始化训练样本的权值分布为$$D_1=(w_{11}, w_{12},…,w_{1N}), w_{1i}=\\frac{1}{N}, i=1,2,…,N$$即各个训练样本的权重均等 (2) 对$m=1,2,…,M, m$表示训练轮数(亦即基分类器个数，因为每一轮产生一个基分类器)： 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器$G_m(x)$ 计算$G_m(x)$在训练数据集上的分类错误率：$$e_m=P(G_m(x_i) \\neq y_i)=\\sum_{i=1}^Nw_{mi}I(G_m(x_i) \\neq y_i)$$即分类错误的样本$\\times$该样本的权重之和 计算最终基分类器在组合分类器中的权重值$$\\alpha_m=\\frac{1}{2}log\\frac{1-e_m}{e_m} $$为什么是这么个值，周志华的西瓜书上有对应推导 更新训练数据集的权值分布$$D_{m+1}=(w_{m+1,1},…,w_{m+1,i},…,w_{m+1,N})$$如果$G_m(x_i)=y_i$那么$$w_{m+1,i}=\\frac{w_{m,i}}{Z_m}e^{-\\alpha_m}$$否则$$w_{m+1,i}=\\frac{w_{m,i}}{Z_m}e^{\\alpha_m}$$ 如果将权重更新简化表示就是$$w_{m+1,i}=\\frac{w_{m,i}}{Z_m}exp(-\\alpha_my_iG_m(x_i))$$其中$Z_m$是规范化因子，用来使权重分布成为一个概率分布$$Z_m=\\sum_{i=1}^Nw_{m,i}exp(-\\alpha_my_iG_m(x_i))$$ (3) 构建基本分类器的线性组合$$f(x)=\\sum_{m=1}^M\\alpha_mG_m(x)$$得到最终分类器$$G(x)=sign(f(x))=sign(\\sum_{m=1}^M\\alpha_mG_m(x))$$ Notes: 关于权值分布更新的计算：由于$Z_m$是未规范化的权值分布更新结果之和，所以一般先通过$$\\acute{w_{m+1,i}}=w_{m,i}exp(-\\alpha_my_iG_m(x_i))$$进一步由$$Z_m=\\sum_{i=1}^N\\acute{w_{m+1,i}}$$求得$Z_m$，最后将$\\acute{w_{m+1,i}}$均除以$Z_m$即得到$w_{m+1,i}$的值，其实本质上就是一个规范化的过程 训练结束的条件：一般是基分类器的组合分类器分类错误率足够小或者达到指定的基分类器数目（亦即迭代次数m）时即停止迭代训练 由上述算法可知AdaBoost的两大特点 不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用（与bagging的区别） 利用基本分类器的线性组合构建最终的分类器（与GBDT的区别） $f(x)$的符号决定实例的类别，其绝对值表示分类的确信度 关于上述算法流程的熟悉，可以通过书上P140简单的例子或者《机器学习实战》一书中对应AdaBoost实现的部分来感受一下 训练误差分析即证明 AdaBoost能在学习过程中不断减少训练误差，即在训练数据集上的分类误差率（关于这一点进一步引出了AdaBoost是否不会过拟合的讨论） AdaBoost的训练误差是以指数速率下降的 由上面两个定理，可以得到推论 如果存在$\\gamma&gt;0$,对所有$m$有$\\gamma_m \\leq \\gamma$【原书这里是$\\gamma_m \\geq \\gamma$，但个人感觉应该是$\\leq$才能推得$exp(-2\\sum_{m=1}^M \\gamma_m^2) \\leq exp(-2M\\gamma^2)$】,则$$\\frac{1}{N}\\sum_{i=1}^NI(G(x_i) \\neq y_i) \\leq exp(-2M\\gamma^2)$$ 因为$$\\frac{1}{N}\\sum_{i=1}^NI(G(x_i) \\neq y_i) \\leq \\prod_mZ_m \\leq exp(-2\\sum_{m=1}^M \\gamma_m^2) \\leq exp(-2M\\gamma^2)$$这也表明在此条件下AdaBoost的训练误差是以指数速率速率下降的 Notes: AdaBoost算法不需要知道下界$\\gamma$ AdaBoost具有适应性，即它能适应弱分类器各自的训练误差率，这也是它的名字Adaptive Boosting的由来 前向分步算法 可以认为AdaBoost算法是模型为加法模型，损失函数为指数函数，学习算法为前向分步算法时的二类分类学习方法。 基本思想 假设现在在给定训练数据和损失函数$L(y, f(x))$的条件下，要学习一个加法模型$$f(x)=\\sum_{m=1}^{M}\\beta_mb(x;\\gamma_m)$$其中$b(x;\\gamma_m)$为基函数，$\\gamma_m$为基函数的参数，$\\beta_m$为基函数的系数 学习该加法模型的问题可以转化为经验风险极小化即损失函数极小化问题$$min_{\\beta_m, \\gamma_m}\\sum_{i=1}^{N}L(y_i,\\sum_{m=1}^{M}\\beta_mb(x_i;\\gamma_m))$$ 而上述问题是一个需要同时求解从$m=1$到$M$所有参数$\\beta_m, \\gamma_m$的优化问题，较为复杂； 而前向分步算法的思想就是，针对该加法模型，从前到后，每一步仅学习一个基函数及其参数，逐步逼近上述优化目标函数式，如此简化优化的复杂度，即每一步只需优化损失函数$$min_{\\beta, \\gamma}\\sum_{i=1}^{N}L(y_i,\\beta b(x_i;\\gamma))$$ 算法-加法模型+前向分步算法 输入： 训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $ 损失函数$L(y, f(x))$ 基函数集$b(x;\\gamma)$ 输出：加法模型$f(x)$ (1) 初始化$f_0(x)=0$ (2) 对$m=1,2,…,M$ 极小化损失函数$$(\\beta_m, \\gamma_m)=argmin_{\\beta, \\gamma}\\sum_{i=1}^{N}L(y_i,f_{m-1}(x_i)+\\beta b(x_i;\\gamma))$$得到参数$\\beta_m, \\gamma_m$ 更新$$f_m(x)=f_{m-1}(x)+\\beta_m b(x;\\gamma_m)$$ (3) 得到加法模型$$f(x)=f_M(x)=\\sum_{m=1}^M\\beta_mb(x;\\gamma_m)$$ Notes:由上述算法的第(2)部分可以看到，前向分布算法每次在上一个基函数得到的累加模型基础上通过极小化当前基函数与累加模型之和的值从而得到当前基函数及其参数，然后累加到前面得到的加法模型上，这样一步一步地逼近想要优化的目标函数，最终会得到一个近似的加法模型 前向分步算法与AdaBoost最后，来看一下每一轮样本权值的更新$$f_m(x)=f_{m-1}(x)+\\alpha_mG_m(x)$$因为$\\overline{w_{mi}}=exp(-y_if_{m-1}(x_i))$，对两边乘上$-y_i$并取指数得到$$\\overline{w_{m+1,i}}=\\overline{w_{mi}}exp(-y_i\\alpha_mG_m(x))$$对比AdaBoost中的权值更新步骤$$w_{m+1,i}=\\frac{w_{m,i}}{Z_m}exp(-\\alpha_my_iG_m(x_i))$$仅相差规范化因子，因而等价 Notes: 对于原书中证明求$\\alpha_m^*$的最后$$e_m=\\frac{\\sum_{i=1}^N\\overline{w_{mi}}I(y_i \\neq G_m(x_i))}{\\sum_{i=1}^N\\overline{w_{mi}}}=\\sum_{i=1}^Nw_{mi}I(y_i \\neq G_m(x_i))$$中的后半部分是怎么来的实在理解不了，猜测可能跟AdaBoost训练误差界中的定理以及后面的$\\overline{w_{m+1,i}}=\\overline{w_{mi}}exp(-y_i\\alpha_mG_m(x))$有关 推导的核心还是基分类器在最终分类器中的权重$\\alpha_m$以及样本权重分布这两项的推导 为什么是指数损失函数？参考周志华《机器学习》一书P174 由最终得到的基分类器的线性组合$f(x)=\\sum_{m=1}^M\\alpha_mG_m(x)$ 若$f(x)$能令指数损失函数最小化，即最小化$L(y,f(x))=e^{-yf(x)}$ 对其求偏导得$$\\frac{d(L(y,f(x)))}{df(x)}=-e^{f(x)}P(y=1|x)+e^{f(x)}P(y=-1|x)$$ 令其为0得到$$f(x)=\\frac{1}{2}ln\\frac{P(y=1|x)}{P(y=-1|x)}$$ 于是最终分类器$$sign(f(x))=sign(\\frac{1}{2}ln\\frac{P(y=1|x)}{P(y=-1|x)})$$ 即$$sign(f(x))=1，如果P(y=1|x)&gt;P(y=-1|x)$$,而$$sign(f(x))=-1，如果P(y=-1|x)&gt;P(y=1|x)$$ 也就表示，$sign(f(x))$达到了贝叶斯最优错误率，即若指数损失函数最小化则分类错误率也将最小化，这说明 指数损失函数是分类任务原本0/1损失函数的一致的替代损失函数； 由于该替代函数有更好的数学性质（比如连续可微），因此我们用其替代0/1损失函数作为优化目标（类似的这种情况在SVM里用合页损失函数代替0-1损失函数也出现过） 提升树模型 提升树是以分类树或回归树为基本分类器的提升方法 以决策树为基函数的提升方法称为提升树（一般是用决策树桩(decision stump)即由一个根结点直接连接两个叶节点的简单决策树） 提升树模型可以表示为决策树的加法模型$$f_M(x)=\\sum_{m=1}^{M}T(x;\\varTheta_m)$$其中$T(x;\\varTheta_m)$表示决策树，$\\varTheta_m$为决策树的参数，$M$为树的个数 算法 不同问题的提升树学习算法区别在于使用的损失函数不同 回归树：平方误差（拟合残差） 分类树：指数损失函数 一般决策树：一般损失函数 回归问题的提升树算法前向分步算法 输入：训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace ,x_i \\in R^n, y_i \\in R $ 输出：提升树$f_M(x)$ (1) 初始化$f_0(x)=0$ (2) 对$m=1,2,…,M$ 计算残差$r_{mi}=y_i - f_{m-1}(x_i), i=1,2,…,N$ 拟合残差$r_{mi}$学习一个回归树，得到$T(x;\\varTheta_m)$ 更新$f_m(x)=f_{m-1}(x)+T(x;\\varTheta_m)$ (3) 得到回归问题提升树$$f_M(x)=\\sum_{m=1}^{M}T(x;\\varTheta_m)$$ Notes:对于上述算法的过程理解可以参照原书P149的例子来感受一下，需要回顾决策树（主要是CART算法）的知识点 除了一开始初始化以外，每一步都是拟合上一轮得到的加法模型的残差来寻找最佳切分点并构建决策树桩，然后将该决策树桩累加到加法模型上作为当前模型并计算平方损失误差 当当前模型的平方损失误差降到某一阈值时则当前模型就是最终输出的模型。 梯度提升由来 提升树利用加法模型与前向分步算法实现学习的优化过程，当损失函数为平方损失（回归）和指数损失函数（分类）时，每一步的优化较为简单，但是对一般损失函数而言，往往每一步的优化并不容易，而梯度提升算法则是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值$$-\\left[\\frac{\\partial L(y,f(x_i)}{\\partial f(x_i)} \\right]_{f(x)=f_{m-1}(x)}$$作为回归问题提升树算法中的残差的近似值，拟合一个回归树 算法（回归树） 输入： 训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace ,x_i \\in R^n, y_i \\in R $ 损失函数$L(y,f(x))$ 输出：回归树$\\widehat{f}(x)$ (1) 初始化$$f_0(x)=argmin_c\\sum_{i=1}^NL(y_i,c)$$来估计使损失函数极小化的常数值，它是只有一个根节点的树 (2) 对$m=1,2,…,M$ 对$i=1,2,…,N$,计算$$r_{mi}=-\\left[\\frac{\\partial L(y,f(x_i)}{\\partial f(x_i)} \\right]_{f(x)=f_{m-1}(x)}$$即损失函数的负梯度在当前模型的值，将它作为残差的估计（对于平方损失函数，它就是通常所说的残差；而对于一般损失函数，它就是残差的近似值） 对$r_{mi}$拟合一个回归树，得到第$m$棵树的叶节点区域$R_{mj}, j=1,2,…,J$ 对$j=1,2,…,J$,计算$$c_{mj}=argmin_c\\sum_{x_i \\in R_{mj}}L(y_i, f_{m-1}(x_i)+c)$$即利用线性搜索估计叶节点区域的值，使损失函数极小化 更新回归树$$f_m(x)=f_{m-1}(x)+\\sum_{j=1}^Jc_{mj}I(x \\in R_{mj})$$ (3) 得到最终的回归树$$\\widehat{f}(x)=f_M(x)=\\sum_{m=1}^M\\sum_{j=1}^Jc_{mj}I(x \\in R_{mj})$$ Notes:梯度提升树（回归树）的算法与原始的提升树（回归树）算法的核心区别主要在于残差计算这里，由于原始的回归树指定了平方损失函数所以可以直接计算残差，而梯度提升树针对一般损失函数，所以采用负梯度来近似求解残差。 实现基于《机器学习实战》一书的第7章内容用决策树桩实现了简单的AdaBoost算法 简要叙述了Boosting及其与Bagging的异同 基于决策树桩实现了简单的AdaBoost算法 应用所实现的AdaBoost分类器对马疝病数据集进行分类 此外简要探讨了非平衡数据集的一些问题，包括 精确率、召回率 过采样、欠采样 roc-auc的实现原始代码可以参考我的github","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"decision tree","slug":"decision-tree","permalink":"http://yoursite.com/tags/decision-tree/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"}]},{"title":"基础算法小记","date":"2017-06-27T02:56:11.000Z","path":"2017/06/27/basic-algorithm/","text":"本文大部分内容基于《算法》4th一书内容，部分补充来自于网络或者其他书籍 查找类二分查找【重点】基本思想在查找时，通过比较中位数与所查找值的大小关系，若找到则返回，若查找失败则相应的调整查找数据集的最高位/最低位的位置来使每次缩减查找范围为原始的一半 Java实现 循环 1234567891011121314151617181920212223242526 //二分查找适用于有序的数组，另外也有一些延伸用法，见剑指offer public static int binarySearch(int[] num, int key)&#123; if(num==null || num.length&lt;=0)&#123; return -1; &#125; int low = 0; //为什么是length-1,因为查找也是从第0位开始的 int high = num.length-1; int pos = -1; while(low &lt;= high)&#123;//注意这里的取等与数组的元素奇偶个数、取边界值、边界以外的值相关，小心 int mid = (low+high) &gt;&gt; 1; if(num[mid] == key)&#123; pos = mid; break; &#125; else if(num[mid] &gt; key)&#123; high = mid - 1; &#125; else&#123; low = mid + 1; &#125; &#125; return pos; //如果是需要在数组中不存在该键时要求返回插入的位置，则修改如下// return low; &#125; 递归实现 123456789101112131415161718192021 public static int binarySearchRecursive(int[] num, int key)&#123; return (num==null||num.length==0)?-1:binarySearchRecursive(num,key,0,num.length-1); &#125; public static int binarySearchRecursive(int[] num, int key, int begin, int end)&#123; if(begin&gt;end)&#123; return -1; //如果是需要在数组中不存在该键时要求返回插入的位置，则修改如下// return begin; &#125; int mid=(begin+end)&gt;&gt;1; if(num[mid]==key)&#123; return mid; &#125; else if(num[mid]&gt;key)&#123; return binarySearchRecursive(num,key,begin,mid-1); &#125; else&#123; return binarySearchRecursive(num,key,mid+1,end); &#125; &#125; 测试 123456789 public static void main(String[] args) &#123; int[] num = &#123;1, 3, 5, 7, 9, 10&#125;;// int[] num = &#123;&#125;; System.out.println(binarySearch(num, 9)); System.out.println(binarySearchRecursive(num, 9)); System.out.println(binarySearch(num, 11)); System.out.println(binarySearchRecursive(num, 0)); &#125; 特点在$N$个键的有序数组中进行二分查找最多需要$logN+1$次比较（无论是否成功），即二分查找所需时间在对数范围之内。 附：符号表（查找）的各种实现的优缺点 符号表的各种实现的优缺点 使用的数据结构 优点 缺点 链表（顺序查找） 适用于小型问题 对于大型数据集很慢 有序数组(二分查找) 最优的查找效率和空间需求，能够进行有序性相关的操作 插入操作很慢 二叉查找树 实现简单，能够进行有序性相关的操作 没有性能上界的保证，此外链接需要额外的空间 平衡二叉查找树（红黑树实现） 最优的查找和插入效率，能够进行有序性相关的操作 链接需要额外的空间 散列表 能够快速的查找和插入常见类型的数据 需要计算各种类型的数据的散列，无法进行有序性相关的操作，此外链接和空结点需要额外的空间 来源于《算法》一书中3.1.7 各种符号表实现的渐进性能总结 算法（数据结构） 查找（最坏情况） 插入（最坏情况） 查找命中（平均情况） 插入（平均情况） 关键接口 顺序查询（无序链表） N N N/2 N equals() 二分查找（有序数组） $lgN$ N $lgN$ N/2 compareTo() 二叉树查找（二叉查找树） N N 1.39$lgN$ 1.39$lgN$ compareTo() 2-3树查找（红黑树） 2$lgN$ 2$lgN$ 1.00$lgN$ 1.00$lgN$ compareTo() 拉链法（链表数组） &lt;$lgN$ &lt;$lgN$ N/2M N/M equals() hashCode() 线性探测法（并行数组） c$lgN$ c$lgN$ &lt;1.5 &lt;2.5 equals() hashCode() N表示N次插入之后，M表示哈希表中的键的个数 Java的java.util.TreeMap和java.util.HashMap分别是基于红黑树和拉链法的散列表的符号表实现 来源于《算法》一书中3.5.1 排序类快速排序【重点】基本思想分治+双指针：核心在于切分(partition)，即首先根据数组中的某一个元素(一般取当前首位元素)，将数组进行切分，最终目的是使该元素左侧元素均不大于该元素，该元素右侧元素均不小于该元素（通过双指针将左侧指针指向较大元素与右侧指针指向较小元素交换实现）；然后进一步递归地对左右子数组调用切分方法直到最终数组有序。 Notes:双指针i,j,保证i左侧元素均不超过pivot,而j右侧元素均不小于pivot，最终i,j相遇的地方即为pivot的位置，所以重点不在于pivot的选取，比如还可以取中间位置或者任意位置，只是需要选定一个pivot来开始进行划分，每次划分都保证了双指针左右的元素的大小符合要求，然后递归的对双指针左右的元素进行切分处理 Java实现 快速排序 1234567891011121314151617181920212223242526272829303132333435363738public static void quickSort(int[] nums)&#123; shuffle(nums); quickSort(nums,0,nums.length-1);&#125;public static void quickSort(int[] nums, int low, int high)&#123; if(low&gt;=high)&#123; return; &#125; int j=partition(nums,low,high); quickSort(nums,low,j-1); quickSort(nums,j+1,high);&#125;public static int partition(int[] nums, int low, int high)&#123; int pivot=nums[low]; //设置分割值为首位的元素值 int i=low+1; int j=high; while(i&lt;=j)&#123; while(i&lt;=j &amp;&amp; nums[i]&lt;pivot)&#123; //找到左侧超过pivot的第一个元素 i++; &#125; while(i&lt;=j &amp;&amp; nums[j]&gt;pivot)&#123; //找到右侧小于pivot的第一个元素 j--; &#125; if(i&lt;=j)&#123; //交换两边的元素 int temp=nums[i]; nums[i]=nums[j]; nums[j]=temp; i++; j--; &#125; &#125; //最后将pivot放置到“中间”位置达成a[low..j-1]&lt;=a[j]&lt;=a[j+1..high] nums[low]=nums[j]; nums[j]=pivot; return j;&#125; 测试及辅助函数 123456789101112131415161718192021public static void randArray(int[] nums) &#123; for(int i=0;i&lt;nums.length;i++)&#123; nums[i]=(int)(Math.random()*100); &#125;&#125;public static void printArray(int[] nums)&#123; for(int i=0;i&lt;nums.length;i++)&#123; System.out.print(nums[i]+\" \"); &#125; System.out.println();&#125;public static void shuffle(int[] nums)&#123; List&lt;Integer&gt; list=new ArrayList&lt;&gt;(); for(int i=0;i&lt;nums.length;i++)&#123; list.add(nums[i]); &#125; Collections.shuffle(list); for(int i=0;i&lt;nums.length;i++)&#123; nums[i]=list.get(i); &#125;&#125; 特点 优点： 实现简单，适用于各种不同的输入数据且在一般应用中比其他排序算法都要快得多； 原地排序（只需要一个很小的辅助栈） 将长度为N的数组排序所需时间和$NlgN$成正比 性能优势 内循环简洁，只涉及元素比较，而像希尔排序、归并排序还涉及元素的移动 比较次数很少 缺点：切分不平衡时程序可能会变得极为低效–&gt; 这也是在快排中一开始对数组进行随机化处理（shuffle()）的原因，目的是为了降低这种情况出现的概率 进一步改进 对于小数组切换到插入排序，修改123if(low&gt;=high)&#123; return;&#125; 为1234if(low+M&gt;=high)&#123; //M为转换参数，与系统相关，一般取5~15之间 insertSort(nums,low,high); return;&#125; 三取样切分：使用数组的一部分元素的中位数来切分数组，代价是需要计算中位数 熵最优的排序——三向切分的快排 针对存在大量重复元素的数组，可将性能提升到线性级别 将数组划分为三个部分，分别对应于小于、等于和大于切分元素的数组元素1234567891011121314151617181920public static void quick3waySort(int[] nums,int low, int high)&#123; if(high&lt;=low)&#123; return; &#125; int lt=low,i=low+1,gt=high; int pivot=nums[low]; while(i&lt;=gt)&#123; //保证nums[low..lt-1]&lt;pivot=nums[lt..gt]&lt;nums[gt+1..high] if(nums[i]&lt;pivot)&#123; //而i在lt与gt之间，循环的过程就是不断调整i与gt之间的元素 exchange(nums,lt++,i++); &#125; else if(nums[i]&gt;pivot)&#123; exchange(nums,i,gt--); &#125; else&#123; i++; &#125; &#125; quick3waySort(nums,low,lt-1); quick3waySort(nums,gt+1,high);&#125; 归并排序【重点】归并 额外数组空间+双指针 先将原始数组复制一份 分别遍历两个子数组，遍历原始数组，逐个比较指针指向的元素和移动指针，将较小的元素复制到原始的数组，若一个子数组指向最后，则将另一个子数组元素逐个复制到原数组中 Java实现123456789101112131415161718192021public static void merge(int[] nums, int low, int mid, int high)&#123; for(int k=low;k&lt;=high;k++)&#123; copy[k]=nums[k]; &#125; int i=low; int j=mid+1; for(int k=low;k&lt;=high;k++)&#123; if(i&gt;mid)&#123;//左子数组元素用尽 nums[k]=copy[j++]; &#125; else if(j&gt;high)&#123;//左子数组元素用尽 nums[k]=copy[i++]; &#125; else if(copy[i]&lt;copy[j])&#123;//否则谁小就填充谁 nums[k]=copy[i++]; &#125; else&#123; nums[k]=copy[j++]; &#125; &#125;&#125; 这里的copy数组在类中一开始显式声明，然后在sort初始部分一次性分配空间初始化1private static int[] copy; //事先声明一个辅助数组 Notes:这里不一开始就复制所有元素到copy数组的原因在于，后续归并时是一个回退的过程，每次都是将前一阶段已经排好序的子数组复制到对应copy部分，然后进一步归并排序，所以如果一开始就复制而后面不复制，后面中间的排序过程就失效了 递归方法（自顶向下） 递归+分治的思想对某一子数组排序，先将其对半分，然后对各部分分别递归调用排序，最后将有序的子数组归并为最终的排序结果 Java实现12345678910111213141516public static void mergeSort(int[] nums)&#123; copy=new int[nums.length]; mergeSort(nums,0,nums.length-1);&#125;private static void mergeSort(int[] nums, int low, int high) &#123; if(low&gt;=high)&#123; return; &#125; int mid=(low+high)&gt;&gt;1; mergeSort(nums,low,mid); mergeSort(nums,mid+1,high); if(nums[mid]&gt;nums[mid+1])&#123; merge(nums,low,mid,high); &#125;&#125; Notes:sort方法的作用在于安排多次merge方法调用的正确顺序，只有子数组有序了，merge方法才是有效的 归并排序的时间复杂度分析 对于长度为N的任意数组，自顶向下的归并排序需要$\\frac{1}{2}NlgN$至$NlgN$次比较 如图，以$N$表示数组元素数目，n表示树的层级则有$n=lgN$,每个节点都表示一个mergeSort()方法通过merge()方法归并而成的子数组，第$k$层有$2^k$个子数组，每个数组的长度为$2^{n-k}$，所以归并最多需要$2^{n-k}$次比较，因此每层的比较次数为$2^k\\times 2^{n-k}=2^n$,$n$层总共是$n2^n=NlgN$ 自底向上（非递归） 先归并微型数组，然后再成对归并得到的子数组，直到将整个数组归并在一起 Java实现12345678public static void mergeSort2(int[] nums)&#123; copy=new int[nums.length]; for(int sz=1;sz&lt;nums.length;sz*=2)&#123; //每轮遍历倍增归并的子数组大小 for(int low=0;low&lt;nums.length;low+=(2*sz))&#123; //成对归并 merge(nums,low,low+sz-1,Math.min(low+2*sz-1, nums.length-1));//最后一组子数组大小可能小于对半分的子数组 &#125; &#125;&#125; 自底向上的归并排序比较适合用链表组织的数据，这种方法只需要重新组织链表链接就能将链表原地排序 特点 优点：能保证将任意长度为$N$的数组排序所需时间和$NlgN$成正比，所以能使用归并排序处理数百万乃至更大规模的数组 缺点：他所需的额外空间与$N$成正比 归并排序是一种渐近最优的基于比较排序的算法：归并排序在最坏情况下的比较次数和任意基于比较的排序算法所需的最少比较次数都是$~NlgN$ 进一步改进的方向 对小规模子数组用插入排序 不将元素复制到辅助数组，而是在递归调用的每个层次交换输入数组和辅助数组的角色 堆排序【重点】基本思想根据《算法》一书的实现方法，堆排序是使用了一种特殊的数据结构——优先队列从而实现快速获取数组中的最大或者最小值来完成排序。 优先队列：两个核心操作：插入元素&amp;获取最大元素，在书中是使用了二叉堆（数组实现完全二叉树的数据结构）来实现的 基本思路是先将传入的数组处理成有序堆，然后逐个取最大元素（根节点）并恢复对有序的结构直到所有元素均被处理完 堆有序：当一棵二叉树的每个节点都大于或等于它的两个子节点时，被称为堆有序 二叉堆是一组能够用堆有序的完全二叉树排序的元素，并在数组中按照层级存储 Notes: 简言之，主要操作是堆的构建和每次取最大元素以后（堆结构局部破坏）调整和恢复堆结构的操作，也就是sink()方法 从下标0开始存储在数组中的完全二叉树有如下特点，对于二叉树的某一内部结点，假设其下标为k，则其父节点下标为(k-1)/2,两个子节点（若存在）的下标节点分别为2k+1和2k+2,所以我们可以根据下标很方便的交换元素来构建堆以及恢复堆 《算法》一书中给出的是从下标1开始的数组，把第0位作为哨兵在其他应用里会有用，这样的话父节点，子节点相应的需要调整为k,2k和2k+1，这里主要考虑到一般传进来的数组是从0开始的，为了避免需要开辟额外的空间建堆所以进行了修改，使其原地排序 Java实现 堆排序12345678910111213141516171819202122232425262728293031323334public static void heapSort(int[] nums)&#123; int N=nums.length-1; //构建堆 for(int k=(N-1)/2;k&gt;=0;k--)&#123; sink(nums,k,N); &#125; //重复取最大元素并恢复堆结构 while(N&gt;0)&#123; exchange(nums, 0, N--); sink(nums,0,N); &#125;&#125;private static void sink(int[] nums, int p, int N) &#123; //自上而下调整 while(p*2+1&lt;=N)&#123; int temp=p*2+1; if(temp+1&lt;=N &amp;&amp; nums[temp]&lt;nums[temp+1])&#123;//取子节点中较大的一个元素 temp+=1; &#125; if(nums[p]&gt;=nums[temp])&#123; //此时堆有序，无需调整，结束循环 break; &#125; //否则元素下沉 exchange(nums,p,temp); p=temp; &#125; &#125;public static void exchange(int[] nums, int i, int j)&#123; int temp=nums[i]; nums[i]=nums[j]; nums[j]=temp;&#125; 特点 堆排序能同时最优地利用空间和时间，即使在最坏的情形下也能保证使用$~2NlgN$次比较和恒定的额外空间 在空间比较紧张的开发中应用广泛(比如嵌入式系统)，但是现代操作系统中较少使用，主要是他无法利用缓存，数组元素很少与相邻的其他元素进行比较，所以缓存未命中的次数要远远高于大多数比较都在相邻元素间进行的算法，比如快排、归并排序甚至希尔排序 但另一方面，用堆实现的优先队列的应用却越来越重要，因为它能在插入操作和删除最大元素操作混合的动态场景中保证对数级别的运行时间，另外常考算法比如topM和MultiWay(特别针对大数据、数据流) 进一步优化可以通过先下沉后上浮来来免去检查元素是否到达正确位置的时间，但是这种方法需要额外的空间 优先队列的应用延伸TopM【考点】问题描述，参照geeksforgeeks 打印一个无序数组中最大的M个元素 思路概述 部分排序，比如修改冒泡排序，执行M次，然后将数组末端的M个元素打印出来（也可以使用选择排序等，思路类似），时间复杂度为$O(NM)$ 使用一个小型的缓存数组，将原数组中的M个元素先保存到一个大小为M的小数组中，然后查找其中的最小值记为min，接下来遍历原始数组剩余部分，遇到比min大的元素则从小数组中去掉min元素，插入新元素并重新查找到当前最小元素，时间复杂度为$O((N-M)*M+MlgM)$,前一项为遍历以及插入元素以后更新（查找）当前最小元素的时间消耗，后一项则是最后要按顺序打印的小数组排序时间复杂度 直接对原始数组进行排序，时间复杂度一般为$O(NlgN)$，但是这种方法对于数据量较大的情况是不合适的 使用大顶堆存储原始数组，这样构建大顶堆的时间为$O(N)$(参见《算法》P206命题R用下沉法构建堆)，构建好堆以后重复取M次堆顶元素的时间复杂度为$O(MlgN)$，因为涉及到每次取最大元素以后调整堆的操作，所以总体时间复杂度为$O(N+MlgN)$ 有序统计【有待进一步分析】 使用有序统计算法（order statistic algorithm）来找到第k大的元素，时间复杂度为$O(N)$ 使用快速排序的切分算法来将前一步找到的第k大元素作为pivot切分点对数组进行切分，时间复杂度为O(N) 最后对与小于切分点的元素排序输出$O(MlgM)$ 总体时间复杂度为$O(N+MlgM)$ 使用小顶堆【重点掌握】，该方法是对第二种方法的一个改进，即不使用暂存数组，而是使用一个大小为M的小顶堆来暂存数据，这样节省查找时间 使用前M个元素构建一个小顶堆，时间复杂度$O(M)$ 对后续每一个元素，将元素与堆顶元素比较，若比堆顶元素大，则交换并下沉调整堆，时间复杂度为$O((N-M)lgM)$ 最后对小顶堆排序输出$O(MlgM)$总体时间复杂度$O(M+(N-M)lgM+MlgM)$ Java实现 针对小顶堆的方法，不过这里进行了微调，在后续插入数据时采取的方法是移除当前最小值然后插入元素调整堆的方法，可以修改delMin()方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135import java.util.Scanner;//用小顶堆实现从数组中找出其中最大的M个元素public class TopM &#123; public static void main(String[] args) &#123; Scanner in=new Scanner(System.in); if(in.hasNext())&#123; int cases=in.nextInt(); while(cases--&gt;=0 &amp;&amp; in.hasNext())&#123; int N=in.nextInt(); int M=in.nextInt(); int[] total=new int[N]; int[] heap=new int[M]; for(int i=0;i&lt;total.length;i++)&#123; total[i]=in.nextInt(); &#125; topM(total,heap); printArray(heap); &#125; &#125; in.close(); &#125; public static void topM(int[] total, int[] top)&#123; int N=total.length; int M=top.length; for(int i=0;i&lt;N;i++)&#123; if(i&lt;M)&#123; insert(top,i,total[i]); &#125; else&#123; if(total[i]&gt;getMin(top))&#123; delMin(top); insert(top,i,total[i]); &#125; &#125; &#125; heapSort(top); &#125; private static int getMin(int[] top) &#123; if(top.length==0)&#123; return -1; &#125; return top[0]; &#125; private static void delMin(int[] top) &#123; int M=top.length; exchange(top,0,M-1); sink(top); &#125; private static void sink(int[] top) &#123; int M=top.length; int i=0; while(2*i+2&lt;=M-1)&#123; int left=2*i+1; if(left+1!=M-1&amp;&amp;top[left+1]&lt;top[left])&#123; left++; &#125; if(top[i]&lt;=top[left])&#123; break; &#125; exchange(top,i,left); i=left; &#125; &#125; private static void insert(int[] top,int index, int key) &#123; int M=top.length; if(index&gt;=M-1)&#123; index=M-1; &#125; top[index]=key; swim(top,index); &#125; private static void swim(int[] top, int index) &#123; while(index&gt;0)&#123; int parent=(index-1)/2; if(top[parent]&gt;top[index])&#123; exchange(top,index,parent); index=parent; &#125; else&#123; break; &#125; &#125; &#125; private static void exchange(int[] nums, int i, int j) &#123; int temp=nums[i]; nums[i]=nums[j]; nums[j]=temp; &#125; //构建小顶堆排序 public static void heapSort(int[] nums)&#123; int N=nums.length-1; //构建堆 for(int k=(N-1)/2;k&gt;=0;k--)&#123; sink(nums,k,N); &#125; //重复取最大元素并恢复堆结构 while(N&gt;0)&#123; exchange(nums, 0, N--); sink(nums,0,N); &#125; &#125; private static void sink(int[] nums, int p, int N) &#123; //自上而下调整 while(p*2+1&lt;=N)&#123; int temp=p*2+1; if(temp+1&lt;=N &amp;&amp; nums[temp]&gt;nums[temp+1])&#123;//取子节点中较小的一个元素 temp+=1; &#125; if(nums[p]&lt;=nums[temp])&#123; //此时堆有序，无需调整，结束循环 break; &#125; //否则元素下沉 exchange(nums,p,temp); p=temp; &#125; &#125; public static void printArray(int[] nums)&#123; for(int i=0;i&lt;nums.length;i++)&#123; System.out.print(nums[i]+\" \"); &#125; System.out.println(); &#125;&#125; 从数据流中得到当前的k个最大的元素（默认数据流为无限） 这样的话上述不少方法就不能使用了，常见思路如下 维持一个大小为k的有序数组，这样和前面一样，针对后续读取的每一个元素，比当前数组中的首位元素（最小元素）小的元素忽略，大的则删除当前数组最小元素并插入该元素 使用大小为k的平衡二叉查找树来存储元素，当然要求是每次插入元素后得调整树结构保持树的平衡 小顶堆的做法，与前一个问题一致 《剑指offer》第二版一书P209面试题40也有关于这个问题的讨论 Multiway（多向归并） 将M个有序输入流归并为一个有序的输出流(《算法》中文版2.4.4.7 P205) 基本思想 维持一个大小为M的小顶(索引)堆，存储来自不同输入流的索引 先逐个遍历输入流将每个输入流的首位元素（最小元素）插入到堆中，并与对应的索引关联 此时输出的最小元素即为所有输入流的最小元素，每输出一个最小元素，即在输出元素索引对应的输入流中读入下一个元素（如果存在）并插入堆中，这样就能保证每次输出的都是所有输入流中的最小元素 PrimDijkstra霍夫曼压缩选择排序基本思想 不断地选择剩余元素中的最小者并与当前剩余元素第一位交换首先，找到数组中最小的那个元素，其次将它和数组的第一个元素交换位置，再次，在剩下的元素中找到最小的元素，将它与数组的第二个元素交换位置，如此往复，直到将整个数组排序。 Java实现 选择排序 12345678910111213public static void selectSort(int[] nums)&#123; for(int i=0;i&lt;nums.length-1;i++)&#123; int minPos=i; for(int j=i+1;j&lt;nums.length;j++)&#123; if(nums[minPos]&gt;nums[j])&#123; minPos=j; &#125; &#125; int temp=nums[i]; nums[i]=nums[minPos]; nums[minPos]=temp; &#125;&#125; 测试 12345678910111213public static void main(String[] args) &#123; int[] nums=&#123;7,5,1,9,3&#125;; printArray(nums); selectSort(nums); printArray(nums);&#125;public static void printArray(int[] nums)&#123; for(int i=0;i&lt;nums.length;i++)&#123; System.out.print(nums[i]+\" \"); &#125; System.out.println();&#125; 特点 对于长度为$N$的数组，选择排序需要大约$N^2/2$次比较和$N$次交换 运行时间和输入无关（缺点），比如元素相等的数组/已经有序的数组与随机数组的排序时间一样长 数据移动是最少的，即元素交换次数是最少的，每次交换只改变两个数组元素的值，即交换次数和数组的大小呈线性关系，而其他的大部分排序算法是线性对数或者平方级别 插入排序基本思想 整理桥牌 保证数组当前遍历的元素位置左方元素已经有序，然后将当前元素前向逐个比较，一边比较一边将前面的元素向后覆盖，直到找到第一个不大于该元素的位置，将该元素插入（《算法》一书中的基本实现采用的方法是将目标元素与前面比他大的元素逐个交换的做法，相较之下，我的这种直接移动所有较大元素的做法速度更快（《算法》一书作为练习题）） Java实现 插入排序1234567891011public static void insertSort(int[] nums)&#123; for(int i=0;i&lt;nums.length-1;i++)&#123; int temp=nums[i+1]; int j=i+1; while(j&gt;0&amp;&amp;temp&lt;nums[j-1])&#123; nums[j]=nums[j-1]; j--; &#125; nums[j]=temp; &#125;&#125; 特点 插入排序所需时间取决于输入中元素的初始顺序，因此插入排序对于部分有序的数组十分高效，也很适合小规模数组（当数组中倒置（倒置指的是数组中两个顺序颠倒的元素）数量很少时，插入排序可能是最快的排序算法） 希尔排序基本思想 考虑到插入排序不适用于大规模乱序数组，所以进行改进得到的算法 原则是保证数组中任意间隔为h的元素有序，即交换间隔为h的元素以对数组的局部进行排序，并最终使用插入排序将局部有序的数组排序 权衡了子数组的规模和有序性：排序之初，各个子数组都很短，排序之后子数组都是部分有序的，这两种情况都很适合插入排序 Java实现 希尔排序 123456789101112131415161718public static void shellSort(int[] nums)&#123; int h=1; while(h&lt;nums.length/3)&#123; h=h*3+1; &#125; while(h&gt;=1)&#123; for(int i=0;i&lt;nums.length-h;i++)&#123; int temp=nums[i+h]; int j=i+h; while(j&gt;=h&amp;&amp;temp&lt;nums[j-h])&#123; nums[j]=nums[j-h]; j-=h; &#125; nums[j]=temp; &#125; h/=3; &#125;&#125; 测试 12345678910111213141516171819public static void printArray(int[] nums)&#123; for(int i=0;i&lt;nums.length;i++)&#123; System.out.print(nums[i]+\" \"); &#125; System.out.println();&#125;public static void randArray(int[] nums) &#123; for(int i=0;i&lt;nums.length;i++)&#123; nums[i]=(int)(Math.random()*100); &#125;&#125;public static void main(String[] args) &#123; int[] nums=new int[20]; randArray(nums); printArray(nums); shellSort(nums); printArray(nums);&#125; 特点 算法性能取决于h值的选择，这里使用的是序列$\\frac{3^k-1}{2}$，关于怎样的h值最好，尚无定论，实际应用中相差不大，但一般其时间复杂度不会到达$O(n^2)$ 希尔排序也可用于大型数组，其优点在于代码量小而且不需要额外的存储空间，在手头没有现成的排序算法可以调用时，不妨实现希尔排序来辅助工作 冒泡排序基本思想 用一个‘指针’遍历数组并比较与该指针指向元素相邻的元素，如果相邻元素呈逆序状态则交换元素，直到移动到数组末端即完成一轮冒泡排序，然后重复上述操作，一般会设置一个flag来在数组已经有序时（即该轮排序没有发生交换）结束排序 另外一种思路，由于每一次冒泡排序可以保证最大的元素‘沉到’最后一位，所以也可以不设置flag，而使用双指针，后一个指针的遍历范围逐轮缩小其中第一个设置flag的方案更优，因为第二种方法即使在数组有序时也会循环遍历两次 Java实现 flag方法 1234567891011121314151617181920public static void bubbleSort(int[] nums)&#123; while(true)&#123; boolean flag=true; //标记该轮排序是否没有发生交换操作（若是则表明当前数组已有序） for(int i=0;i&lt;nums.length-1;i++)&#123; if(nums[i+1]&lt;nums[i])&#123; flag=false; exchange(nums,i,i+1); &#125; &#125; if(flag)&#123; break; &#125; &#125;&#125;private static void exchange(int[] nums, int i, int j) &#123; int temp=nums[i]; nums[i]=nums[j]; nums[j]=temp;&#125; 双指针 123456789public static void bubbleSort2(int[] nums)&#123; for(int i=0;i&lt;nums.length-1;i++)&#123; for(int j=0;j&lt;nums.length-i-1;j++)&#123; if(nums[j]&gt;nums[j+1])&#123; exchange(nums,j,j+1); &#125; &#125; &#125;&#125; 特点 数组有序时冒泡排序只需遍历一次数组，但是若当数组完全逆序时，则时间复杂度达到$O(n^2)$，这点和插入排序类似 排序算法小结 算法 是否稳定 是否为原地排序 时间复杂度 空间复杂度 备注 选择排序 否 是 $N^2$ 1 - 插入排序 是 是 介于$N$和$N^2$之间 1 取决于输入元素的排列情况 希尔排序 否 是 $NlgN?$,$N^{\\frac{6}{5}}$ 1 - 快速排序 否 是 $NlgN$ lgN 运行效率由概率提供保证 三向快速排序 否 是 介于$N$和$NlgN$之间 $lgN$ 运行效率由概率保证，同时也取决于输入元素的分布情况 归并排序 是 否 $NlgN$ $N$ - 堆排序 否 是 $NlgN$ 1 - 冒泡排序 是 是 介于$N$和$N^2$之间 1 取决于输入元素的排列情况 来源于《算法》一书中2.5.2 稳定性 如果一个排序算法能够保留数组中重复元素的相对位置则可以被称为是稳定的 稳定排序算法：插入排序、归并排序、冒泡排序 另外有很多办法将任意排序算法变成稳定的，不过比较复杂 原地排序 除了函数调用所需的栈和固定数目的实例变量之外无需额外内存的原地排序算法 原地排序算法：上述算法中除了归并排序以外都是 对于Java而言，java.util.Arrays.sort实际上代表了一系列排序方法 每种原始数据类型都有一个不同的排序方法 一个适用于所有实现了Comparable接口的数据类型的排序方法 一个适用于所有实现了比较器Comparator的数据类型的排序方法 一般而言，Java系统程序员选择对原始数据类型使用（三向切分的）快速排序，对引用类型使用归并排序 _ Notes:关于排序算法稳定性以及其他更多的排序算法，可以参考这里_ 排序的应用延伸在线性对数时间内计算两组排列之间的Kendall tau距离关键字：逆序对 找出一组元素的中位数快速排序的partition的进一步应用 基本思想partition+二分查找：由于对原始数组进行一次切分可以得到数组中的一个索引k，而且此时的数组中该位置左边的元素都不超过该元素，右边的都不小于该元素，即k是第k+1大的元素（从1开始计）如果此时k恰为数组大小的一半，那么这个位置的元素就恰好是数组的中位数，如果不是，则采取二分查找的思想，在该索引的某一边进行进一步切分（比如k小于len(array)/2就在k的右侧进行切分），经过有限次切分就可以找到中位数对应的元素。 Notes:本题的变体是寻找数组中出现次数超过一半的数字 最长重复子字符串特殊的字符串排序算法 动态规划贪心回溯法位运算参考资料 Robert Sedgewick《算法》4th http://www.algolist.net/Algorithms/Sorting/Quicksort 排序算法稳定性 geeksforgeeks","tags":[{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"algorithm","slug":"algorithm","permalink":"http://yoursite.com/tags/algorithm/"},{"name":"校招","slug":"校招","permalink":"http://yoursite.com/tags/校招/"},{"name":"面试","slug":"面试","permalink":"http://yoursite.com/tags/面试/"},{"name":"Coding","slug":"Coding","permalink":"http://yoursite.com/tags/Coding/"}]},{"title":"京东JData算法大赛","date":"2017-06-25T06:04:31.000Z","path":"2017/06/25/JData/","text":"比赛简介 本次大赛以京东商城真实的用户、商品和行为数据（脱敏后）为基础，参赛队伍需要通过数据挖掘的技术和机器学习的算法，构建用户购买商品的预测模型，输出高潜用户和目标商品的匹配结果，为精准营销提供高质量的目标群体。同时，希望参赛队伍能通过本次比赛，挖掘数据背后潜在的意义，为电商用户提供更简单、快捷、省心的购物体验。 这是2017年京东举办的一场数据挖掘比赛，比赛地址，吸引了不少人参加，和实验室的两个同学也一起参加进来体验一下真实比赛的氛围，总的来说整个过程确实比较辛苦，不过也算是痛并快乐着。看过赛题的朋友可能大多会和往年的阿里移动推荐大赛作比较，甚至更早一点的天猫推荐大赛比较，总的来说，确实很相像，目的都是以用户的历史行为记录以及商品、用户的一些特征来构建模型，预测用户未来的购买状况。 我们团队的最终比赛成绩如下，A榜排名139/4240，B榜排名97/4240 关于比赛的一点思考推荐算法？首先针对题目的目的需要明确一点，这并不是如比赛标题所言的那种构建推荐系统类型的比赛，关于这一点，我想引用网上大牛的原话来说明 首先引用阿里移动推荐比赛内部赛的冠军算者最后获胜时的文章中的内容 1、代价不同。你推荐给我一首歌，不好听，大不了换一首，不好听，再换，最后总能找到自己喜欢的歌。可是购物呢，你推荐我一个服装品牌，我花了半个月的伙食费买了，等待了几天，到货后发现不满意，怎么办？扔掉？太可惜了！穿在身上？天天别扭！退掉？还要再花点邮费！不管结局怎们样，总之不爽。如果是买个家居什么的，不喜欢的话，可能后悔一辈子。所以购物的用户体验周期要远长于音乐、影视推荐。推荐的试错成本很大。2、需求的单一性与喜好的相似性。喜欢看动作片，那么相似的动作片我都能看一遍。但是购物就不同了，购物更多的是刚需，喜欢可爱的衣服，一般人也不会把所有喜欢的可爱的衣服都买下来，更多的是每个季节只买一件。家居类的频次就低了，可能一辈子就买一次。不同的类目的需求频率是不一样的。 另一个是天猫推荐算法大赛的top9的获奖感言里提到的 和很多拿到这个问题的同学一样，一开始都觉得是推荐问题，那么通过查阅相关资料自然而然的都是协同过滤等相关推荐算法，实际效果可想而知。具体原因我认为是品牌的转移成本，具体是什么意思呢？打个比方，一个普通的消费者，前三个月某一天购买了苹果的一款手机，根据商品相似等算法我们可能得到苹果和三星很相似，然后在第四个月系统向他推荐了三星，从一个消费者的正常心理来看，这种推荐很难促成交易的发生。也许这个比方不太恰当，但是这个例子反映的信息包括品牌的实际周期，用户对品牌的黏度，消费能力，还有时间季节等因素，我们仅仅通过赛题给出的字段很难分析出这些因素，而这些都是反映品牌的转移成本的关键因素，因为购买行为的发生不是一个单步，而是涉及到很多前因后果以及最终的整体相关性的问题。基于以上考虑，实际上留给我们的就是挖掘买买相关，看买相关的这部分信息，从这点出发这次的问题实际上就是一个有监督的二分类问题。 通过以上的观点可以看到本题并非协同过滤算法那种构建推荐系统的比赛，而是一种根据离线历史行为记录预测用户未来某段时间买了又买的可能性 与以往比赛的区别抛开数据量等的区别，本次比赛与天猫、阿里推荐算法大赛的最主要的区别在于本次比赛并非预测未来某一天的可能发生购买行为的用户商品对，而是预测未来5天可能发生购买行为的用户商品对，如果说预测未来一天可能发生购买行为的用户商品对难以捕捉到当天购买（冲动消费等）的用户行为，那么把时间跨度加大到5天的话其实我们就间接地损失了不少可能预测出的用户商品对： 首先根据我们的统计结果，大部分发生购买行为的用户商品对的交互在前三~五天，这也是在分析以前的一些比赛的ppt里提到的时间衰减规律的类似结论 此外，取某一个时间区间发生购买行为的用户商品对分析用户对该商品的交互行为，发现在这五天再往前取三天左右的时间交互的用户商品对仅占所有发生购买行为的用户商品对的约四分之一，仅用户的话能占到约一半，但是其中有四分之一的用户在这三天交互的对象并不是后五天购买的了的目标对象，而剩下的一半用户在这三天根本没有交互行为；而究其原因就是预测后五天的用户商品对购买情况的话，这五天对于我们而言就是一个黑匣子，而统计结果却显示我们需要目标时间点最近三天的交互状况，所以造成了预测的困难，后期也曾想过使用类似关联分析之类的方法，然而受精力时间限制也没尝试，所以这也是一个遗憾，不知道那些大牛是怎么处理的。 接下来我就简单的从三个步骤来介绍下我们团队在这场比赛所做的一些尝试 数据分析和预处理 特征工程 模型设计和评估 数据分析和预处理官方信息 符号定义： S：提供的商品全集； P：候选的商品子集，P是S的子集； U：用户集合； A：用户对S的行为数据集合； C：S的评价数据。 数据 按用途：* 训练数据（2016-2-1~2016-4-15）：用户集合U中的用户，对商品集合S中部分商品的行为、评价、用户数据；提供部分候选商品的数据P。 * 预测数据（2016-4-16~2016-4-20）：用户是否下单P(P为候选商品集)中的商品，**每个用户只会下单一个商品** 按类别 用户：ID,年龄，性别，用户等级，用户注册日期 商品：商品编号，属性1，属性2，属性3，品类ID，品牌ID 评价：商品编号，累计评论数（分段），是否有差评，差评率 行为：用户编号，商品编号，行为时间，点击模块编号，类型（浏览，加入购物车，购物车删除，下单，关注，点击），品类ID，品牌ID 注： 自行组成特征和数据格式，自由组合训练测试数据比例 预测数据分A,B榜计算分数 数据均采样和脱敏，存在空值 预测用户在未来5天内，对某个目标品类下商品的购买意向。对于训练集中出现的每一个用户，参赛者的模型需要预测该用户在未来5天内是否购买目标品类下的商品以及所购买商品的SKU_ID。 评分标准：所有用户在2016-04-16到2016-04-20是否下单P中的商品 是否下单： F11=6*Recall*Precise/(5*Recall+Precise) 若下单，预测下单的商品ID是否正确： F12=5*Recall*Precise/(2*Recall+3*Precise) Score=0.4*F11 + 0.6*F12 Precise为精确率，Recall为召回率 FAQ中的一些信息 提供的数据文件编码格式为GBK 一个用户（user_id）只购买一个所提供的候选商品集合P中的商品（sku_id）；用户购买候选商品集合P之外的商品无需提交 评论数据为截止到当日的累计数据 model_id表明用户在页面上点击了哪一个位置；数据中可能存在一些空值，有可能是异常值，也有可能是在页面上点击了一个空白的位置产生的数据,自行理解并处理 集合P为候选商品集合，即参赛者预测的结果中的sku需要在集合P中；集合S为行为数据中的商品全集； 日期格式统一为”yyyy-mm-dd”，时间格式统一为”yyyy-mm-dd hh:mi:ss” 关于用户表中存在10条记录，用户注册时间晚于2016年4月15日情况为真实生产环境中的异常数据,自行理解并处理 数据集解析这里涉及到的数据集是京东最新的数据集： JData_User.csv 用户数据集 105,321个用户 JData_Comment.csv 商品评论 558,552条记录 JData_Product.csv 预测商品集合 24,187条记录 JData_Action_201602.csv 2月份行为交互记录 11,485,424条记录 JData_Action_201603.csv 3月份行为交互记录 25,916,378条记录 JData_Action_201604.csv 4月份行为交互记录 13,199,934条记录 异常值判断12345%matplotlib inlineimport matplotlibimport matplotlib.pyplot as pltimport numpy as npimport pandas as pd 123456789#定义文件名ACTION_201602_FILE = \"data/JData_Action_201602.csv\"ACTION_201603_FILE = \"data/JData_Action_201603.csv\"ACTION_201604_FILE = \"data/JData_Action_201604.csv\"COMMENT_FILE = \"data/JData_Comment.csv\"PRODUCT_FILE = \"data/JData_Product.csv\"USER_FILE = \"data/JData_User.csv\"USER_TABLE_FILE = \"data/User_table.csv\"ITEM_TABLE_FILE = \"data/Item_table.csv\" 数据背景信息根据官方给出的数据介绍里，可以知道数据可能存在哪些异常信息 用户文件 用户的age存在未知的情况，标记为-1 用户的sex存在保密情况，标记为2 后续分析发现，用户注册日期存在系统异常导致在预测日之后的情况，不过目前针对该特征没有想法，所以不作处理 商品文件 属性a1,a2,a3均存在未知情形，标记为-1 行为文件 model_id为点击模块编号，针对用户的行为类型为6时，可能存在空值 空值判断12345678910def check_empty(file_path, file_name): df_file = pd.read_csv(file_path) print 'Is there any missing value in &#123;0&#125;? &#123;1&#125;'.format(file_name, df_file.isnull().any().any()) check_empty(USER_FILE, 'User')check_empty(ACTION_201602_FILE, 'Action 2')check_empty(ACTION_201603_FILE, 'Action 3')check_empty(ACTION_201604_FILE, 'Action 4')check_empty(COMMENT_FILE, 'Comment')check_empty(PRODUCT_FILE, 'Product') Is there any missing value in User? True Is there any missing value in Action 2? True Is there any missing value in Action 3? True Is there any missing value in Action 4? True Is there any missing value in Comment? False Is there any missing value in Product? False 由上述简单的分析可知，用户表及行为表中均存在空值记录，而评论表和商品表则不存在，但是结合之前的数据背景分析商品表中存在属性未知的情况，后续也需要针对分析，进一步的我们看看用户表和行为表中的空值情况 123456789def empty_detail(f_path, f_name): df_file = pd.read_csv(f_path) print 'empty info in detail of &#123;0&#125;:'.format(f_name) print pd.isnull(df_file).any()empty_detail(USER_FILE, 'User')empty_detail(ACTION_201602_FILE, 'Action 2')empty_detail(ACTION_201603_FILE, 'Action 3')empty_detail(ACTION_201604_FILE, 'Action 4') empty info in detail of User: user_id False age True sex True user_lv_cd False user_reg_tm True dtype: bool empty info in detail of Action 2: user_id False sku_id False time False model_id True type False cate False brand False dtype: bool empty info in detail of Action 3: user_id False sku_id False time False model_id True type False cate False brand False dtype: bool empty info in detail of Action 4: user_id False sku_id False time False model_id True type False cate False brand False dtype: bool 上面简单的输出了下存在空值的文件中具体哪些列存在空值(True)，结果如下 User age sex user_reg_tm Action model_id 接下来具体看看各文件中的空值情况： 123456789101112def empty_records(f_path, f_name, col_name): df_file = pd.read_csv(f_path) missing = df_file[col_name].isnull().sum().sum() print 'No. of missing &#123;0&#125; in &#123;1&#125; is &#123;2&#125;'.format(col_name, f_name, missing) print 'percent: ', missing * 1.0 / df_file.shape[0]empty_records(USER_FILE, 'User', 'age')empty_records(USER_FILE, 'User', 'sex')empty_records(USER_FILE, 'User', 'user_reg_tm')empty_records(ACTION_201602_FILE, 'Action 2', 'model_id')empty_records(ACTION_201603_FILE, 'Action 3', 'model_id')empty_records(ACTION_201604_FILE, 'Action 4', 'model_id') No. of missing age in User is 3 percent: 2.84843478509e-05 No. of missing sex in User is 3 percent: 2.84843478509e-05 No. of missing user_reg_tm in User is 3 percent: 2.84843478509e-05 No. of missing model_id in Action 2 is 4959617 percent: 0.431818363867 No. of missing model_id in Action 3 is 10553261 percent: 0.4072043169 No. of missing model_id in Action 4 is 5143018 percent: 0.38962452388 对比下数据集的记录数： 文件 文件说明 记录数 1. JData_User.csv 用户数据集 105,321个用户 2. JData_Comment.csv 商品评论 558,552条记录 3. JData_Product.csv 预测商品集合 24,187条记录 4. JData_Action_201602.csv 2月份行为交互记录 11,485,424条记录 5. JData_Action_201603.csv 3月份行为交互记录 25,916,378条记录 6. JData_Action_201604.csv 4月份行为交互记录 13,199,934条记录 两相对比结合前面输出的情况，针对不同数据进行不同处理 用户文件 age,sex:先填充为对应的未知状态（-1|2），后续作为未知状态的值进一步分析和处理 user_reg_tm:暂时不做处理 行为文件 model_id涉及数目接近一半，而且当前针对该特征没有很好的处理方法，待定 123user = pd.read_csv(USER_FILE)user['age'].fillna('-1', inplace=True)user['sex'].fillna(2, inplace=True) 1print pd.isnull(user).any() user_id False age False sex False user_lv_cd False user_reg_tm True dtype: bool 12nan_reg_tm = user[user['user_reg_tm'].isnull()]print nan_reg_tm user_id age sex user_lv_cd user_reg_tm 34072 234073 -1 2.0 1 NaN 38905 238906 -1 2.0 1 NaN 67704 267705 -1 2.0 1 NaN 123print len(user['age'].unique())print len(user['sex'].unique())print len(user['user_lv_cd'].unique()) 7 3 5 1prod = pd.read_csv(PRODUCT_FILE) 12345print len(prod['a1'].unique())print len(prod['a2'].unique())print len(prod['a3'].unique())# print len(prod['a2'].unique())print len(prod['brand'].unique()) 4 3 3 102 未知记录接下来看看各个文件中的未知记录占的比重 1234print 'No. of unknown age user: &#123;0&#125; and the percent: &#123;1&#125; '.format(user[user['age']=='-1'].shape[0], user[user['age']=='-1'].shape[0]*1.0/user.shape[0])print 'No. of unknown sex user: &#123;0&#125; and the percent: &#123;1&#125; '.format(user[user['sex']==2].shape[0], user[user['sex']==2].shape[0]*1.0/user.shape[0]) No. of unknown age user: 14415 and the percent: 0.136867291423 No. of unknown sex user: 54738 and the percent: 0.519725410887 123456789def unknown_records(f_path, f_name, col_name): df_file = pd.read_csv(f_path) missing = df_file[df_file[col_name]==-1].shape[0] print 'No. of unknown &#123;0&#125; in &#123;1&#125; is &#123;2&#125;'.format(col_name, f_name, missing) print 'percent: ', missing * 1.0 / df_file.shape[0] unknown_records(PRODUCT_FILE, 'Product', 'a1')unknown_records(PRODUCT_FILE, 'Product', 'a2')unknown_records(PRODUCT_FILE, 'Product', 'a3') No. of unknown a1 in Product is 1701 percent: 0.0703270351842 No. of unknown a2 in Product is 4050 percent: 0.167445321867 No. of unknown a3 in Product is 3815 percent: 0.157729358746 小结一下 空值部分对3条用户的sex,age填充为未知值,注册时间不作处理，此外行为数据部分model_id待定: 43.2%,40.7%,39.0% 未知值部分，用户age存在部分未知值: 13.7%，sex存在大量保密情况(超过一半) 52.0% 商品中各个属性存在部分未知的情况，a1&lt;a3&lt;a2，分别为： 7.0%,16.7%,15.8% 数据一致性验证首先检查JData_User中的用户和JData_Action中的用户是否一致，保证行为数据中的所产生的行为均由用户数据中的用户产生（但是可能存在用户在行为数据中无行为） 思路：利用pd.Merge连接sku 和 Action中的sku, 观察Action中的数据是否减少 1234567891011def user_action_check(): df_user = pd.read_csv('data/JData_User.csv') df_sku = df_user.ix[:,'user_id'].to_frame() df_month2 = pd.read_csv('data/JData_Action_201602.csv') print 'Is action of Feb. from User file? ', len(df_month2) == len(pd.merge(df_sku,df_month2)) df_month3 = pd.read_csv('data/JData_Action_201603.csv') print 'Is action of Mar. from User file? ', len(df_month3) == len(pd.merge(df_sku,df_month3)) df_month4 = pd.read_csv('data/JData_Action_201604.csv') print 'Is action of Apr. from User file? ', len(df_month4) == len(pd.merge(df_sku,df_month4))user_action_check() Is action of Feb. from User file? True Is action of Mar. from User file? True Is action of Apr. from User file? True 结论： User数据集中的用户和交互行为数据集中的用户完全一致 根据merge前后的数据量比对，能保证Action中的用户ID是User中的ID的子集 重复记录分析除去各个数据文件中完全重复的记录,结果证明线上成绩反而大幅下降，可能解释是重复数据是有意义的，比如用户同时购买多件商品，同时添加多个数量的商品到购物车等… 1234567891011def deduplicate(filepath, filename, newpath): df_file = pd.read_csv(filepath) before = df_file.shape[0] df_file.drop_duplicates(inplace=True) after = df_file.shape[0] n_dup = before-after print 'No. of duplicate records for ' + filename + ' is: ' + str(n_dup) if n_dup != 0: df_file.to_csv(newpath, index=None) else: print 'no duplicate records in ' + filename 123456# deduplicate('data/JData_Action_201602.csv', 'Feb. action', 'data/JData_Action_201602_dedup.csv')deduplicate('data/JData_Action_201603.csv', 'Mar. action', 'data/JData_Action_201603_dedup.csv')deduplicate('data/JData_Action_201604.csv', 'Feb. action', 'data/JData_Action_201604_dedup.csv')deduplicate('data/JData_Comment.csv', 'Comment', 'data/JData_Comment_dedup.csv')deduplicate('data/JData_Product.csv', 'Product', 'data/JData_Product_dedup.csv')deduplicate('data/JData_User.csv', 'User', 'data/JData_User_dedup.csv') No. of duplicate records for Mar. action is: 7085038 No. of duplicate records for Feb. action is: 3672710 No. of duplicate records for Comment is: 0 no duplicate records in Comment No. of duplicate records for Product is: 0 no duplicate records in Product No. of duplicate records for User is: 0 no duplicate records in User 123IsDuplicated = df_month.duplicated() df_d=df_month[IsDuplicated]df_d.groupby('type').count() #发现重复数据大多数都是由于浏览（1），或者点击(6)产生 type user_id sku_id time model_id cate brand 1 2176378 2176378 2176378 0 2176378 2176378 2 636 636 636 0 636 636 3 1464 1464 1464 0 1464 1464 4 37 37 37 0 37 37 5 1981 1981 1981 0 1981 1981 6 575597 575597 575597 545054 575597 575597 检查是否存在注册时间在2016年-4月-15号之后的用户1234import pandas as pddf_user = pd.read_csv('data\\JData_User.csv',encoding='gbk')df_user['user_reg_tm']=pd.to_datetime(df_user['user_reg_tm'])df_user.ix[df_user.user_reg_tm &gt;= '2016-4-15'] - user_id age sex user_lv_cd user_reg_tm 7457 207458 -1 2.0 1 2016-04-15 7463 207464 26-35岁 2.0 2 2016-04-15 7467 207468 36-45岁 2.0 3 2016-04-15 7472 207473 -1 2.0 1 2016-04-15 7482 207483 26-35岁 2.0 3 2016-04-15 7492 207493 16-25岁 2.0 3 2016-04-15 7493 207494 16-25岁 2.0 3 2016-04-15 7503 207504 16-25岁 2.0 4 2016-04-15 7510 207511 46-55岁 2.0 5 2016-04-15 7512 207513 -1 2.0 1 2016-04-15 7518 207519 26-35岁 2.0 2 2016-04-15 7521 207522 26-35岁 0.0 3 2016-04-15 7525 207526 -1 2.0 3 2016-04-15 7533 207534 -1 2.0 1 2016-04-15 7543 207544 26-35岁 2.0 3 2016-04-15 7544 207545 -1 2.0 1 2016-04-15 7551 207552 26-35岁 2.0 3 2016-04-15 7553 207554 16-25岁 2.0 4 2016-04-15 8545 208546 16-25岁 0.0 2 2016-04-29 9394 209395 16-25岁 1.0 2 2016-05-11 10362 210363 56岁以上 2.0 2 2016-05-24 10367 210368 -1 2.0 1 2016-05-24 11019 211020 36-45岁 2.0 3 2016-06-06 12014 212015 36-45岁 2.0 2 2016-07-05 13850 213851 26-35岁 2.0 3 2016-09-11 14542 214543 -1 2.0 1 2016-10-05 16746 216747 16-25岁 2.0 1 2016-11-25 由于注册时间是京东系统错误造成，如果行为数据中没有在4月15号之后的数据的话，那么说明这些用户还是正常用户，并不需要删除。 123df_month = pd.read_csv('data\\JData_Action_201604.csv')df_month['time'] = pd.to_datetime(df_month['time'])df_month.ix[df_month.time &gt;= '2016-4-16'] - user_id sku_id time model_id type cate brand 结论：说明用户没有异常操作数据，所以这一批用户不删除 行为数据中的user_id为浮点型，进行INT类型转换12345678910111213import pandas as pddf_month = pd.read_csv('data\\JData_Action_201602.csv')df_month['user_id'] = df_month['user_id'].apply(lambda x:int(x))print df_month['user_id'].dtypedf_month.to_csv('data\\JData_Action_201602.csv',index=None)df_month = pd.read_csv('data\\JData_Action_201603.csv')df_month['user_id'] = df_month['user_id'].apply(lambda x:int(x))print df_month['user_id'].dtypedf_month.to_csv('data\\JData_Action_201603.csv',index=None)df_month = pd.read_csv('data\\JData_Action_201604.csv')df_month['user_id'] = df_month['user_id'].apply(lambda x:int(x))print df_month['user_id'].dtypedf_month.to_csv('data\\JData_Action_201604.csv',index=None) int64 int64 int64 按照星期对用户购买行为进行分析12345678910111213141516171819# 提取购买(type=4)的行为数据def get_from_action_data(fname, chunk_size=100000): reader = pd.read_csv(fname, header=0, iterator=True) chunks = [] loop = True while loop: try: chunk = reader.get_chunk(chunk_size)[ [\"user_id\", \"sku_id\", \"type\", \"time\"]] chunks.append(chunk) except StopIteration: loop = False print(\"Iteration is stopped\") df_ac = pd.concat(chunks, ignore_index=True) # type=4,为购买 df_ac = df_ac[df_ac['type'] == 4] return df_ac[[\"user_id\", \"sku_id\", \"time\"]] 12345df_ac = []df_ac.append(get_from_action_data(fname=ACTION_201602_FILE))df_ac.append(get_from_action_data(fname=ACTION_201603_FILE))df_ac.append(get_from_action_data(fname=ACTION_201604_FILE))df_ac = pd.concat(df_ac, ignore_index=True) Iteration is stopped Iteration is stopped Iteration is stopped 1print(df_ac.dtypes) user_id int64 sku_id int64 time object dtype: object 12345# 将time字段转换为datetime类型df_ac['time'] = pd.to_datetime(df_ac['time'])# 使用lambda匿名函数将时间time转换为星期(周一为1, 周日为７)df_ac['time'] = df_ac['time'].apply(lambda x: x.weekday() + 1) 1234# 周一到周日每天购买用户个数df_user = df_ac.groupby('time')['user_id'].nunique()df_user = df_user.to_frame().reset_index()df_user.columns = ['weekday', 'user_num'] 1234# 周一到周日每天购买商品个数df_item = df_ac.groupby('time')['sku_id'].nunique()df_item = df_item.to_frame().reset_index()df_item.columns = ['weekday', 'item_num'] 1234# 周一到周日每天购买记录个数df_ui = df_ac.groupby('time', as_index=False).size()df_ui = df_ui.to_frame().reset_index()df_ui.columns = ['weekday', 'user_item_num'] 123456789101112131415161718# 条形宽度bar_width = 0.2# 透明度opacity = 0.4plt.bar(df_user['weekday'], df_user['user_num'], bar_width, alpha=opacity, color='c', label='user')plt.bar(df_item['weekday']+bar_width, df_item['item_num'], bar_width, alpha=opacity, color='g', label='item')plt.bar(df_ui['weekday']+bar_width*2, df_ui['user_item_num'], bar_width, alpha=opacity, color='m', label='user_item')plt.xlabel('weekday')plt.ylabel('number')plt.title('A Week Purchase Table')plt.xticks(df_user['weekday'] + bar_width * 3 / 2., (1,2,3,4,5,6,7))plt.tight_layout() plt.legend(prop=&#123;'size':10&#125;) 分析：周六，周日购买量较少 一个月中各天购买量2016年2月1234df_ac = get_from_action_data(fname=ACTION_201602_FILE)# 将time字段转换为datetime类型并使用lambda匿名函数将时间time转换为天df_ac['time'] = pd.to_datetime(df_ac['time']).apply(lambda x: x.day) Iteration is stopped 1234567891011df_user = df_ac.groupby('time')['user_id'].nunique()df_user = df_user.to_frame().reset_index()df_user.columns = ['day', 'user_num']df_item = df_ac.groupby('time')['sku_id'].nunique()df_item = df_item.to_frame().reset_index()df_item.columns = ['day', 'item_num']df_ui = df_ac.groupby('time', as_index=False).size()df_ui = df_ui.to_frame().reset_index()df_ui.columns = ['day', 'user_item_num'] 1234567891011121314151617181920212223# 条形宽度bar_width = 0.2# 透明度opacity = 0.4# 天数day_range = range(1,len(df_user['day']) + 1, 1)# 设置图片大小plt.figure(figsize=(14,10))plt.bar(df_user['day'], df_user['user_num'], bar_width, alpha=opacity, color='c', label='user')plt.bar(df_item['day']+bar_width, df_item['item_num'], bar_width, alpha=opacity, color='g', label='item')plt.bar(df_ui['day']+bar_width*2, df_ui['user_item_num'], bar_width, alpha=opacity, color='m', label='user_item')plt.xlabel('day')plt.ylabel('number')plt.title('February Purchase Table')plt.xticks(df_user['day'] + bar_width * 3 / 2., day_range)# plt.ylim(0, 80)plt.tight_layout() plt.legend(prop=&#123;'size':9&#125;) 分析： 2月份5,6,7,8,9,10 这几天购买量非常少，原因可能是中国农历春节，快递不营业 2016年3月1234df_ac = get_from_action_data(fname=ACTION_201603_FILE)# 将time字段转换为datetime类型并使用lambda匿名函数将时间time转换为天df_ac['time'] = pd.to_datetime(df_ac['time']).apply(lambda x: x.day) Iteration is stopped 1234567891011df_user = df_ac.groupby('time')['user_id'].nunique()df_user = df_user.to_frame().reset_index()df_user.columns = ['day', 'user_num']df_item = df_ac.groupby('time')['sku_id'].nunique()df_item = df_item.to_frame().reset_index()df_item.columns = ['day', 'item_num']df_ui = df_ac.groupby('time', as_index=False).size()df_ui = df_ui.to_frame().reset_index()df_ui.columns = ['day', 'user_item_num'] 1234567891011121314151617181920212223# 条形宽度bar_width = 0.2# 透明度opacity = 0.4# 天数day_range = range(1,len(df_user['day']) + 1, 1)# 设置图片大小plt.figure(figsize=(14,10))plt.bar(df_user['day'], df_user['user_num'], bar_width, alpha=opacity, color='c', label='user')plt.bar(df_item['day']+bar_width, df_item['item_num'], bar_width, alpha=opacity, color='g', label='item')plt.bar(df_ui['day']+bar_width*2, df_ui['user_item_num'], bar_width, alpha=opacity, color='m', label='user_item')plt.xlabel('day')plt.ylabel('number')plt.title('March Purchase Table')plt.xticks(df_user['day'] + bar_width * 3 / 2., day_range)# plt.ylim(0, 80)plt.tight_layout() plt.legend(prop=&#123;'size':9&#125;) 分析：3月份14,15,16不知名节日，造成购物量剧增，总体来看，购物记录多于2月份 2016年4月1234df_ac = get_from_action_data(fname=ACTION_201604_FILE)# 将time字段转换为datetime类型并使用lambda匿名函数将时间time转换为天df_ac['time'] = pd.to_datetime(df_ac['time']).apply(lambda x: x.day) Iteration is stopped 1234567891011df_user = df_ac.groupby('time')['user_id'].nunique()df_user = df_user.to_frame().reset_index()df_user.columns = ['day', 'user_num']df_item = df_ac.groupby('time')['sku_id'].nunique()df_item = df_item.to_frame().reset_index()df_item.columns = ['day', 'item_num']df_ui = df_ac.groupby('time', as_index=False).size()df_ui = df_ui.to_frame().reset_index()df_ui.columns = ['day', 'user_item_num'] 1234567891011121314151617181920212223# 条形宽度bar_width = 0.2# 透明度opacity = 0.4# 天数day_range = range(1,len(df_user['day']) + 1, 1)# 设置图片大小plt.figure(figsize=(14,10))plt.bar(df_user['day'], df_user['user_num'], bar_width, alpha=opacity, color='c', label='user')plt.bar(df_item['day']+bar_width, df_item['item_num'], bar_width, alpha=opacity, color='g', label='item')plt.bar(df_ui['day']+bar_width*2, df_ui['user_item_num'], bar_width, alpha=opacity, color='m', label='user_item')plt.xlabel('day')plt.ylabel('number')plt.title('April Purchase Table')plt.xticks(df_user['day'] + bar_width * 3 / 2., day_range)# plt.ylim(0, 80)plt.tight_layout() plt.legend(prop=&#123;'size':9&#125;) 分析：似乎每个月中旬都有较强的购物欲望？ 商品分类别销售统计周一到周日各商品类别销售情况12345678910111213141516171819# 从行为记录中提取商品类别数据def get_from_action_data(fname, chunk_size=100000): reader = pd.read_csv(fname, header=0, iterator=True) chunks = [] loop = True while loop: try: chunk = reader.get_chunk(chunk_size)[ [\"cate\", \"brand\", \"type\", \"time\"]] chunks.append(chunk) except StopIteration: loop = False print(\"Iteration is stopped\") df_ac = pd.concat(chunks, ignore_index=True) # type=4,为购买 df_ac = df_ac[df_ac['type'] == 4] return df_ac[[\"cate\", \"brand\", \"type\", \"time\"]] 12345df_ac = []df_ac.append(get_from_action_data(fname=ACTION_201602_FILE))df_ac.append(get_from_action_data(fname=ACTION_201603_FILE))df_ac.append(get_from_action_data(fname=ACTION_201604_FILE))df_ac = pd.concat(df_ac, ignore_index=True) Iteration is stopped Iteration is stopped Iteration is stopped 12345# 将time字段转换为datetime类型df_ac['time'] = pd.to_datetime(df_ac['time'])# 使用lambda匿名函数将时间time转换为星期(周一为1, 周日为７)df_ac['time'] = df_ac['time'].apply(lambda x: x.weekday() + 1) 12# 观察有几个类别商品df_ac.groupby(df_ac['cate']).count() cate cate brand type time 4 9326 9326 9326 9326 5 8138 8138 8138 8138 6 6982 6982 6982 6982 7 6214 6214 6214 6214 8 13281 13281 13281 13281 9 4104 4104 4104 4104 10 189 189 189 189 11 18 18 18 18 1234# 周一到周日每天购买商品类别4的数量统计df_product = df_ac['brand'].groupby([df_ac['time'],df_ac['cate']]).count()df_product=df_product.unstack()df_product.plot(kind='bar',title='Cate Purchase Table in a Week',figsize=(14,10)) 分析：星期二买类别8的最多，星期天最少。 每月各类商品销售情况（只关注商品8）2016年2，3，4月123456789101112df_ac2 = get_from_action_data(fname=ACTION_201602_FILE)# 将time字段转换为datetime类型并使用lambda匿名函数将时间time转换为天df_ac2['time'] = pd.to_datetime(df_ac2['time']).apply(lambda x: x.day)df_ac3 = get_from_action_data(fname=ACTION_201603_FILE)# 将time字段转换为datetime类型并使用lambda匿名函数将时间time转换为天df_ac3['time'] = pd.to_datetime(df_ac3['time']).apply(lambda x: x.day)df_ac4 = get_from_action_data(fname=ACTION_201604_FILE)# 将time字段转换为datetime类型并使用lambda匿名函数将时间time转换为天df_ac4['time'] = pd.to_datetime(df_ac4['time']).apply(lambda x: x.day) 1234567891011121314dc_cate2 = df_ac2[df_ac2['cate']==8]dc_cate2 = dc_cate2['brand'].groupby(dc_cate2['time']).count()dc_cate2 = dc_cate2.to_frame().reset_index()dc_cate2.columns = ['day', 'product_num']dc_cate3 = df_ac3[df_ac3['cate']==8]dc_cate3 = dc_cate3['brand'].groupby(dc_cate3['time']).count()dc_cate3 = dc_cate3.to_frame().reset_index()dc_cate3.columns = ['day', 'product_num']dc_cate4 = df_ac4[df_ac4['cate']==8]dc_cate4 = dc_cate4['brand'].groupby(dc_cate4['time']).count()dc_cate4 = dc_cate4.to_frame().reset_index()dc_cate4.columns = ['day', 'product_num'] 1234567891011121314151617181920212223# 条形宽度bar_width = 0.2# 透明度opacity = 0.4# 天数day_range = range(1,len(dc_cate3['day']) + 1, 1)# 设置图片大小plt.figure(figsize=(14,10))plt.bar(dc_cate2['day'], dc_cate2['product_num'], bar_width, alpha=opacity, color='c', label='February')plt.bar(dc_cate3['day']+bar_width, dc_cate3['product_num'], bar_width, alpha=opacity, color='g', label='March')plt.bar(dc_cate4['day']+bar_width*2, dc_cate4['product_num'], bar_width, alpha=opacity, color='m', label='April')plt.xlabel('day')plt.ylabel('number')plt.title('Cate-8 Purchase Table')plt.xticks(dc_cate3['day'] + bar_width * 3 / 2., day_range)# plt.ylim(0, 80)plt.tight_layout() plt.legend(prop=&#123;'size':9&#125;) 分析：2月份对类别8商品的购买普遍偏低，3，4月份普遍偏高，3月15日购买极其多！可以对比3月份的销售记录，发现类别8将近占了3月15日总销售的一半！同时发现，3,4月份类别8销售记录在前半个月特别相似，除了4月8号，9号和3月15号。 特征工程1234567891011121314151617181920212223242526272829303132333435comment_date = [ \"2016-02-01\", \"2016-02-08\", \"2016-02-15\", \"2016-02-22\", \"2016-02-29\", \"2016-03-07\", \"2016-03-14\", \"2016-03-21\", \"2016-03-28\", \"2016-04-04\", \"2016-04-11\", \"2016-04-15\"]def get_actions_1(): action = pd.read_csv(action_1_path) return actiondef get_actions_2(): action2 = pd.read_csv(action_2_path) return action2def get_actions_3(): action3 = pd.read_csv(action_3_path) return action3# 读取并拼接所有行为记录文件def get_all_action(): action_1 = get_actions_1() action_2 = get_actions_2() action_3 = get_actions_3() actions = pd.concat([action_1, action_2, action_3]) # type: pd.DataFrame# actions = pd.read_csv(action_path) return actions# 获取某个时间段的行为记录def get_actions(start_date, end_date, all_actions): \"\"\" :param start_date: :param end_date: :return: actions: pd.Dataframe \"\"\" actions = all_actions[(all_actions.time &gt;= start_date) &amp; (all_actions.time &lt; end_date)].copy() return actions 用户特征用户基本特征获取基本的用户特征，基于用户本身属性多为类别特征的特点，对age,sex,usr_lv_cd进行独热编码操作，对于用户注册时间暂时不处理 123456789101112131415161718from sklearn import preprocessingdef get_basic_user_feat(): # 针对年龄的中文字符问题处理，首先是读入的时候编码，填充空值，然后将其数值化，最后独热编码，此外对于sex也进行了数值类型转换 user = pd.read_csv(user_path, encoding='gbk')# user['age'].fillna('-1', inplace=True)# user['sex'].fillna(2, inplace=True) user['sex'] = user['sex'].astype(int) user['age'] = user['age'].astype(unicode) le = preprocessing.LabelEncoder() age_df = le.fit_transform(user['age'])# print list(le.classes_) age_df = pd.get_dummies(age_df, prefix='age') sex_df = pd.get_dummies(user['sex'], prefix='sex') user_lv_df = pd.get_dummies(user['user_lv_cd'], prefix='user_lv_cd') user = pd.concat([user['user_id'], age_df, sex_df, user_lv_df], axis=1) return user 商品特征商品基本特征根据商品文件获取基本的特征，针对属性a1,a2,a3进行独热编码，商品类别和品牌直接作为特征 1234567def get_basic_product_feat(): product = pd.read_csv(product_path) attr1_df = pd.get_dummies(product[\"a1\"], prefix=\"a1\") attr2_df = pd.get_dummies(product[\"a2\"], prefix=\"a2\") attr3_df = pd.get_dummies(product[\"a3\"], prefix=\"a3\") product = pd.concat([product[['sku_id', 'cate', 'brand']], attr1_df, attr2_df, attr3_df], axis=1) return product 评论特征 分时间段 对评论数进行独热编码 12345678910111213141516171819202122def get_comments_product_feat(end_date): comments = pd.read_csv(comment_path) comment_date_end = end_date comment_date_begin = comment_date[0] for date in reversed(comment_date): if date &lt; comment_date_end: comment_date_begin = date break comments = comments[comments.dt==comment_date_begin] df = pd.get_dummies(comments['comment_num'], prefix='comment_num') # 为了防止某个时间段不具备评论数为0的情况（测试集出现过这种情况） for i in range(0, 5): if 'comment_num_' + str(i) not in df.columns: df['comment_num_' + str(i)] = 0 df = df[['comment_num_0', 'comment_num_1', 'comment_num_2', 'comment_num_3', 'comment_num_4']] comments = pd.concat([comments, df], axis=1) # type: pd.DataFrame #del comments['dt'] #del comments['comment_num'] comments = comments[['sku_id', 'has_bad_comment', 'bad_comment_rate','comment_num_0', 'comment_num_1', 'comment_num_2', 'comment_num_3', 'comment_num_4']] return comments 行为特征 分时间段 对行为类别进行独热编码 分别按照用户-类别行为分组和用户-类别-商品行为分组统计，然后计算 用户对同类别下其他商品的行为计数 针对用户对同类别下目标商品的行为计数与该时间段的行为均值作差 12345678910111213141516171819202122232425262728293031323334def get_action_feat(start_date, end_date, all_actions, i): actions = get_actions(start_date, end_date, all_actions) actions = actions[['user_id', 'sku_id', 'cate','type']] # 不同时间累积的行为计数（3,5,7,10,15,21,30） df = pd.get_dummies(actions['type'], prefix='action_before_%s' %i) before_date = 'action_before_%s' %i actions = pd.concat([actions, df], axis=1) # type: pd.DataFrame # 分组统计，用户-类别-商品,不同用户对不同类别下商品的行为计数 actions = actions.groupby(['user_id', 'sku_id','cate'], as_index=False).sum() # 分组统计，用户-类别，不同用户对不同商品类别的行为计数 user_cate = actions.groupby(['user_id','cate'], as_index=False).sum() del user_cate['sku_id'] del user_cate['type'] actions = pd.merge(actions, user_cate, how='left', on=['user_id','cate']) #本类别下其他商品点击量 # 前述两种分组含有相同名称的不同行为的计数，系统会自动针对名称调整添加后缀,x,y，所以这里作差统计的是同一类别下其他商品的行为计数 actions[before_date+'_1_y'] = actions[before_date+'_1_y'] - actions[before_date+'_1_x'] actions[before_date+'_2_y'] = actions[before_date+'_2_y'] - actions[before_date+'_2_x'] actions[before_date+'_3_y'] = actions[before_date+'_3_y'] - actions[before_date+'_3_x'] actions[before_date+'_4_y'] = actions[before_date+'_4_y'] - actions[before_date+'_4_x'] actions[before_date+'_5_y'] = actions[before_date+'_5_y'] - actions[before_date+'_5_x'] actions[before_date+'_6_y'] = actions[before_date+'_6_y'] - actions[before_date+'_6_x'] # 统计用户对不同类别下商品计数与该类别下商品行为计数均值（对时间）的差值 actions[before_date+'minus_mean_1'] = actions[before_date+'_1_x'] - (actions[before_date+'_1_x']/i) actions[before_date+'minus_mean_2'] = actions[before_date+'_2_x'] - (actions[before_date+'_2_x']/i) actions[before_date+'minus_mean_3'] = actions[before_date+'_3_x'] - (actions[before_date+'_3_x']/i) actions[before_date+'minus_mean_4'] = actions[before_date+'_4_x'] - (actions[before_date+'_4_x']/i) actions[before_date+'minus_mean_5'] = actions[before_date+'_5_x'] - (actions[before_date+'_5_x']/i) actions[before_date+'minus_mean_6'] = actions[before_date+'_6_x'] - (actions[before_date+'_6_x']/i) del actions['type'] # 保留cate特征# del actions['cate'] return actions 用户-行为累积用户特征 分时间段 用户不同行为的 购买转化率 均值 标准差 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384def get_accumulate_user_feat(end_date, all_actions, day): start_date = datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=day) start_date = start_date.strftime('%Y-%m-%d') before_date = 'user_action_%s' % day feature = [ 'user_id', before_date + '_1', before_date + '_2', before_date + '_3', before_date + '_4', before_date + '_5', before_date + '_6', before_date + '_1_ratio', before_date + '_2_ratio', before_date + '_3_ratio', before_date + '_5_ratio', before_date + '_6_ratio', before_date + '_1_mean', before_date + '_2_mean', before_date + '_3_mean', before_date + '_4_mean', before_date + '_5_mean', before_date + '_6_mean', before_date + '_1_std', before_date + '_2_std', before_date + '_3_std', before_date + '_4_std', before_date + '_5_std', before_date + '_6_std' ] actions = get_actions(start_date, end_date, all_actions) df = pd.get_dummies(actions['type'], prefix=before_date)# actions['date'] = pd.to_datetime(actions['time']).apply(lambda x: x.date()) actions = pd.concat([actions[['user_id', 'date']], df], axis=1) # 分组统计，用户不同日期的行为计算标准差 actions_date = actions.groupby(['user_id', 'date']).sum() actions_date = actions_date.unstack() actions_date.fillna(0, inplace=True) action_1 = np.std(actions_date[before_date + '_1'], axis=1) action_1 = action_1.to_frame() action_1.columns = [before_date + '_1_std'] action_2 = np.std(actions_date[before_date + '_2'], axis=1) action_2 = action_2.to_frame() action_2.columns = [before_date + '_2_std'] action_3 = np.std(actions_date[before_date + '_3'], axis=1) action_3 = action_3.to_frame() action_3.columns = [before_date + '_3_std'] action_4 = np.std(actions_date[before_date + '_4'], axis=1) action_4 = action_4.to_frame() action_4.columns = [before_date + '_4_std'] action_5 = np.std(actions_date[before_date + '_5'], axis=1) action_5 = action_5.to_frame() action_5.columns = [before_date + '_5_std'] action_6 = np.std(actions_date[before_date + '_6'], axis=1) action_6 = action_6.to_frame() action_6.columns = [before_date + '_6_std'] actions_date = pd.concat( [action_1, action_2, action_3, action_4, action_5, action_6], axis=1) actions_date['user_id'] = actions_date.index # 分组统计，按用户分组，统计用户各项行为的转化率、均值 actions = actions.groupby(['user_id'], as_index=False).sum()# days_interal = (datetime.strptime(end_date, '%Y-%m-%d') -# datetime.strptime(start_date, '%Y-%m-%d')).days # 转化率# actions[before_date + '_1_ratio'] = actions[before_date +# '_4'] / actions[before_date +# '_1']# actions[before_date + '_2_ratio'] = actions[before_date +# '_4'] / actions[before_date +# '_2']# actions[before_date + '_3_ratio'] = actions[before_date +# '_4'] / actions[before_date +# '_3']# actions[before_date + '_5_ratio'] = actions[before_date +# '_4'] / actions[before_date +# '_5']# actions[before_date + '_6_ratio'] = actions[before_date +# '_4'] / actions[before_date +# '_6'] actions[before_date + '_1_ratio'] = np.log(1 + actions[before_date + '_4']) - np.log(1 + actions[before_date +'_1']) actions[before_date + '_2_ratio'] = np.log(1 + actions[before_date + '_4']) - np.log(1 + actions[before_date +'_2']) actions[before_date + '_3_ratio'] = np.log(1 + actions[before_date + '_4']) - np.log(1 + actions[before_date +'_3']) actions[before_date + '_5_ratio'] = np.log(1 + actions[before_date + '_4']) - np.log(1 + actions[before_date +'_5']) actions[before_date + '_6_ratio'] = np.log(1 + actions[before_date + '_4']) - np.log(1 + actions[before_date +'_6']) # 均值 actions[before_date + '_1_mean'] = actions[before_date + '_1'] / day actions[before_date + '_2_mean'] = actions[before_date + '_2'] / day actions[before_date + '_3_mean'] = actions[before_date + '_3'] / day actions[before_date + '_4_mean'] = actions[before_date + '_4'] / day actions[before_date + '_5_mean'] = actions[before_date + '_5'] / day actions[before_date + '_6_mean'] = actions[before_date + '_6'] / day actions = pd.merge(actions, actions_date, how='left', on='user_id') actions = actions[feature] return actions 用户近期行为特征在上面针对用户进行累积特征提取的基础上，分别提取用户近一个月、近三天的特征，然后提取一个月内用户除去最近三天的行为占据一个月的行为的比重 12345678910111213141516171819202122def get_recent_user_feat(end_date, all_actions): actions_3 = get_accumulate_user_feat(end_date, all_actions, 3) actions_30 = get_accumulate_user_feat(end_date, all_actions, 30) actions = pd.merge(actions_3, actions_30, how ='left', on='user_id') del actions_3 del actions_30 actions['recent_action1'] = np.log(1 + actions['user_action_30_1']-actions['user_action_3_1']) - np.log(1 + actions['user_action_30_1']) actions['recent_action2'] = np.log(1 + actions['user_action_30_2']-actions['user_action_3_2']) - np.log(1 + actions['user_action_30_2']) actions['recent_action3'] = np.log(1 + actions['user_action_30_3']-actions['user_action_3_3']) - np.log(1 + actions['user_action_30_3']) actions['recent_action4'] = np.log(1 + actions['user_action_30_4']-actions['user_action_3_4']) - np.log(1 + actions['user_action_30_4']) actions['recent_action5'] = np.log(1 + actions['user_action_30_5']-actions['user_action_3_5']) - np.log(1 + actions['user_action_30_5']) actions['recent_action6'] = np.log(1 + actions['user_action_30_6']-actions['user_action_3_6']) - np.log(1 + actions['user_action_30_6']) # actions['recent_action1'] = (actions['user_action_30_1']-actions['user_action_3_1'])/actions['user_action_30_1']# actions['recent_action2'] = (actions['user_action_30_2']-actions['user_action_3_2'])/actions['user_action_30_2']# actions['recent_action3'] = (actions['user_action_30_3']-actions['user_action_3_3'])/actions['user_action_30_3']# actions['recent_action4'] = (actions['user_action_30_4']-actions['user_action_3_4'])/actions['user_action_30_4']# actions['recent_action5'] = (actions['user_action_30_5']-actions['user_action_3_5'])/actions['user_action_30_5']# actions['recent_action6'] = (actions['user_action_30_6']-actions['user_action_3_6'])/actions['user_action_30_6'] return actions 用户对同类别下各种商品的行为 用户对各个类别的各项行为操作统计 用户对各个类别操作行为统计占对所有类别操作行为统计的比重 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114#增加了用户对不同类别的交互特征def get_user_cate_feature(start_date, end_date, all_actions): actions = get_actions(start_date, end_date, all_actions) actions = actions[['user_id', 'cate', 'type']] df = pd.get_dummies(actions['type'], prefix='type') actions = pd.concat([actions[['user_id', 'cate']], df], axis=1) actions = actions.groupby(['user_id', 'cate']).sum() actions = actions.unstack() actions.columns = actions.columns.swaplevel(0, 1) actions.columns = actions.columns.droplevel() actions.columns = [ 'cate_4_type1', 'cate_5_type1', 'cate_6_type1', 'cate_7_type1', 'cate_8_type1', 'cate_9_type1', 'cate_10_type1', 'cate_11_type1', 'cate_4_type2', 'cate_5_type2', 'cate_6_type2', 'cate_7_type2', 'cate_8_type2', 'cate_9_type2', 'cate_10_type2', 'cate_11_type2', 'cate_4_type3', 'cate_5_type3', 'cate_6_type3', 'cate_7_type3', 'cate_8_type3', 'cate_9_type3', 'cate_10_type3', 'cate_11_type3', 'cate_4_type4', 'cate_5_type4', 'cate_6_type4', 'cate_7_type4', 'cate_8_type4', 'cate_9_type4', 'cate_10_type4', 'cate_11_type4', 'cate_4_type5', 'cate_5_type5', 'cate_6_type5', 'cate_7_type5', 'cate_8_type5', 'cate_9_type5', 'cate_10_type5', 'cate_11_type5', 'cate_4_type6', 'cate_5_type6', 'cate_6_type6', 'cate_7_type6', 'cate_8_type6', 'cate_9_type6', 'cate_10_type6', 'cate_11_type6' ] actions = actions.fillna(0) actions['cate_action_sum'] = actions.sum(axis=1) actions['cate8_percentage'] = ( actions['cate_8_type1'] + actions['cate_8_type2'] + actions['cate_8_type3'] + actions['cate_8_type4'] + actions['cate_8_type5'] + actions['cate_8_type6'] ) / actions['cate_action_sum'] actions['cate4_percentage'] = ( actions['cate_4_type1'] + actions['cate_4_type2'] + actions['cate_4_type3'] + actions['cate_4_type4'] + actions['cate_4_type5'] + actions['cate_4_type6'] ) / actions['cate_action_sum'] actions['cate5_percentage'] = ( actions['cate_5_type1'] + actions['cate_5_type2'] + actions['cate_5_type3'] + actions['cate_5_type4'] + actions['cate_5_type5'] + actions['cate_5_type6'] ) / actions['cate_action_sum'] actions['cate6_percentage'] = ( actions['cate_6_type1'] + actions['cate_6_type2'] + actions['cate_6_type3'] + actions['cate_6_type4'] + actions['cate_6_type5'] + actions['cate_6_type6'] ) / actions['cate_action_sum'] actions['cate7_percentage'] = ( actions['cate_7_type1'] + actions['cate_7_type2'] + actions['cate_7_type3'] + actions['cate_7_type4'] + actions['cate_7_type5'] + actions['cate_7_type6'] ) / actions['cate_action_sum'] actions['cate9_percentage'] = ( actions['cate_9_type1'] + actions['cate_9_type2'] + actions['cate_9_type3'] + actions['cate_9_type4'] + actions['cate_9_type5'] + actions['cate_9_type6'] ) / actions['cate_action_sum'] actions['cate10_percentage'] = ( actions['cate_10_type1'] + actions['cate_10_type2'] + actions['cate_10_type3'] + actions['cate_10_type4'] + actions['cate_10_type5'] + actions['cate_10_type6'] ) / actions['cate_action_sum'] actions['cate11_percentage'] = ( actions['cate_11_type1'] + actions['cate_11_type2'] + actions['cate_11_type3'] + actions['cate_11_type4'] + actions['cate_11_type5'] + actions['cate_11_type6'] ) / actions['cate_action_sum'] actions['cate8_type1_percentage'] = np.log( 1 + actions['cate_8_type1']) - np.log( 1 + actions['cate_8_type1'] + actions['cate_4_type1'] + actions['cate_5_type1'] + actions['cate_6_type1'] + actions['cate_7_type1'] + actions['cate_9_type1'] + actions['cate_10_type1'] + actions['cate_11_type1']) actions['cate8_type2_percentage'] = np.log( 1 + actions['cate_8_type2']) - np.log( 1 + actions['cate_8_type2'] + actions['cate_4_type2'] + actions['cate_5_type2'] + actions['cate_6_type2'] + actions['cate_7_type2'] + actions['cate_9_type2'] + actions['cate_10_type2'] + actions['cate_11_type2']) actions['cate8_type3_percentage'] = np.log( 1 + actions['cate_8_type3']) - np.log( 1 + actions['cate_8_type3'] + actions['cate_4_type3'] + actions['cate_5_type3'] + actions['cate_6_type3'] + actions['cate_7_type3'] + actions['cate_9_type3'] + actions['cate_10_type3'] + actions['cate_11_type3']) actions['cate8_type4_percentage'] = np.log( 1 + actions['cate_8_type4']) - np.log( 1 + actions['cate_8_type4'] + actions['cate_4_type4'] + actions['cate_5_type4'] + actions['cate_6_type4'] + actions['cate_7_type4'] + actions['cate_9_type4'] + actions['cate_10_type4'] + actions['cate_11_type4']) actions['cate8_type5_percentage'] = np.log( 1 + actions['cate_8_type5']) - np.log( 1 + actions['cate_8_type5'] + actions['cate_4_type5'] + actions['cate_5_type5'] + actions['cate_6_type5'] + actions['cate_7_type5'] + actions['cate_9_type5'] + actions['cate_10_type5'] + actions['cate_11_type5']) actions['cate8_type6_percentage'] = np.log( 1 + actions['cate_8_type6']) - np.log( 1 + actions['cate_8_type6'] + actions['cate_4_type6'] + actions['cate_5_type6'] + actions['cate_6_type6'] + actions['cate_7_type6'] + actions['cate_9_type6'] + actions['cate_10_type6'] + actions['cate_11_type6']) actions['user_id'] = actions.index actions = actions[[ 'user_id', 'cate8_percentage', 'cate4_percentage', 'cate5_percentage', 'cate6_percentage', 'cate7_percentage', 'cate9_percentage', 'cate10_percentage', 'cate11_percentage', 'cate8_type1_percentage', 'cate8_type2_percentage', 'cate8_type3_percentage', 'cate8_type4_percentage', 'cate8_type5_percentage', 'cate8_type6_percentage' ]] return actions 商品-行为累积商品特征 分时间段 针对商品的不同行为的 购买转化率 均值 标准差 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879def get_accumulate_product_feat(start_date, end_date, all_actions): feature = [ 'sku_id', 'product_action_1', 'product_action_2', 'product_action_3', 'product_action_4', 'product_action_5', 'product_action_6', 'product_action_1_ratio', 'product_action_2_ratio', 'product_action_3_ratio', 'product_action_5_ratio', 'product_action_6_ratio', 'product_action_1_mean', 'product_action_2_mean', 'product_action_3_mean', 'product_action_4_mean', 'product_action_5_mean', 'product_action_6_mean', 'product_action_1_std', 'product_action_2_std', 'product_action_3_std', 'product_action_4_std', 'product_action_5_std', 'product_action_6_std' ] actions = get_actions(start_date, end_date, all_actions) df = pd.get_dummies(actions['type'], prefix='product_action') # 按照商品-日期分组，计算某个时间段该商品的各项行为的标准差# actions['date'] = pd.to_datetime(actions['time']).apply(lambda x: x.date()) actions = pd.concat([actions[['sku_id', 'date']], df], axis=1) actions_date = actions.groupby(['sku_id', 'date']).sum() actions_date = actions_date.unstack() actions_date.fillna(0, inplace=True) action_1 = np.std(actions_date['product_action_1'], axis=1) action_1 = action_1.to_frame() action_1.columns = ['product_action_1_std'] action_2 = np.std(actions_date['product_action_2'], axis=1) action_2 = action_2.to_frame() action_2.columns = ['product_action_2_std'] action_3 = np.std(actions_date['product_action_3'], axis=1) action_3 = action_3.to_frame() action_3.columns = ['product_action_3_std'] action_4 = np.std(actions_date['product_action_4'], axis=1) action_4 = action_4.to_frame() action_4.columns = ['product_action_4_std'] action_5 = np.std(actions_date['product_action_5'], axis=1) action_5 = action_5.to_frame() action_5.columns = ['product_action_5_std'] action_6 = np.std(actions_date['product_action_6'], axis=1) action_6 = action_6.to_frame() action_6.columns = ['product_action_6_std'] actions_date = pd.concat( [action_1, action_2, action_3, action_4, action_5, action_6], axis=1) actions_date['sku_id'] = actions_date.index actions = actions.groupby(['sku_id'], as_index=False).sum() days_interal = (datetime.strptime(end_date, '%Y-%m-%d') - datetime.strptime(start_date, '%Y-%m-%d')).days # 针对商品分组，计算购买转化率# actions['product_action_1_ratio'] = actions['product_action_4'] / actions[# 'product_action_1']# actions['product_action_2_ratio'] = actions['product_action_4'] / actions[# 'product_action_2']# actions['product_action_3_ratio'] = actions['product_action_4'] / actions[# 'product_action_3']# actions['product_action_5_ratio'] = actions['product_action_4'] / actions[# 'product_action_5']# actions['product_action_6_ratio'] = actions['product_action_4'] / actions[# 'product_action_6'] actions['product_action_1_ratio'] = np.log(1 + actions['product_action_4']) - np.log(1 + actions['product_action_1']) actions['product_action_2_ratio'] = np.log(1 + actions['product_action_4']) - np.log(1 + actions['product_action_2']) actions['product_action_3_ratio'] = np.log(1 + actions['product_action_4']) - np.log(1 + actions['product_action_3']) actions['product_action_5_ratio'] = np.log(1 + actions['product_action_4']) - np.log(1 + actions['product_action_5']) actions['product_action_6_ratio'] = np.log(1 + actions['product_action_4']) - np.log(1 + actions['product_action_6']) # 计算各种行为的均值 actions['product_action_1_mean'] = actions[ 'product_action_1'] / days_interal actions['product_action_2_mean'] = actions[ 'product_action_2'] / days_interal actions['product_action_3_mean'] = actions[ 'product_action_3'] / days_interal actions['product_action_4_mean'] = actions[ 'product_action_4'] / days_interal actions['product_action_5_mean'] = actions[ 'product_action_5'] / days_interal actions['product_action_6_mean'] = actions[ 'product_action_6'] / days_interal actions = pd.merge(actions, actions_date, how='left', on='sku_id') actions = actions[feature] return actions 类别特征分时间段下各个商品类别的 购买转化率 标准差 均值 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859def get_accumulate_cate_feat(start_date, end_date, all_actions): feature = ['cate','cate_action_1', 'cate_action_2', 'cate_action_3', 'cate_action_4', 'cate_action_5', 'cate_action_6', 'cate_action_1_ratio', 'cate_action_2_ratio', 'cate_action_3_ratio', 'cate_action_5_ratio', 'cate_action_6_ratio', 'cate_action_1_mean', 'cate_action_2_mean', 'cate_action_3_mean', 'cate_action_4_mean', 'cate_action_5_mean', 'cate_action_6_mean', 'cate_action_1_std', 'cate_action_2_std', 'cate_action_3_std', 'cate_action_4_std', 'cate_action_5_std', 'cate_action_6_std'] actions = get_actions(start_date, end_date, all_actions)# actions['date'] = pd.to_datetime(actions['time']).apply(lambda x: x.date()) df = pd.get_dummies(actions['type'], prefix='cate_action') actions = pd.concat([actions[['cate','date']], df], axis=1) # 按照类别-日期分组计算针对不同类别的各种行为某段时间的标准差 actions_date = actions.groupby(['cate','date']).sum() actions_date = actions_date.unstack() actions_date.fillna(0, inplace=True) action_1 = np.std(actions_date['cate_action_1'], axis=1) action_1 = action_1.to_frame() action_1.columns = ['cate_action_1_std'] action_2 = np.std(actions_date['cate_action_2'], axis=1) action_2 = action_2.to_frame() action_2.columns = ['cate_action_2_std'] action_3 = np.std(actions_date['cate_action_3'], axis=1) action_3 = action_3.to_frame() action_3.columns = ['cate_action_3_std'] action_4 = np.std(actions_date['cate_action_4'], axis=1) action_4 = action_4.to_frame() action_4.columns = ['cate_action_4_std'] action_5 = np.std(actions_date['cate_action_5'], axis=1) action_5 = action_5.to_frame() action_5.columns = ['cate_action_5_std'] action_6 = np.std(actions_date['cate_action_6'], axis=1) action_6 = action_6.to_frame() action_6.columns = ['cate_action_6_std'] actions_date = pd.concat([action_1, action_2, action_3, action_4, action_5, action_6], axis=1) actions_date['cate'] = actions_date.index # 按照类别分组，统计各个商品类别下行为的转化率 actions = actions.groupby(['cate'], as_index=False).sum() days_interal = (datetime.strptime(end_date, '%Y-%m-%d')-datetime.strptime(start_date, '%Y-%m-%d')).days # actions['cate_action_1_ratio'] = actions['cate_action_4'] / actions['cate_action_1']# actions['cate_action_2_ratio'] = actions['cate_action_4'] / actions['cate_action_2']# actions['cate_action_3_ratio'] = actions['cate_action_4'] / actions['cate_action_3']# actions['cate_action_5_ratio'] = actions['cate_action_4'] / actions['cate_action_5']# actions['cate_action_6_ratio'] = actions['cate_action_4'] / actions['cate_action_6'] actions['cate_action_1_ratio'] =(np.log(1 + actions['cate_action_4']) - np.log(1 + actions['cate_action_1'])) actions['cate_action_2_ratio'] =(np.log(1 + actions['cate_action_4']) - np.log(1 + actions['cate_action_2'])) actions['cate_action_3_ratio'] =(np.log(1 + actions['cate_action_4']) - np.log(1 + actions['cate_action_3'])) actions['cate_action_5_ratio'] =(np.log(1 + actions['cate_action_4']) - np.log(1 + actions['cate_action_5'])) actions['cate_action_6_ratio'] =(np.log(1 + actions['cate_action_4']) - np.log(1 + actions['cate_action_6'])) # 按照类别分组，统计各个商品类别下行为在一段时间的均值 actions['cate_action_1_mean'] = actions['cate_action_1'] / days_interal actions['cate_action_2_mean'] = actions['cate_action_2'] / days_interal actions['cate_action_3_mean'] = actions['cate_action_3'] / days_interal actions['cate_action_4_mean'] = actions['cate_action_4'] / days_interal actions['cate_action_5_mean'] = actions['cate_action_5'] / days_interal actions['cate_action_6_mean'] = actions['cate_action_6'] / days_interal actions = pd.merge(actions, actions_date, how ='left',on='cate') actions = actions[feature] return actions 构造训练集/验证集 标签,采用滑动窗口的方式，构造训练集的时候针对产生购买的行为标记为1 整合特征 12345678910def get_labels(start_date, end_date, all_actions): actions = get_actions(start_date, end_date, all_actions)# actions = actions[actions['type'] == 4] # 修改为预测购买了商品8的用户预测 actions = actions[(actions['type'] == 4) &amp; (actions['cate']==8)] actions = actions.groupby(['user_id', 'sku_id'], as_index=False).sum() actions['label'] = 1 actions = actions[['user_id', 'sku_id', 'label']] return actions 构造训练集 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def make_actions(user, product, all_actions, train_start_date): train_end_date = datetime.strptime(train_start_date, '%Y-%m-%d') + timedelta(days=3) train_end_date = train_end_date.strftime('%Y-%m-%d') # 修正prod_acc,cate_acc的时间跨度 start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=30) start_days = start_days.strftime('%Y-%m-%d') print train_end_date user_acc = get_recent_user_feat(train_end_date, all_actions) print 'get_recent_user_feat finsihed' user_cate = get_user_cate_feature(train_start_date, train_end_date, all_actions) print 'get_user_cate_feature finished' product_acc = get_accumulate_product_feat(start_days, train_end_date, all_actions) print 'get_accumulate_product_feat finsihed' cate_acc = get_accumulate_cate_feat(start_days, train_end_date, all_actions) print 'get_accumulate_cate_feat finsihed' comment_acc = get_comments_product_feat(train_end_date) print 'get_comments_product_feat finished' # 标记 test_start_date = train_end_date test_end_date = datetime.strptime(test_start_date, '%Y-%m-%d') + timedelta(days=5) test_end_date = test_end_date.strftime('%Y-%m-%d') labels = get_labels(test_start_date, test_end_date, all_actions) print \"get labels\" actions = None for i in (3, 5, 7, 10, 15, 21, 30): start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=i) start_days = start_days.strftime('%Y-%m-%d') if actions is None: actions = get_action_feat(start_days, train_end_date, all_actions, i) else: # 注意这里的拼接key actions = pd.merge(actions, get_action_feat(start_days, train_end_date, all_actions, i), how='left', on=['user_id', 'sku_id', 'cate']) actions = pd.merge(actions, user, how='left', on='user_id') actions = pd.merge(actions, user_acc, how='left', on='user_id') actions = pd.merge(actions, user_cate, how='left', on='user_id') # 注意这里的拼接key actions = pd.merge(actions, product, how='left', on=['sku_id', 'cate']) actions = pd.merge(actions, product_acc, how='left', on='sku_id') actions = pd.merge(actions, cate_acc, how='left', on='cate') actions = pd.merge(actions, comment_acc, how='left', on='sku_id') actions = pd.merge(actions, labels, how='left', on=['user_id', 'sku_id']) # 主要是填充拼接商品基本特征、评论特征、标签之后的空值 actions = actions.fillna(0)# return actions # 采样 action_postive = actions[actions['label'] == 1] action_negative = actions[actions['label'] == 0] del actions neg_len = len(action_postive) * 10 action_negative = action_negative.sample(n=neg_len) action_sample = pd.concat([action_postive, action_negative], ignore_index=True) return action_sample 12345678910111213141516171819202122def make_train_set(train_start_date, setNums ,f_path): train_actions = None all_actions = get_all_action() print \"get all actions!\" user = get_basic_user_feat() print 'get_basic_user_feat finsihed' product = get_basic_product_feat() print 'get_basic_product_feat finsihed' # 滑窗,构造多组训练集/验证集 for i in range(setNums): print train_start_date if train_actions is None: train_actions = make_actions(user, product, all_actions, train_start_date) else: train_actions = pd.concat([train_actions, make_actions(user, product, all_actions, train_start_date)], ignore_index=True) # 接下来每次移动一天 train_start_date = datetime.strptime(train_start_date, '%Y-%m-%d') + timedelta(days=1) train_start_date = train_start_date.strftime('%Y-%m-%d') print \"round &#123;0&#125;/&#123;1&#125; over!\".format(i+1, setNums) train_actions.to_csv(f_path, index=False) 123# 训练集train_start_date = '2016-03-01'make_train_set(train_start_date, 34, 'train_set.csv') 构造线下测试集 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061def make_val_answer(val_start_date, val_end_date, all_actions, label_val_s1_path): actions = get_actions(val_start_date, val_end_date,all_actions) actions = actions[(actions['type'] == 4) &amp; (actions['cate'] == 8)] actions = actions[['user_id', 'sku_id']] actions = actions.drop_duplicates() actions.to_csv(label_val_s1_path, index=False)def make_val_set(train_start_date, train_end_date, val_s1_path): # 修改时间跨度 start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=30) start_days = start_days.strftime('%Y-%m-%d') all_actions = get_all_action() print \"get all actions!\" user = get_basic_user_feat() print 'get_basic_user_feat finsihed' product = get_basic_product_feat() print 'get_basic_product_feat finsihed'# user_acc = get_accumulate_user_feat(train_end_date,all_actions,30)# print 'get_accumulate_user_feat finished' user_acc = get_recent_user_feat(train_end_date, all_actions) print 'get_recent_user_feat finsihed' user_cate = get_user_cate_feature(train_start_date, train_end_date, all_actions) print 'get_user_cate_feature finished' product_acc = get_accumulate_product_feat(start_days, train_end_date, all_actions) print 'get_accumulate_product_feat finsihed' cate_acc = get_accumulate_cate_feat(start_days, train_end_date, all_actions) print 'get_accumulate_cate_feat finsihed' comment_acc = get_comments_product_feat(train_end_date) print 'get_comments_product_feat finished' actions = None for i in (3, 5, 7, 10, 15, 21, 30): start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=i) start_days = start_days.strftime('%Y-%m-%d') if actions is None: actions = get_action_feat(start_days, train_end_date, all_actions,i) else: actions = pd.merge(actions, get_action_feat(start_days, train_end_date,all_actions,i), how='left', on=['user_id', 'sku_id', 'cate']) actions = pd.merge(actions, user, how='left', on='user_id') actions = pd.merge(actions, user_acc, how='left', on='user_id') actions = pd.merge(actions, user_cate, how='left', on='user_id') # 注意这里的拼接key actions = pd.merge(actions, product, how='left', on=['sku_id', 'cate']) actions = pd.merge(actions, product_acc, how='left', on='sku_id') actions = pd.merge(actions, cate_acc, how='left', on='cate') actions = pd.merge(actions, comment_acc, how='left', on='sku_id') actions = actions.fillna(0) # print actions # 构造真实用户购买情况作为后续验证 val_start_date = train_end_date val_end_date = datetime.strptime(val_start_date, '%Y-%m-%d') + timedelta(days=5) val_end_date = val_end_date.strftime('%Y-%m-%d') make_val_answer(val_start_date, val_end_date, all_actions, 'label_'+val_s1_path) actions.to_csv(val_s1_path, index=False) 123456# 验证集# train_start_date = '2016-04-06'# make_train_set(train_start_date, 3, 'val_set.csv')make_val_set('2016-04-06', '2016-04-09', 'val_1.csv')make_val_set('2016-04-07', '2016-04-10', 'val_2.csv')make_val_set('2016-04-08', '2016-04-11', 'val_3.csv') 构造测试集123456789101112131415161718192021222324252627282930313233343536373839404142434445def make_test_set(train_start_date, train_end_date): start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=30) start_days = start_days.strftime('%Y-%m-%d') all_actions = get_all_action() print \"get all actions!\" user = get_basic_user_feat() print 'get_basic_user_feat finsihed' product = get_basic_product_feat() print 'get_basic_product_feat finsihed' user_acc = get_recent_user_feat(train_end_date, all_actions) print 'get_accumulate_user_feat finsihed' user_cate = get_user_cate_feature(train_start_date, train_end_date, all_actions) print 'get_user_cate_feature finished' product_acc = get_accumulate_product_feat(start_days, train_end_date, all_actions) print 'get_accumulate_product_feat finsihed' cate_acc = get_accumulate_cate_feat(start_days, train_end_date, all_actions) print 'get_accumulate_cate_feat finsihed' comment_acc = get_comments_product_feat(train_end_date) actions = None for i in (3, 5, 7, 10, 15, 21, 30): start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=i) start_days = start_days.strftime('%Y-%m-%d') if actions is None: actions = get_action_feat(start_days, train_end_date, all_actions,i) else: actions = pd.merge(actions, get_action_feat(start_days, train_end_date,all_actions,i), how='left', on=['user_id', 'sku_id', 'cate']) actions = pd.merge(actions, user, how='left', on='user_id') actions = pd.merge(actions, user_acc, how='left', on='user_id') actions = pd.merge(actions, user_cate, how='left', on='user_id') # 注意这里的拼接key actions = pd.merge(actions, product, how='left', on=['sku_id', 'cate']) actions = pd.merge(actions, product_acc, how='left', on='sku_id') actions = pd.merge(actions, cate_acc, how='left', on='cate') actions = pd.merge(actions, comment_acc, how='left', on='sku_id') actions = actions.fillna(0) actions.to_csv(\"test_set.csv\", index=False) 4.13~4.16这三天的评论记录似乎并不存在为0的情况，导致构建测试集时出错 KeyError: &quot;[&#39;comment_num_0&#39;] not in index&quot; 1234# 预测结果sub_start_date = '2016-04-13'sub_end_date = '2016-04-16'make_test_set(sub_start_date, sub_end_date) 模型设计和评估123456789101112#!/usr/bin/env python# -*- coding: UTF-8 -*-import sysimport pandas as pdimport numpy as npimport xgboost as xgbfrom sklearn.model_selection import train_test_splitimport operatorfrom matplotlib import pylab as pltfrom datetime import datetimeimport timefrom sklearn.model_selection import GridSearchCV 1234567891011121314151617181920# import gcdef show_record(): train = pd.read_csv('train_set.csv')# valid = pd.read_csv('val_set.csv')# label_val = pd.read_csv('label_val_set.csv') valid1 = pd.read_csv('val_1.csv') valid2 = pd.read_csv('val_2.csv') valid3 = pd.read_csv('val_3.csv')# test = pd.read_csv('test_set.csv') print train.shape# print valid.shape# print label_val.shape# print test.shape print valid1.shape print valid2.shape print valid3.shape# show_record()# del train, valid, test# gc.collect() 训练模型 返回训练后的模型 生成特征map文件作为后续特征重要性之用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667def create_feature_map(features): outfile = open(r'xgb.fmap', 'w') i = 0 for feat in features: outfile.write('&#123;0&#125;\\t&#123;1&#125;\\tq\\n'.format(i, feat)) i = i + 1 outfile.close()def xgb_model(train_set): actions = pd.read_csv(train_set) #read train_set # 单纯的删掉模型前一遍训练认为无用的特征（根据特征重要性中不存在的特征） lst_useless = ['brand'] actions.drop(lst_useless, inplace=True, axis=1) users = actions[['user_id', 'sku_id']].copy() labels = actions['label'].copy() del actions['user_id'] del actions['sku_id'] del actions['label'] # 尝试通过设置scale_pos_weight来调整政府比例不均的问题，但是经过采样的正负比为1:10，训练结果反而不如设置为1# ratio = float(np.sum(labels==0)) / np.sum(labels==1)# print ratio # write to feature map features = list(actions.columns[:]) print 'total features: ', len(features) create_feature_map(features) # 训练时即传入特征名# features = list(actions.columns.values) user_index=users training_data=actions label=labels X_train, X_valid, y_train, y_valid = train_test_split(training_data.values, label.values, test_size=0.2, random_state=0) # 尝试通过提前设置传入训练的正负例的权重来改善正负比例不均的问题# weights = np.zeros(len(y_train))# weights[y_train==0] = 1# weights[y_train==1] = 10 # dtrain = xgb.DMatrix(X_train, label=y_train, weight=weights) dtrain = xgb.DMatrix(X_train, label=y_train) dvalid = xgb.DMatrix(X_valid, label=y_valid)# dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=features)# dvalid = xgb.DMatrix(X_valid, label=y_valid, feature_names=features)# dtrain = xgb.DMatrix(training_data.values, label.values) param = &#123;'n_estimators': 4000, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0, 'subsample': 1.0, 'colsample_bytree': 0.8, 'scale_pos_weight':10, 'eta': 0.1, 'silent': 1, 'objective': 'binary:logistic', 'eval_metric':'auc'&#125;# param = &#123;'n_estimators': 4000, 'max_depth': 6, 'seed': 7, 'min_child_weight': 5, 'gamma': 0, 'subsample': 1.0, # 'colsample_bytree': 0.8, 'scale_pos_weight': 1, 'eta': 0.09, 'silent': 1, 'objective': 'binary:logistic',# 'eval_metric':'auc'&#125; num_round = param['n_estimators']# param['nthread'] = 4 # param['eval_metric'] = \"auc\" plst = param.items() evallist = [(dtrain, 'train'), (dvalid, 'eval')]# evallist = [(dvalid, 'eval'), (dtrain, 'train')]# evallist = [(dtrain, 'train')] bst = xgb.train(plst, dtrain, num_round, evallist, early_stopping_rounds=10) bst.save_model('bst.model') return bst, featuresbst_xgb, features = xgb_model('train_set.csv') total features: 301 [0] train-auc:0.913108 eval-auc:0.911621 Multiple eval metrics have been passed: &apos;eval-auc&apos; will be used for early stopping. Will train until eval-auc hasn&apos;t improved in 10 rounds. [1] train-auc:0.932872 eval-auc:0.930423 [2] train-auc:0.936241 eval-auc:0.93338 ... [416] train-auc:0.982069 eval-auc:0.976755 [417] train-auc:0.982076 eval-auc:0.976751 [418] train-auc:0.982087 eval-auc:0.976753 Stopping. Best iteration: [408] train-auc:0.981964 eval-auc:0.976777 1print bst_xgb.attributes() {&apos;best_iteration&apos;: &apos;408&apos;, &apos;best_msg&apos;: &apos;[408]\\ttrain-auc:0.981964\\teval-auc:0.976777&apos;, &apos;best_score&apos;: &apos;0.976777&apos;} 对验证集进行线下测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546def report(pred, label): actions = label result = pred # 所有实际用户商品对 all_user_item_pair = actions['user_id'].map(str) + '-' + actions['sku_id'].map(str) all_user_item_pair = np.array(all_user_item_pair) # 所有购买用户 all_user_set = actions['user_id'].unique() # 所有预测购买的用户 all_user_test_set = result['user_id'].unique() all_user_test_item_pair = result['user_id'].map(str) + '-' + result['sku_id'].map(str) all_user_test_item_pair = np.array(all_user_test_item_pair) # 计算所有用户购买评价指标 pos, neg = 0,0 for user_id in all_user_test_set: if user_id in all_user_set: pos += 1 else: neg += 1 all_user_acc = 1.0 * pos / ( pos + neg) all_user_recall = 1.0 * pos / len(all_user_set) print '所有用户中预测购买用户的准确率为 ' + str(all_user_acc) print '所有用户中预测购买用户的召回率' + str(all_user_recall) pos, neg = 0, 0 for user_item_pair in all_user_test_item_pair: if user_item_pair in all_user_item_pair: pos += 1 else: neg += 1 all_item_acc = 1.0 * pos / ( pos + neg) all_item_recall = 1.0 * pos / len(all_user_item_pair) print '所有用户中预测购买商品的准确率为 ' + str(all_item_acc) print '所有用户中预测购买商品的召回率' + str(all_item_recall) F11 = 6.0 * all_user_recall * all_user_acc / (5.0 * all_user_recall + all_user_acc) F12 = 5.0 * all_item_acc * all_item_recall / (2.0 * all_item_recall + 3 * all_item_acc) score = 0.4 * F11 + 0.6 * F12 print 'F11=' + str(F11) print 'F12=' + str(F12) print 'score=' + str(score) return all_user_acc, all_user_recall, F11, all_item_acc, all_item_recall, F12, score 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465def validate(valid_set, val_label, model): actions = pd.read_csv(valid_set) #read test_set # users = actions[['user_id', 'sku_id']].copy() # 避免预测到非8类商品，所以最后还是再筛一遍的好 users = actions[['user_id', 'sku_id', 'cate']].copy() actions['user_id'] = actions['user_id'].astype(np.int64)# test_label= actions[actions['label'] == 1]# test_label= actions[(actions['label']==1) &amp; (actions['cate']==8)] test_label = pd.read_csv(val_label) lst_useless = ['brand'] actions.drop(lst_useless, inplace=True, axis=1) # test_label = test_label[['user_id','sku_id','label']] del actions['user_id'] del actions['sku_id'] # features = list(actions.columns.values) # del actions['label'] sub_user_index = users# sub_trainning_data = xgb.DMatrix(actions.values, feature_names=features) sub_trainning_data = xgb.DMatrix(actions.values)# y = model.predict(sub_trainning_data,ntree_limit=model.best_iteration) y = model.predict(sub_trainning_data, ntree_limit=model.best_ntree_limit) sub_user_index['label'] = y sub_user_index.to_csv('result_' + valid_set, index=False) # sub_user_index = sub_user_index[sub_user_index['cate']==8]# del sub_user_index['cate'] rank = 1000 pred = sub_user_index.sort_values(by='label', ascending=False)[:rank]# pred = sub_user_index[sub_user_index['label'] &gt;= 0.05] print 'No. of raw pred users: ', len(pred['user_id'].unique()) pred = pred[pred['cate']==8] print 'No. of pred users bought cate 8: ', len(pred['user_id'].unique()) # pred = pred[['user_id', 'sku_id']] pred = pred[['user_id', 'sku_id', 'label']] pred = pred.groupby('user_id').first().reset_index() pred['user_id'] = pred['user_id'].astype(int) pred['sku_id'] = pred['sku_id'].astype(int) # print 'No. of pred users after deduplicates: ', len(pred['user_id'].unique()) true_user = len(test_label['user_id']) pred_ui = len(pred['user_id'].unique()) print 'pred item: ', len(pred['sku_id'].unique()) print 'true users: ', true_user print 'pred users: ', pred_ui test_label['user_id'] = test_label['user_id'].astype(int) test_label['sku_id'] = test_label['sku_id'].astype(int) all_user_acc, all_user_recall, F11, all_item_acc, all_item_recall, F12, score = report(pred, test_label) f_name = 'pred_' + str(rank) + '_' + valid_set pred.to_csv(f_name, index=False) return rank, true_user, pred_ui, all_user_acc, all_user_recall, F11, all_item_acc, all_item_recall, F12, score# validate('val_set.csv', bst_xgb) 评分文件 1234567891011121314151617181920212223242526272829303132def avg_score(): rank1, true_user1, pred_ui1, user_acc1, user_recall1, F11_1, item_acc1, item_recall1, F12_1, score1 = validate('val_1.csv', 'label_val_1.csv', bst_xgb) print '-------------------------------------------' rank2, true_user2, pred_ui2, user_acc2, user_recall2, F11_2, item_acc2, item_recall2, F12_2, score2 = validate('val_2.csv', 'label_val_2.csv', bst_xgb) print '-------------------------------------------' rank3, true_user3, pred_ui3, user_acc3, user_recall3, F11_3, item_acc3, item_recall3, F12_3, score3 = validate('val_3.csv', 'label_val_3.csv', bst_xgb) print '===========================================' print 'avg user acc: ', (user_acc1+user_acc2+user_acc3)/3 print 'avg user recall: ', (user_recall1+user_recall2+user_recall3)/3 print 'avg item acc: ', (item_acc1+item_acc2+item_acc3)/3 print 'avg item recall: ', (item_recall1+item_recall2+item_recall3)/3 print 'avg F11: ', (F11_1+F11_2+F11_3)/3 print 'avg F12: ', (F12_1+F12_2+F12_3)/3 print 'avg score: ', (score1+score2+score3)/3 # make the csv file dct_score = &#123;&#125; dct_score['rank'] = [rank1, rank2, rank3] dct_score['true_user'] = [true_user1, true_user2, true_user3] dct_score['pred_ui'] = [pred_ui1, pred_ui2, pred_ui3] dct_score['user_acc'] = [user_acc1, user_acc2, user_acc3] dct_score['user_recall'] = [user_recall1, user_recall2, user_recall3] dct_score['F11'] = [F11_1, F11_2, F11_3] dct_score['item_acc'] = [item_acc1, item_acc2, item_acc3] dct_score['item_recall'] = [item_recall1, item_recall2, item_recall3] dct_score['F12'] = [F12_1, F12_2, F12_3] dct_score['score'] = [score1, score2, score3] column_order = ['rank', 'true_user', 'pred_ui', 'user_acc', 'user_recall', 'item_acc', 'item_recall', 'F11', 'F12', 'score'] df_score = pd.DataFrame(dct_score) file_name = 'score_' + str(datetime.now().date())[5:] +'_'+ str(rank1) + '.csv' df_score[column_order].to_csv(file_name, index=False)avg_score() No. of raw pred users: 950 No. of pred users bought cate 8: 950 pred item: 220 true users: 1211 pred users: 950 所有用户中预测购买用户的准确率为 0.147368421053 所有用户中预测购买用户的召回率0.116569525396 所有用户中预测购买商品的准确率为 0.108421052632 所有用户中预测购买商品的召回率0.0850536746491 F11=0.141152747437 F12=0.0930778962588 score=0.11230783673 ------------------------------------------- No. of raw pred users: 950 No. of pred users bought cate 8: 950 pred item: 203 true users: 1259 pred users: 950 所有用户中预测购买用户的准确率为 0.163157894737 所有用户中预测购买用户的召回率0.12360446571 所有用户中预测购买商品的准确率为 0.116842105263 所有用户中预测购买商品的召回率0.0881652104845 F11=0.15489673551 F12=0.0977629029417 score=0.120616435969 ------------------------------------------- No. of raw pred users: 960 No. of pred users bought cate 8: 960 pred item: 219 true users: 1385 pred users: 960 所有用户中预测购买用户的准确率为 0.161458333333 所有用户中预测购买用户的召回率0.11231884058 所有用户中预测购买商品的准确率为 0.120833333333 所有用户中预测购买商品的召回率0.0837545126354 F11=0.150485436893 F12=0.0954732510288 score=0.117478125375 =========================================== avg user acc: 0.157328216374 avg user recall: 0.117497610562 avg item acc: 0.115365497076 avg item recall: 0.0856577992563 avg F11: 0.14884497328 avg F12: 0.0954380167431 avg score: 0.116800799358 输出特征重要性12345678910def feature_importance(bst_xgb): importance = bst_xgb.get_fscore(fmap=r'xgb.fmap') importance = sorted(importance.items(), key=operator.itemgetter(1), reverse=True) df = pd.DataFrame(importance, columns=['feature', 'fscore']) df['fscore'] = df['fscore'] / df['fscore'].sum() file_name = 'feature_importance_' + str(datetime.now().date())[5:] + '.csv' df.to_csv(file_name)feature_importance(bst_xgb) 生成提交结果12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# sub_filedef submit(pred_set, model): actions = pd.read_csv(pred_set) #read test_set # print 'total user before: ', len(actions['user_id'].unique())# potential = pd.read_csv('potential_user_04-28.csv')# lst_user = potential['user_id'].unique().tolist()# actions = actions[actions['user_id'].isin(lst_user)]# print 'total user after: ', len(actions['user_id'].unique()) # 提前去掉部分特征 lst_useless = ['brand'] actions.drop(lst_useless, inplace=True, axis=1) users = actions[['user_id', 'sku_id', 'cate']].copy()# users = actions[['user_id', 'sku_id']].copy() actions['user_id'] = actions['user_id'].astype(np.int64) del actions['user_id'] del actions['sku_id'] sub_user_index = users sub_trainning_data = xgb.DMatrix(actions.values) y = model.predict(sub_trainning_data, ntree_limit=model.best_ntree_limit) sub_user_index['label'] = y # sub_user_index = sub_user_index[sub_user_index['cate']==8]# del sub_user_index['cate'] rank = 1200 pred = sub_user_index.sort_values(by='label', ascending=False)[:rank]# pred = sub_user_index[sub_user_index['label'] &gt;= 0.05]# pred = pred[['user_id', 'sku_id', 'label']]# pred = pred[pred['label']&gt;0.45] print 'No. of raw pred users: ', len(pred['user_id'].unique()) pred = pred[pred['cate']==8] print 'No. of pred users bought cate 8: ', len(pred['user_id'].unique()) pred = pred[['user_id', 'sku_id']]# print pred = pred.groupby('user_id').first().reset_index() pred['user_id'] = pred['user_id'].astype(int) pred['sku_id'] = pred['sku_id'].astype(int) sub_file = 'submission_' + str(rank) + '_' + str(datetime.now().date())[5:] + '.csv'# sub_file = 'submission_detail_' + str(datetime.now().date())[5:] + '.csv' pred.to_csv(sub_file, index=False, index_label=False) submit('test_set.csv', bst_xgb) No. of raw pred users: 1142 No. of pred users bought cate 8: 1142 提交结果除重（可选）除去最后临近三天的发生购买行为的用户商品对 123456789101112131415161718192021222324252627282930from datetime import datetimedef sub_improv(action, sub_file): # 获取4月最近三天的目标用户商品对 action_4 = pd.read_csv(action) action_4['time'] = pd.to_datetime(action_4['time']).apply(lambda x: x.date()) aim_date = [datetime.strptime(s, '%Y-%m-%d').date() for s in ['2016-04-09', '2016-04-10', '2016-04-11' , '2016-04-12', '2016-04-13', '2016-04-14', '2016-04-15']] aim_action = action_4[(action_4['type']==4) &amp; (action_4['cate']==8) &amp; (action_4['time'].isin(aim_date))] aim_ui = aim_action['user_id'].map(int).map(str) + '-' + aim_action['sku_id'].map(str) # 拼接提交数据的用户商品 sub = pd.read_csv(sub_file) before = sub.shape[0] sub_ui = sub['user_id'].map(str) + '-' + sub['sku_id'].map(str) # 交集 lst_aim = aim_ui.unique().tolist() lst_sub = sub_ui.unique().tolist() lst_common = [i for i in lst_aim if i in lst_sub] dct_ui = &#123;i.split('-')[0]: i.split('-')[1] for i in lst_common&#125; # 从提交结果除掉交集部分 for k in dct_ui: sub.drop(sub[(sub['user_id']==int(k)) &amp; (sub['sku_id']==int(dct_ui[k]))].index, inplace=True) print 'No. of records after remove dup: ', sub.shape[0] print 'No. of dup: ', before - sub.shape[0] if (before - sub.shape[0])!=0: file_name = 'submission_' + str(datetime.now().date())[5:] + '_improv.csv' sub.to_csv(file_name, index=False, index_label=False) sub_improv('data/JData_Action_201604.csv', 'submission_1200_05-25.csv') No. of records after remove dup: 1142 No. of dup: 0 比赛过程中遇到和解决的问题线上线下测试的分相差悬殊问题传入report的真实预测用户商品对有问题，之前使用滑动窗口时仅用前三天预测后五天，对后五天发生了购买(类别8)行为的用户打上标签，生成训练集、验证集，这里隐式的划定了前三天有交互的用户群体，但是事实在后五天发生购买行为的用户应该理论上不小于这个数目(存在用户前三天并无交互但是在后五天发生购买行为的情况)，所以在计算report中的callback等值时传入的应该是这5天实际购买的用户行为记录，修改后，线上线下测试误差控制在0.01左右 线下测试集组数开始只设定了一组，导致有时候线下线上的变化不一致，后期设定了三组并结合三组的输出以及均值判断，基本能保证线上线下成绩同步 编码问题文件本身是gbk编码，其中的年龄属性就是中文的，导致转换时经常出现编码错误，后来修改了独热编码部分的代码，一开始对年龄这类含有中文的特征进行独热编码前，先将文本转化为数值（用数字表示类别）LabelEncoder + get_dummies()来源 feature_importance列未对应，导致一段时间分析的特征重要性文件存在问题，产生误导合并数据集时产生空值，由数据不一致造成手动添加一些特征列并填充缺失值，此外注意拼接的键 除零出现无限值平滑部分除零特征 优化时间计算效率开始时，构建时间差的地方用到了日期和字符串转换等操作，apply等比较费时，后面不使用apply而且一次性全部计算所有时间差并优先存储到数据中直接读入，提升了特征构建的速度 失败的尝试以及未尝试的 是否自定义衡量标准，而不使用roc-auc? 参照 正负例比例不均的问题是否使用权重来矫正？参照 参数：scale_pos_weight DMatrix中的weight 尝试设置正负比例的参数，特别分阶段而且没有采样的条件下，不同阶段模型里的正负例是不一样的，尝试过，可能是提前随机欠抽样导致本身数据的随机性而再修改该属性反而没有效果；但是考虑到本身使用滑窗会产生大量数据，若不采样则机器内存不足以承受，而且后面尝试另一种滑窗的方法并不采样使用scale_pos_weight却也并无改善，可能需要参考《机器学习》使用EasyEnsemble的方法 分阶段预测第一阶段预测用户，第二阶段预测前一阶段预测出的用户可能购买的商品设计特征如何分割和构建的问题以及标签的问题没有解决，导致失败 打标签方式的尝试差别不大 时间窗的修改最后是要预测五天内的购买，我们可以先随机取几个五天发生购买的用户，看看这些用户与所购买商品的交互时间周期覆盖率较大的一个值，然后反过来取另外的时间周期的交互用户商品对，看看这些里面有多少在时间周期后五天发生了购买行为，主要是交互周期的一个覆盖率问题将对这三个月购买过的类别8商品后五天之前14天有过交互的所有用户商品对提取出来，而这里面根据前面所分析的，可以涵盖所有存在交互的用户商品对的80%的用户但实际上虽然这14天能覆盖到80%的用户商品对，但是其实这部分用户商品对在不少交互其实只是在这14天的前面的某几天出现过，而后续并未进一步交互，虽然最后发生了购买行为，但是对于我们通过统计交互行为来构建特征的方法并没有什么帮助，反而扩大的时间窗囊括进了大量的噪声数据致使结果变糟 平滑特征 去重 填充空值 优化问题，如何加速训练 时间窗口构造上避开统计结果中的高峰、低谷时间区间效果反而变差 其他模型以及模型融合尝试过lightgbm,randomforest,gbdt，以及averaging但是效果不佳 后记以上大概是本次比赛的一个简要的流程，完整的代码可以到我的github查看，下面简要回顾下本次比赛的收获和有待进一步改进的地方 团队合作很重要，特别是队员之间工作协调好，此外code review非常重要 对数据的分析是很关键的，只有通过统计分析才能说明你对数据的了解，你以为的常识是靠不住的 有想法在分析过后的基础上最好马上实践来验证，并做好相应的记录 做好分析记录很重要，无论是过程中的数据分析、特征尝试还是模型的参数调整；另外后面线下验证的过程也有必要做记录，毕竟线上测试机会有限，线下测试做的好、记录完备可以提高成绩 如开头所述，如何选取恰当的时间区间来预测用户商品对以及最近一段时间没有交互记录的用户商品对该如何挖掘出来是个难题 本次最终只提交了xgboost单模型的预测，后期也有尝试lightgbm、RandomForest、GBDT等模型，但是效果不是很好，而且时间仓促，模型集成这块也来不及做，尝试了bagging但是效果不是很好 数据本身存在正负比例不均的问题，查阅过一些资料，最后使用的是随机欠采样负例按与正例1：10的比例构建训练集，但是这种方法随机性较大，造成结果不稳定，对训练集敏感性较强，应该还有更好的方法 特征筛选问题，中间为了快速提高成绩，一味的添加特征，对于特征却没有进行适当的筛选，后期聚集了上百个特征却难以筛选，尝试过方差法、通过xgboost模型本身输出的特征重要性来筛选，但是效果不佳，这一块应该还有很多提升空间 关于分步预测的问题，也有尝试过按照群里大神提到的分阶段预测，即第一阶段仅预测未来可能会购买的用户，第二阶段预测这部分用户可能购买的商品，然而中间尝试很长一段时间效果不好，主要是不同阶段之间特征的取舍和冲突，最终效果不好而作罢，但是不失为一种思路","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"JData","slug":"JData","permalink":"http://yoursite.com/tags/JData/"},{"name":"比赛","slug":"比赛","permalink":"http://yoursite.com/tags/比赛/"}]},{"title":"概率统计小记","date":"2017-06-22T02:53:49.000Z","path":"2017/06/22/Statistics/","text":"似然函数知乎上的一些理解wiki 最大似然估计知乎wiki 最大后验概率最大似然估计 vs 最大后验概率","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"统计学","slug":"统计学","permalink":"http://yoursite.com/tags/统计学/"}]},{"title":"统计学习方法笔记(五) —— 决策树","date":"2017-06-19T08:41:25.000Z","path":"2017/06/19/Decision-Tree/","text":"决策树是一种基本的分类与回归方法，其模型呈树形结构，在分类问题中表示基于特征对实例进行分类的过程，可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布； 其主要优点是模型具有可读性，分类速度快； 学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型，预测时，对新的数据，利用决策树模型进行分类； 决策树学习的三个主要步骤 特征选择 决策树的生成 决策树的剪枝 常用决策树算法 ID3 C4.5 CART Notes:关于损失函数最小化可以回顾第一章节的模型选择部分的内容。 决策树模型定义分类决策树模型是一种描述对实例进行分类的树形结构，其中包含两种类型的节点 内部节点：表示一个特征（属性） 叶节点：表示一个类 if-then规则集合 一条由根节点到叶节点的路径 –&gt; 一条规则 路径上内部节点的特征 –&gt; 规则的条件 叶节点的类 –&gt; 规则的结论 性质：互斥且完备 条件概率分布给定特征条件下类的条件概率分布 决策树的学习 决策树学习本质上是从训练数据集中归纳出一组分类规则，另一个角度，学习是由训练数据集估计条件概率模型 目的：得到一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力 策略：以损失函数（通常为正则化的极大似然函数）为目标函数的最小化，并在损失函数确定后，选择最优决策树 学习算法： 理论上：从所有可能的决策树中选取最优决策树，NP完全问题 实际中：采用启发式方法，近似求解（得到次最优决策树）–&gt; 递归的选择最优特征，并根据该最优特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。 主要步骤： 特征选择 决策树的生成 决策树的剪枝 Notes: 决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。 特征选择 实质：选取对于训练数据具有分类能力的特征（决定用哪个特征来划分特征空间） 常用准则 信息增益 –&gt; ID3 信息增益比 –&gt; C4.5 基尼指数 –&gt; CART Notes: 分类能力强即表示给定一个特征使得实例能较准确地被分类，及减少了实例的不确定性，掌握了更多的信息（信息增益角度的理解） 每次挑一个特征（列），根据该特征下各记录的不同取值来划分实例点（过程让我联想起k-nn中kd树的构建）–&gt; 所以一般要求特征值为离散值，或者事先对特征进行离散化处理 信息增益定义$$g(D, A)=H(D)-H(D|A)$$ $g(D, A)$即信息增益，表示得知特征$A$的信息而使得类$D$的信息的不确定性减少的程度 $H(D)$为集合$D$的经验熵 其中假设$D$是一个取有限个值的离散随机变量，概率分布为$P(X=x_i)=p_i, i=1, 2,…,n$ 熵是表示随机变量不确定性的度量，定义$H(D)=- \\sum_{i=1}^n p_ilogp_i$，熵越大，随机变量的不确定性就越大，$0 \\leq H(D) \\leq logn$ $H(D|A)$即经验条件熵表示在已知随机变量$A$（特征）的条件下随机变量$D$的不确定性$H(D|A)= \\sum_{i=1}^{n}p_iH(D|A=a_i)$ 一般将熵$H(D)$与条件熵$H(D|A)$之差称为互信息，决策树学习中的信息增益等价于训练数据集中类与特征的互信息。 小结 给定训练数据集$D$和特征$A$，经验熵$H(D)$表示对数据集$D$进行分类的不确定性，而经验条件熵$H(D|A)$表示在特征$A$给定的条件下对数据集进行分类的不确定性，因此两者之差即信息增益表示由于特征$A$而使得数据集$D$的分类的不确定性减少的程度。 对于数据集$D$而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益，信息增益大的特征具有更强的分类能力（也就是我们需要挑选的目标） 算法——特征选择 输入：训练数据集$D$和特征$A$； 输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$ (1) 计算数据集$D$的经验熵$H(D)$，$$H(D)=- \\sum_{k=1}^{K}\\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|}$$ (2) 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$,$$H(D|A)= \\sum_{i=1}^{n} \\frac{|D_i|}{|D|}H(D_i)=- \\sum_{i=1}^{n} \\frac{|D_i|}{|D|} \\sum_{k=1}^{K} \\frac{|D_{ik}|}{|D_i|}log_2 \\frac{|D_{ik}|}{|D_i|}$$ (3) 计算信息增益$g(D, A)$,$$g(D|A)=H(D)-H(D|A)$$其中假设训练数据集$D$，$|D|$表示样本个数，设有$K$个类$C_k$, $|C_k|$表示属于类$C_k$的样本个数，根据特征$A$的$n$个不同的取值将训练数据集$D$划分为$n$个子集$D_1, D_2,…,D_n$，其中$|D_i|$表示子集$D_i$中的样本数，记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$ Notes:上述过程是一次特征挑选的过程，首先计算原始数据的经验熵，然后对每一个特征（轮流判断），首先根据该特征的所有不同取值将原始数据进行划分，得到的数据子集分别计算该子集的经验熵，然后按照该子集本身样本占比加权（按概率）加和起来作为经验条件熵，两相作差即为该特征对应的信息增益值，而在计算所有特征对应的信息增益之后，我们选出其中信息增益最大的特征作为分割的特征对数据集进行划分，而且后面涉及到决策树的生成过程就是一个递归的对子数据集挑选最佳特征然后划分数据集的过程。 如果觉得抽象可以参照原书P62的例子，或者参考《机器学习实战》中决策树章节的ID3算法的实现就能大概知道是什么情况了。 信息增益比 以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比可以对这一问题进行校正。 信息增益比定义特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比：$$g_R(D,A)= \\frac{g(D,A)}{H_A(D)}$$其中$$H_A(D)=- \\sum_{i=1}^{n} \\frac{|D_{i}|}{|D|}log_2 \\frac{|D_{i}|}{|D|}$$$n$为特征$A$的取值个数，$D_i$表示据特征$A$的取值将$D$分成的子集 决策树的生成ID3核心思想 在决策树的各个结点上应用信息增益准则选择特征，递归地构建决策树。 递归终止条件：所有特征的信息增益（设置信息增益的阈值来判断是否进一步划分）均很小或没有特征可以选择（每选择一个特征则后期划分子树不再使用前面使用过的特征，因为子树已经是在该特征下属于同一取值的实例集合）为止。 ID3相当于用极大似然法进行概率模型的选择。 算法-决策树生成 输入：训练数据集$D$和特征集$A$，阈值$\\varepsilon$； 输出：决策树$T$ (1) （叶子结点）若$D$中所有实例属于同一类$C_k$,则$T$为单节点树，并将类$C_k$作为该结点的类标记，返回$T$ (2) (终止条件之没有特征可供选择)若$A= \\emptyset$,则$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记（多数表决规则），返回$T$ (3) 否则计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$ (4) (终止条件之阈值)若$A_g$的信息增益小于阈值$\\varepsilon$，则置$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$ (5) 否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将$D$划分为若干非空子集$D_i$，并将$D_i$中实例数最大的类作为标记构建子节点，返回$T_i$ (6) 对第$i$个子节点，以$D_i$为训练集，$A- \\lbrace A_g \\rbrace$为特征集，递归地调用(1)~(5)得到子树$T_i$并返回。 C4.5C4.5算法与ID3算法类似，不同之处在于，C4.5在生成的过程中，用信息增益比来选择特征。 Notes:上述决策树的生成算法只有树的生成，而且是针对训练集构造的树，容易产生过拟合。 决策树的剪枝过拟合 过拟合产生的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树，解决该问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。 剪枝定义在决策树学习中将已生成的树进行简化的过程 实现决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现 决策树的损失函数$$C_\\alpha(T)= \\sum_{t=1}^{|T|}|N_t|H_t(T)+ \\alpha|T|$$其中，树$T$的叶节点数为$|T|$，叶节点$t$有$|N_t|$个样本点，其中属于$k$类的数目为$|N_{tk}|$个 其中经验熵$H_t(T)$为$$H_t(T)=- \\sum_{k=1}^{K} \\frac{|N_{tk}|}{|N_t|}log_2 \\frac{|N_{tk}|}{|N_t|}$$ 令$C(T)=\\sum_{t=1}^{|T|}|N_t|H_t(T)$，则损失函数可表示为$$C_\\alpha(T)=C(T)+\\alpha|T|$$$C(T)$表示模型对训练数据的预测误差（个人理解为叶节点的经验熵与该叶节点的样本点数之积的加和，类似一种叶节点的总体的不确定性），即模型与训练数据的拟合程度，$|T|$表示模型的复杂度（叶节点数），参数$\\alpha$控制两者之间的影响 Notes: 剪枝就是当$\\alpha$确定时，选择损失函数最小的模型，及损失函数最小的子树。 利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。 算法-决策树剪枝 输入：生成算法产生的整棵树$T$，参数$\\alpha$ 输出：修剪后的子树$T\\alpha$ (1)计算每个结点的经验熵 (2)递归地从叶节点向上回缩:设叶节点回缩到其父节点之前与之后的整体树分别为$T_B$和$T_A$，如果其对应的损失函数有：$$C_{\\alpha}(T_A) \\leq C_{\\alpha}(T_B)$$则进行剪枝，即将父节点变为新的叶结点(关于这里叶节点的类别标记应该仍是以多数表决的方法)。 (3)返回(2)直至不能继续，得到损失函数最小的子树$T\\alpha$。 Notes:在向上回缩过程中判断损失函数值大小时，计算可以在局部进行，所以决策树的剪枝算法可以由一种动态规划的算法实现 CART(classification and regression tree)算法与ID3,C4.5的区别 CART假设决策树是二叉树，而ID3,C4.5生成的过程中并无此假设，这也导致了两者的根本不同，ID3,C4.5每次选择出最佳特征之后，是按照该特征的每一个取值划分子树；而CART则是对每一个特征、每一个特征的每一个取值计算基尼指数（分类树）然后从所有特征、所有特征对应的取值计算所得的基尼指数中最小的特征及特征值作为切分点来划分子树，而划分依据则是判断实例对应特征的值是否等于该选定的特征值 子树划分（特征选择）的准则不同 回归树，平方误差最小化准则 分类树，基尼指数最小化准则 CART生成回归树理解设$X, Y$分别为输入和输出变量，其中$Y$为连续变量，给定训练数据集$D= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $ 一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值，所以我们的主要目的是要构建回归树，也就是如何划分输入空间，因为一旦划分好输入空间，如将输入空间划分为$M$个单元$R_1, R_2,…,R_M$,并且在每个单元$R_m$上有一个固定的输出值$c_m$，那么回归树的模型就可以表示为$$f(x)=\\sum_{m=1}^Mc_mI(x \\in R_m)$$ 如何划分输入空间？仍然是采用启发式的方法（尝试），假设我们取$j$的某个值$s$来划分实例点，就可以根据各实例点对应该特征$j$的取值与选定的特征值$s$比较大小来划分，即$$R_1(j, s)= \\lbrace x|x^{(j)} \\leq s \\rbrace$$$$R_2(j, s)= \\lbrace x|x^{(j)} &gt; s \\rbrace$$ 如何选择最优切分点$s$和对应的切分变量$j$?逐一计算和比较，比较的标准是平方误差最小化，即$$min_{j,s} \\left[ min_{c_1} \\sum_{x_i \\in R_1(j,s)} (y_i-c_1)^2 + min_{c_2} \\sum_{x_i \\in R_2(j,s)} (y_i-c_2)^2 \\right]$$,这里的$c_1, c_2$就是各个区域的输出，那么各个区域的输出又是怎么得到的呢？ 如何得到各区域的输出？仍然是依照平方误差最小准则，计算$\\sum_{x_i \\in R_m}(y_i-f(x_i))^2$，所以计算得到的单元$R_m$上的$c_m$的最优值刚好是$R_m$上所有输入实例$x_i$对应的输出$y_i$的均值，即$$\\hat{c_m}=ave(y_i|x_i \\in R_m)$$ 小结一下，通过计算各区域的实例对应的输出的均值可以得到当前尝试的划分点和划分变量划分的各区域的输出，从而可以计算和比较不同的切分点和切分变量下的误差，并进一步根据平方误差最小化准则从中选取出最优的切分点和切分变量，而确定了切分点和切分变量就相当于确定了区域的一个划分，接下来针对已划分的区域进一步根据需求进行划分或者停止划分就最终将输入空间划分为多个区域，而每个区域对应有一个确定的输出值，也就是构建好了一颗回归树 算法-CART回归树 输入：训练数据集$D$ 输出：回归树$f(x)$在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树 (1) 选择最优切分点$s$和切分变量$j$，求解$$min_{j,s} \\left[ min_{c_1} \\sum_{x_i \\in R_1(j,s)} (y_i-c_1)^2 + min_{c_2} \\sum_{x_i \\in R_2(j,s)} (y_i-c_2)^2 \\right]$$遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小的$s, j$ (2) 用选定的$s, j$划分区域并决定相应的输出值：$$R_1(j, s)= \\lbrace x|x^{(j)} \\leq s \\rbrace; R_2(j, s)= \\lbrace x|x^{(j)} &gt; s \\rbrace$$$$\\hat{c_m}=ave(y_i|x_i \\in R_m)$$ (3) 继续对两个子区域调用步骤(1),(2)直至满足停止条件 (4) 将输入空间划分为$M$个区域$R_1, R_2,…,R_M$，生成决策树$$f(x)=\\sum_{m=1}^Mc_mI(x \\in R_m)$$ 分类树分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点 基尼指数 定义分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$,则概率分布的基尼指数定义为$$Gini(p)=\\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^Kp_k^2$$对于给定的样本集合$D$,其基尼指数为$$Gini(D)=1-\\sum_{k=1}^{K}\\left(\\frac{|C_k|}{|D|}\\right)^2$$,其中，$C_k$为$D$中属于第$k$类的样本子集，$K$是类的个数 特征$A$的条件下，集合$D$的基尼指数$$Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$$，其中样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1, D_2$两部分$$D_1=\\lbrace (x, y) \\in D|A(x)=a \\rbrace; D_2=D-D_1$$ 基尼指数$Gini(D)$表示集合$D$的不确定性，基尼指数$Gini(D,A)$表示经$A=a$分割后集合$D$的不确定性，基尼指数值越大，样本集合的不确定性也就越大。 算法-CART分类树 输入：训练数据集$D$，停止计算的条件（如结点中样本个数小于预定阈值，或样本集的基尼指数小于预定阈值(样本基本属于同一类)，或者没有更多的特征） 输出：CART决策树从根结点开始，递归地对每个结点进行以下操作： (1) 设结点的训练数据集为$D$，计算现有特征对该数据集的基尼指数，对每一个特征$A$，对其可能的每一个取值$a$，根据样本点对$A=a$的测试为“是”或“否”将$D$分割成$D_1$和$D_2$两部分，计算$A=a$时的基尼指数。 (2) 在所有可能的特征$A$以及它们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。 (3) 对两个节点递归地调用(1),(2)直至满足停止条件 (4) 生成CART决策树 CART剪枝基本原理 首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个子树序列$\\lbrace T_0,T_1,…,T_n \\rbrace$ 然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树 剪枝过程理解 基本原理如上所述，就是由先前生成的CART决策树开始逆向剪枝得到一系列不同的子树集合，然后使用另一份验证集来对上述子树测试选出其中损失函数最小的子树 而由于我们根据损失函数最小来选取决策树，而由损失函数的定义$$C_\\alpha(T)= \\sum_{t=1}^{|T|}|N_t|H_t(T)+ \\alpha|T|$$而我们主要是通过调整$\\alpha$的值来权衡模型对训练集的预测能力和模型复杂度，因此对于固定的$\\alpha$必然存在使损失函数$C_\\alpha(T)$最小的子树，而极端情况下，前面根据CART生成算法得到的决策树$T_0$可认为是模型复杂度最大时的树，即此时可认为$\\alpha=0$，而当$\\alpha=+\\infty$时对应的是以根结点为单节点树的简单模型 如此一来对于每一个$\\alpha$的值，存在一个不同的子树模型，我们可以通过慢慢的从原始生成树中剪去叶节点并收集新得到的决策树，最后可以得到一个决策树的集合，而这个修剪和收集的过程，为了保证收集到完备的决策树序列，我们每次在原始生成树的基础上比较每个结点的剪枝前后的损失函数减少程度（即比较以该节点为根的节点的模型的损失与以该节点为单节点的模型的损失之差），并选取损失函数减少程度最小的一个子树部分，从原始生成树中剪去该子树得到新的子树保存到子树序列，并作为进一步剪枝的起始子树，直到最后得到以根节点为单节点模型。 算法-CART剪枝 输入：CART算法生成的决策树$T_0$ 输出：最优决策树$T_{\\alpha}$ (1) 设$k=0, T=T_0$ (2) 设$\\alpha = +\\infty $ (3) 自下而上对各内部结点$t$计算$C(T_t), |T_t|$，以及$$g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1}$$$$\\alpha=min(\\alpha,g(t))$$其中$T_t$表示以$t$为根节点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶节点个数 (4) 对$g(t)=\\alpha$的内部节点$t$进行剪枝，剪去$g(t)$最小的子树，并对叶节点$t$以多数表决法决定其类，得到树$T$ (5) 设$k=k+1, \\alpha_k=\\alpha, T_k=T$ (6) 若$T_k$不是由根结点及两个叶结点构成的树，就返回(3)，否则令$T_k=T_n$(即最后一次剪枝得到根结点为单节点模型) (7) 采用交叉验证在剪枝得到的子树序列$T_0,T_1,…,T_n$中选取最优子树$T_{\\alpha}$ 决策树算法对比如果按照特征选择、决策树生成和决策树剪枝三个步骤来比较的话 在特征选择上，三个决策树算法的选择标准都不一样：ID3(信息增益)、C4.5(信息增益比)、CART(分类：基尼指数，回归：最小二乘法)；此外就分类而言，ID3和C4.5分别是选择是信息增益、信息增益比最大的特征而CART则选择基尼指数最小的特征； 在决策树生成时：三者的终止条件大致类似（预设阈值、特征集合非空等），但是ID3和C4.5生成过程中是根据每一个特征值来生成一个对应子树，而CART则是根据特征值是否等于选定的特征值来进行二分（回归中则是根据大于小于某选定值划分区域）；也就是说前两者是树，而CART是二叉树（而这也大概决定了为什么原始的CART可以用来做回归，而前两者不行） 决策树剪枝方面：标准都是判断决策树整体的损失函数（预测误差+正则化项（模型复杂度））；不同的是，ID3,C4.5是局部的对比叶节点删除前后的损失来决定是否剪枝，而CART则是自下而上从最复杂的完整二叉树一直剪枝到最简单的单节点决策树得到一系列决策树，并最后在验证集上验证选择其中最佳的决策树（每次剪枝为了保证最后得到的决策树序列尽可能完备，每次选择前后损失函数减少最小的一种剪枝策略） 决策树生成算法ID3的实现按照机器学习实战第三章节的内容实现了简单的ID3算法 特征选择 计算熵 计算条件熵 计算信息增益 递归构建决策树并使用matplotlib实现决策树的可视化 使用pickle序列化保存生成的决策树模型 实例：使用决策树模型预测隐形眼镜的类型具体实验代码见github","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"decision tree","slug":"decision-tree","permalink":"http://yoursite.com/tags/decision-tree/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"}]},{"title":"统计学习方法笔记(四) —— 朴素贝叶斯法","date":"2017-06-09T11:59:44.000Z","path":"2017/06/09/naive-bayes/","text":"朴素贝叶斯法是基于贝叶斯定理和特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布，然后基于此模型，对给定的输入$x$，利用贝叶斯定理求出后验概率最大的输出$y$。 关键词：贝叶斯定理、特征条件独立假设、实现简单、学习预测效率高、生成学习方法 基本方法 贝叶斯定理 特征条件独立假设 贝叶斯定理若$X$是定义在输入空间$R^n$上的随机向量，$Y$是定义在输出空间$\\lbrace c_1, c_2,…, c_K \\rbrace$上的随机变量，$P(X, Y)$是$X$和$Y$的联合概率分布，训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $由$P(X, Y)$独立同分布产生。 后验概率现对于实例$x$需要判断其所属分类，即我们需要知道x属于哪个分类的概率最大，用公式表示就是求取$$argmax_{c_k} P(Y=c_k | X=x)$$即后验概率，其中$k=1, 2,…, K$表示所有可能分类，而对于求解$P(Y=c_k | X=x)$，根据贝叶斯定理，我们需要知道训练数据的： 先验概率分布$$P(Y=c_k), k = 1,2,…,K$$ 条件概率分布，即每种分类下各个实例特征取不同值的概率 $$P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)},…,X^{(n)}=x^{(n)}|Y=c_k)$$然后根据贝叶斯定理进行计算求得后验概率 贝叶斯定理$$P(Y|X)=\\frac{P(X,Y)}{P(X)}=\\frac{P(X|Y)P(Y)}{\\sum P(Y)P(X|Y)}$$即$$P(Y=c_k|X=x)=\\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_k P(Y=c_k)P(X=x|Y=c_k)}$$但是如果直接这么求解，由于条件概率分布涉及到大量的参数，所以其估计在实际中是不可行的，所以我们在这里进行了一个大胆的假设 条件独立性假设假设用于分类的特征在类确定的条件下都是条件独立的，所以对于条件分布，我们有$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},…,X^{(n)}=x^{(n)} | Y=c_k)=\\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$这一假设使朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率 如此一来我们就可以得到最终的朴素贝叶斯分类器$$y=argmax_{c_k}P(Y=c_k) \\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$ 后验概率最大化后验概率最大化等价于期望风险最小化，证明如下 参数估计极大似然估计学习意味着估计先验概率和条件概率 先验概率的极大似然估计$$P(Y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)}{N}$$ 条件概率的极大似然估计$$P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl}, y_i=c_k)}{\\sum_{i=1}^{N}I(y_i=c_k)}$$其中假设第$j$个特征$x^{(j)}$可能取值的集合为$\\lbrace a_{j1}, a_{j2},…,a_{jS_j}$，$j=1, 2,…,n; l=1, 2,…,S_j; k=1,2,…,K; x_i^{(j)}$是第$i$个样本的第$j$个特征；$a_{jl}$为第$j$个特征可能取的第$l$个值，$I$为指示函数 算法 输入：训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $，其中$x_i=(x_i^{(1)}, x_i^{(2)},…,x_i^{(n)})^T, x_i^{(j)}$是第$i$个样本的第$j$个特征，$x_i^{(j)} \\in \\lbrace a_{j1}, a_{j2},…,a_{jS_j}$，$a_{jl}$为第$j$个特征可能取的第$l$个值，$j=1, 2,…,n; l=1, 2,…,S_j; y_i \\in \\lbrace c_1, c_2,…, c_K \\rbrace $; 实例$x$； 输出：实例$x$的分类 (1) 计算先验概率$$P(Y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)}{N}, k = 1,2,…,K$$计算条件概率$$P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl}, y_i=c_k)}{\\sum_{i=1}^{N}I(y_i=c_k)}$$ (2) 对于给定的实例$x=(x^{(1)}, x^{(2)},…,x^{(n)})^T$，计算$$P(Y=c_k)\\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$ (3) 确定实例$x$的类$$y=argmax_{c_k}P(Y=c_k)\\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$ 贝叶斯估计由于用极大似然估计可能会出现索要估计的概率值为0的情况(条件概率的某种取值的统计量为0)，会影响到后验概率的计算结果，使分类产生偏差，我们可以采取的做法是在随机变量各个取值的频数上赋予一个正数$\\lambda$，即贝叶斯估计，此时 条件概率的贝叶斯估计为$$P_{\\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl}, y_i=c_k)+ \\lambda}{\\sum_{i=1}^{N}I(y_i=c_k)+S_j \\lambda}$$ 先验概率的贝叶斯估计$$P_{\\lambda}(Y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)+\\lambda}{N+K\\lambda}$$常取$\\lambda=1$称为拉普拉斯平滑 Note:朴素贝叶斯法中假设输入变量都是条件独立的，如果假设他们之间存在概率依存关系，模型就变成了贝叶斯网络。 实现——机器学习实战按照机器学习实战第四章节的内容实现了基础的朴素贝叶斯分类器不同于k-nn和感知机将实力硬性的划分到某一类别，朴素贝叶斯分类器是返回某一实例属于某类别的概率，此外，朴素贝叶斯分类器是一种生成模型，在数据较为缺少的情况下依然有效。 本实验主要是针对文档的分类 根据文档中的词构建词集、词袋子 涉及文档的词向量的解析、提取 将文档中每一个词作为特征，计算先验概率、条件概率 实际应用中使用对数处理了下溢出等问题 实例 使用朴素贝叶斯分类器过滤侮辱性留言 使用朴素贝叶斯分类器过滤垃圾邮件 使用朴素贝叶斯分类器从个人广告中获取地域倾向具体实验代码见github","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"k-NN","slug":"k-NN","permalink":"http://yoursite.com/tags/k-NN/"}]},{"title":"剑指offer阅读笔记","date":"2017-06-09T02:33:40.000Z","path":"2017/06/09/offer/","text":"本文主要是个人针对《剑指offer》一书的一些笔记，并不涉及具体的题目或者方法，只是作为一个整体的知识点框架梳理，便于查缺补漏。 辅助网站 剑指offer第二版作者源代码（C++实现） 第一版面试题Java实现 牛客网上配套练习 第一章面试形式&amp;流程 电面 ——&gt; 远程 ——&gt; 现场 尽可能用形象化的语言把细节描述清楚 不确定问题时，主动提问 编程习惯&amp;调试能力 编程前理清思路 命名、缩进习惯 测试在前、开发在后(编程前尽可能考虑多个测试用例) 项目介绍 STAR模型 —— 简短项目背景，突出自己所做的工作和成绩 技术面5种素质 扎实的基础知识 高质量代码 分析问题时思路清晰 能优化时间、空间效率 学习、沟通能力 基础 编程语言——熟悉程度 数据结构：链表、树、栈、队列、哈希表 算法： 查找、排序：二分查找、归并排序、快排 动态规划、贪心 高质量代码 鲁棒性——检查空指针 特殊输入 边界条件 异常处理 Note: 考虑本身结构的取值范围 输入变量的可能取值 开发之前多想一些测试用例 分析&amp;思路——针对复杂问题 画图使抽象问题形象化 举例使抽象问题具体化 分解使复杂问题简单化 效率 首先要知道如何分析效率 数值各种数据结构的优缺点、并能选择合适的数据结构解决问题 熟练掌握常用的算法 软技能 沟通能力——思路、逻辑、合作 学习能力：读书、新概念 知识迁移能力 抽象建模能力 发散思维能力 关于提问（技术面） 忌 不要问与自己职位无关的问题 不要问薪水（HR面再详谈） 不要打听面试结果 荐 问与应聘职位或项目相关的问题 事先搜集应聘职位、项目背景相关信息 面试中留心面试官说过的话 第二章 —— 基础知识编程语言语言面试的3种类型 概念的理解：eg:关键字（特点及使用场合） 分析代码运行结果 写代码 数据结构 数组&amp;字符串 —— 基本 链表&amp;树 —— 常考，代码鲁棒性 栈 —— 递归 队列 —— 广度优先搜索 算法与数据操作 实现方式：循环或递归 排序&amp;查找：重点（重中之重：二分查找、归并排序、快速排序） 回溯法 —— 递归、栈、二维数组（矩阵）、迷宫问题 动态规划 分析：自上而下 ——&gt; 递归 实现：自下而上 ——&gt; 循环 涉及动态规划求解问题的四个特点 问题目标是求其最优解 整体问题的最优解依赖于各个子问题的最优解 将大问题分解为若干小问题之后，小问题之间还存在相互重叠的更小的公共子问题 自上而下分析问题，自下而上求解问题 贪婪 分解子问题时存在特殊选择 证明是最优解——需要较强的数学功底 位运算：与、或、异或、左移、右移 重要结论：将一个整数减去1后再与原来的整数做位与运算，得到的结果相当于把原整数的二进制表示中的最右边的1变成0 常考点：涉及统计二进制表示中1的个数 第三章——高质量的代码代码的规范性 书写——白板编程 布局——缩进 命名——表明用途、意图 代码的完整性 功能测试 边界测试 负面测试 ###代码的鲁棒性 容错性 防御性编程 在函数入口添加代码以验证用户输入是否符合要求 突破思维局限，多问几个“如果不…那么…” 处理无效的输入 小技巧：链表——双指针，当用一个指针遍历链表不能解决问题时，尝试用两个指针遍历链表，一种思路让两个指针速率不等；另一种思路让一个指针先遍历一段距离。 第四章——解决面试题的思路画图举例分解分治法/动态规划等 第五章——优化时间和空间效率时间效率 编程习惯、细节 循环vs.递归 数据结构&amp;算法功底 知识点：快速排序(partition)、红黑树、最大堆、最小堆 思维能力&amp;激情 时间效率与空间效率的平衡 以空间换时间 权衡：可以与面试官探讨 对于Java需要回顾集合相关的知识点 小结——如何降低时间复杂度 改用更高效的算法 空间换时间 用数组实现简单的哈希表 创建缓存保存中间的计算结果 递归–&gt; 保存求解子问题的结果避免重复计算 例外：针对嵌入式开发，空间换时间不一定可行","tags":[{"name":"校招","slug":"校招","permalink":"http://yoursite.com/tags/校招/"},{"name":"面试","slug":"面试","permalink":"http://yoursite.com/tags/面试/"},{"name":"Coding","slug":"Coding","permalink":"http://yoursite.com/tags/Coding/"},{"name":"剑指offer","slug":"剑指offer","permalink":"http://yoursite.com/tags/剑指offer/"}]},{"title":"统计学习方法笔记(三) —— K近邻","date":"2017-06-06T12:12:28.000Z","path":"2017/06/06/knn/","text":"k近邻算法概述 给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类 算法 输入：训练数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $，其中$x_i \\in R^n$为实例的特征向量,$y_i \\in \\lbrace c_1, c_2,…, c_K \\rbrace$ 为实例的类别,$i=1, 2, 3…, N$； 输出：实例$x$所属的类$y$ 根据所给的距离度量，在训练集中找到与$x$最近的k个点，涵盖这k个点的$x$的邻域记作$N_k(x)$； $N_k(x)$中根据分类决策规则（常用多数表决）决定$x$的类别$y$ $$y = argmax_{c_j} \\sum_{x_i \\in N_k(x)} I(y_i = c_j), i=1, 2,…, N; j=1, 2,…, K$$ 其中$I$为指示函数，当$y_i = c_j$时$I$为1，否则为0 Note: k近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型” k=1时称为最近邻算法 k近邻法没有显式的学习过程 k近邻模型——对特征空间的划分 k近邻法中，当训练集、距离度量、k值以及分类决策规则确定后，对于任何一个新的输入实例，它所属的类唯一地确定，这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。 k近邻法的三个基本要素k值的选择 k值 学习的近似误差 估计误差 特点 较小 下降 上升 整体模型变得复杂，对紧邻的实例点变得非常敏感，容易发生过拟合 较大 上升 下降 模型变得简单，但与输入实例较远的点也会对输入实例点产生影响 Note: 实际应用中，k值一般取一个比较小的数值，通常采用交叉验证法来选取最优的k值。 另外关于近似误差和估计误差，网上没有找到让我满意的答案，目前的一点理解如下： k值越小，学习的近似误差(approximation error)越小，估计误差(estimation error)越大，反之则相反 个人感觉这里有点像偏差和方差的区别，是否可以理解为近似误差即偏差，在k值较小时，选择的邻域范围较小，所以在空间内切割的“比较细”，但与此同时导致模型更加复杂，对于近邻的实例点非常敏感，而估计误差理解为方差，也就是说划分的邻域范围较大，所以平均下来根据分类决策规则可以减小邻域内的噪音点的影响，但是范围大的同时也会产生较远的点对于实例点也产生影响 搜索到的一些结果比如linian2763的博客 估计误差（estimation error）:度量预测结果与最优结果的相近程度近似误差（approximation error）:度量与最优误差之间的相近程度 此外还有stackexchange上的讨论，但是看得更加迷糊。 距离度量——实例点相似程度的反应常用距离度量 欧氏距离 $L_p$距离 Minkowski距离 其中对于$L_p$距离，设特征空间为n维实数向量空间$R^n$，$x_i, x_j \\in R^n$，$x_i = (x_i^{(1)}, x_i^{(2)},…, x_i^{(n)})^T, x_j = (x_j^{(1)}, x_j^{(2)},…, x_j^{(n)})^T$，则$x_i, x_j$的$L_p$距离定义为 $$L_p(x_i,x_j)=(\\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p)^{\\frac{1}{p}}$$ 其中$p \\geq 1$，且当$p=2$时转化为欧氏距离，$p=1$时转化为曼哈顿距离 Note: 不同的距离度量所确定的最近邻点是不同的 分类决策规则常用多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类，多数表决规则等价于经验风险最小化 实现方法——kd树(k维树) 特征空间维数大 训练数据容量大如何对训练数据进行快速k近邻搜索 线性扫描：耗时，不可行 使用特殊的结构存储训练数据，以减少计算距离的次数——kd树 构造kd树概述 kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构 Note: kd树是二叉树 kd树的每个结点对应于一个k维超矩形区域，而各实例点存在于不同的超矩形区域内，即在kd树的不同结点里 一般这个k维就是数据实例点的维度，即特征数，每次选定一个特征，然后根据范围内的实例点（记录）的该特征的值来进行二分，过程有点像快排里的分割 算法 输入：k维空间数据集$T= \\lbrace x_1, x_2,…, x_N \\rbrace $，其中$x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(k)})^T, i=1,2,…,N$； 输出：kd树 （1）开始：构造根结点，根结点对应于包含T的k维空间的超矩形区域。 选择$x^{(1)}$为坐标轴，以T中所有实例的$x^{(1)}$坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。由根结点生成深度为1的左、右结点，左子结点区域的$x^{(1)}$坐标对应的值小于切分点的子区域；右子结点区域的$x^{(1)}$坐标对应的值大于切分点的子区域。将落在切分超平面上的实例点保存在根结点。 （2）重复：对于深度为j的结点，选择$x^{(1)}$为切分的坐标轴，$l=(j \\mod k) + 1$，以该结点区域中所有实例的$x^{(1)}$坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。由该结点生成深度为j+1的左、右结点，左子结点区域的$x^{(1)}$坐标对应的值小于切分点的子区域；右子结点区域的$x^{(1)}$坐标对应的值大于切分点的子区域。将落在切分超平面上的实例点保存在该结点。 （3）直到两个区域没有实例存在时停止。 Note: 书中的方法是轮流按每个维度对某结点进行切分，各个维度（特征）可能会被重复利用来进行切分，当然此时的实例点范围不同，切分点也不同（中位数发生变化） 对于kd构造的实例，其一书中的例子比较直观，如下此外，在wiki里的一幅三维空间的配图也是非常直观 一个三维k-d树。第一次划分（红色）把根节点（白色）划分成两个节点，然后它们分别再次被划分（绿色）为两个子节点。最后这四个子节点的每一个都被划分（蓝色）为两个子节点。因为没有更进一步的划分，最后得到的八个节点称为叶子节点。 搜索kd树 利用kd树可以省去对大部分的数据点的搜索，从而减少搜索的计算量 算法 输入：已构造的kd树；目标点x； 输出：x的最近邻 （1）在kd树中找到包含目标点x的叶结点：思想有点像二分查找，从根节点出发，递归地向下访问kd树，若目标点x当前维的坐标小于切分点的坐标，则移动到左子节点，否则移动到右子节点，直到子节点为叶节点为止 （2）以此叶节点为“当前最近点” （3）递归地向上回退，在每个节点执行以下操作： （a）如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点” （b）当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。即以目标点为球心，以目标点与当前最近点间的距离为半径的球体是否与另一子结点对应的区域相交。如果相交，可能在另一个子结点对应的区域中存在更近的点。移动到另一个子结点，接着递归地进行最邻近搜索。如果不相交，则向上回退。 （4）当回退到根结点，搜索结束，此时的当前最近结点即为x的最邻近点。 Note: 若实例点随机分布，kd树搜索的平均计算复杂度是O(log N)，N为训练实例数 kd树更适用于训练实例数远大于空间维数时的k近邻搜索 关于这个算法的理解最好还是看书上的例子3.3，基本思路是一个重复二分查找和回溯的过程，另外关于kd树的搜索，有一篇很不错的文章，详见kd树的细致图文讲解 实现——机器学习实战按照机器学习实战关于k-NN的章节实现了原始的k-NN分类器，并做了几个小实例： 首先是一个基础的原理理解，对于二维空间的点按照坐标之间的距离进行分类，意图是理解k-NN的基本原理，即基于实例的学习算法，按照距离分类 在实现了原始分类器的基础上构建小应用，使用约会网站的异性数据，针对三项特征来判断新输入的异性是否有魅力 同样基于原始分类器，针对二进制点阵构成的数字图像文本文件进行识别和分类 上述三个例子都是使用的同一个原始分类器，不同之处仅仅是输入的数据有所改变，此外为了使输入数据能够被原始分类器处理，针对不同数据的特征进行了不同的处理： 针对约会网站数据由于各项特征的取值范围不同可能对于距离计算产生影响所以进行了归一化操作 对于图像文本文件将其由32*32的二进制点阵转换为1*1024的向量便于分类器处理 此外还有一些基础的文件格式化为矩阵以及可视化操作 原始的Jupyter Notebook可以参考我的github","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"k-NN","slug":"k-NN","permalink":"http://yoursite.com/tags/k-NN/"}]},{"title":"统计学习方法笔记(二) —— 感知机","date":"2017-06-03T08:53:02.000Z","path":"2017/06/03/Perceptron/","text":"概述 感知机（perceptron）是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值。感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于判别模型。感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此导入了基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。感知机预测是用学习得到的感知基模型对新的输入实例进行分类。感知机是神经网络与支持向量机的基础。关键字：二分类，线性分类模型，分离超平面，判别模型，基于误分类的损失函数，随机梯度下降法 感知机模型——分离超平面数学表达$$f(x) = sign(w \\cdot x + b)$$ 输入空间：$R^n$ 输出空间：$\\lbrace+1, -1 \\rbrace$ 假设空间：$\\lbrace f|f(x) = w \\cdot x + b\\rbrace$，表示定义在特征空间中的所有线性分类模型或线性分类器 $w \\in R^n$，其中$w$表示权值向量，几何意义为超平面的法向量 $b \\in R$，$b$表示偏置，几何意义为超平面的截距 Note:模型学习的目的在于通过训练集求得模型的参数$w$和$b$ 几何解释 感知机的几何解释：线性方程$w \\cdot x + b = 0$ 对应特征空间$R^n$中的一个超平面$S$,其中$w$是超平面的法向量，$b$是超平面的截距，这个超平面将特征空间划分为两个部分，位于两部分的点（特征向量）分别被分为正、负两类，因此，超平面$S$称为分离超平面。 感知机的学习策略数据集的线性可分性给定一个数据集$T={(x_1, y_1), (x_2, y_2),…,(x_N, y_N)}$,其中$x_i \\in R^n$,$y_i \\in \\lbrace+1, -1 \\rbrace$,$i=1, 2, 3…, N$，若存在超平面$S$设为$w \\cdot x + b = 0$将数据集的正实例点和负实例点完全正确地划分到$S$两侧，则称数据集$T$为线性可分数据集 学习策略 目标：求得一个能将训练集正实例点和负实例点完全正确分开的分类超平面——&gt;确定$w, b$ 转化：经验损失函数最小化——基于误分类 直观思路：误分类点总数，非$w, b$的连续可导函数，不易优化 修改：误分类点到超平面$S$的总距离$$L(w, b)=-\\sum_{x_i \\in M} y_i(w \\cdot x_i + b)$$,其中$M$为误分类点的集合，损失函数$L(w, b)$是$w, b$的连续可导函数。(简单的解释一下，此处计算误差取的是所有误分类点的集合，所以式中$y_i(w\\cdot x_i+b)$符号为负，这是前面的负号的由来，另外表示点到平面的距离的主要是$w\\cdot x_i+b$这部分，其中$x_i$为平面外一点，$w,b$分别为平面的法向量和截距，这里的计算省略了绝对值符号（原因同负号的由来），另外省略了分子法向量的平方和，应该是求解最小化误差时，该项不影响所以简化了计算，具体的推导可以参见下面的链接) Note: 感知机的学习策略是在假设空间中选取使损失函数最小的模型参数$w, b$，即感知机模型。 点到平面的距离推导 范数的通俗解释 另外关于范数、规范化的理解，这篇博文写的深入浅出 感知机学习算法原始形式概述随机梯度下降法 输入：训练集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $，其中$x_i \\in R^n$,$y_i \\in \\lbrace+1, -1 \\rbrace$,$i=1, 2, 3…, N$，学习率$\\eta (0&lt; \\eta \\leq 1)$ 输出：$w, b$;感知机模型$f(x) = sign(w \\cdot x + b)$ (1) 选取初值$w_0, b_0$ (2) 在训练集中选取数据$(x_i, y_i)$ (3) if $y_i(w \\cdot x_i + b) \\leq 0$即为误分类点 $w \\leftarrow w + \\eta y_i x_i$ $b \\leftarrow b + \\eta y_i $ (4) 转至(2)，直至训练集中没有误分类点 几何解释当一个实例点被误分类，即位于分离超平面的错误的一侧时，则调整$w, b$的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。 随机梯度下降法 首先任意选取一个超平面$w_0, b_0$，然后用随机梯度下降法不断地极小化目标函数 $$L(w, b)= - \\sum_{x_i \\in M} y_i(w \\cdot x_i + b)$$ 极小化过程中不是一次使$M$中的所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降 若误分类点集合M固定，损失函数的梯度计算 $$\\frac{\\partial L(w,b)}{\\partial w}=- \\sum_{x_i \\in M}y_i x_i$$ $$\\frac{\\partial L(w,b)}{\\partial b}=- \\sum_{x_i \\in M}y_i$$ 随机选取一个误分类点$(x_i, y_i)$对$w, b$进行更新： $$w \\leftarrow w + \\eta y_i x_i$$ $$b \\leftarrow b + \\eta y_i $$ 通过迭代可使损失函数$L(w, b)$不断减小，直到为0。 算法的收敛性证明如下 另外可以看看其他人写的证明过程 Note: 当训练数据集线性可分时，感知机学习算法存在无穷多个解，其解由于不同的初值或不同的迭代顺序而可能有所不同。相对的，线性可分支持向量机则克服了这一点，根据间隔最大化从无穷多个超平面中唯一确定了一个 对偶形式基本思想将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得$w, b$。 假设初始值$w_0, b_0$均为0，对误分类点$(x_i, y_i)$，通过 $$w \\leftarrow w + \\eta y_i x_i$$ $$b \\leftarrow b + \\eta y_i $$ 逐步修改$w, b$，假设修改$n$次，则$w, b$可分别表示为： $$w= \\sum_{i=1}^{N} \\alpha_i y_i x_i$$ $$b= \\sum_{i=1}^{N} \\alpha_i y_i$$ 每个实例点对应有一个$\\alpha_i$满足$\\alpha_i \\geq 0$且当$\\eta = 1$时，$\\alpha_i$就表示第$i$个实例点由于误分类而进行更新的次数。 实例点更新次数越多，意味着它距离分离超平面越接近，也就越难以正确分类，换言之，这种实例对学习结果影响最大（有点支持向量机中的支持向量的意思）。 算法 输入：线性可分的数据集$T= \\lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \\rbrace $，其中$x_i \\in R^n$,$y_i \\in \\lbrace +1, -1 \\rbrace$,$i=1, 2, 3…, N$，学习率$\\eta (0&lt; \\eta \\leq 1)$ 输出：$\\alpha, b$；感知机模型$f(x) = sign(\\sum_{j=1}^{N} \\alpha_j y_j x_j \\cdot x + b)$,其中$\\alpha=(alpla_1, alpla_2,…,alpla_N)^T$(1): $\\alpha \\leftarrow 0$, $b \\leftarrow 0$(2): 计算所有样本内积形成的Gram矩阵$G$， $$G=[x_i \\cdot x_j ]_{N \\times N}$$ (3): 在训练集中选取数据$(x_i, y_i)$，若 $$y_i (\\sum_{j=1}^{N} \\alpha_j y_j x_j \\cdot x_i + b) \\leq 0$$ （计算过程中可通过查$G$的值来提高效率）则更新： $$\\alpha_i \\leftarrow \\alpha_i + \\eta$$ $$b \\leftarrow b + \\eta y_i$$(4): 转至(3)直至没有误分类数据 Note:为什么要引入对偶形式？ 首先原始形式中，书中为何要使用随机梯度下降而非批量梯度下降法，个人搜索到的一篇博文里感觉说的有点道理，引用如下 用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，只有误分类的M集合里面的样本才能参与损失函数的优化。所以我们不能用最普通的批量梯度下降,只能采用随机梯度下降（SGD）或者小批量梯度下降（MBGD）。 此外，关于对偶形式的优势，小结一下： 对偶形式将权重向量$w$转化为实例$x_i$和标记$y_i$的线性组合形式，且在原书中也提到，对偶形式中的训练实例仅以内积的形式出现，所以可以预先使用Gram矩阵存储，也就是空间换时间的方法提高计算效率 书中这里应该也有点为后续介绍支持向量机做铺垫，所以这里为核函数的引入埋一个伏笔，毕竟感知机是神经网络与支持向量机的基础，而且后面书中在支持向量机部分的讲解也多次使用对偶形式的求解 参考资料 李航《统计学习方法》 点到平面的距离推导 范数的通俗解释 关于范数、规范化的理解 http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf http://www.cnblogs.com/pinard/p/6042320.html https://www.zhihu.com/question/26526858","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"},{"name":"Perceptron","slug":"Perceptron","permalink":"http://yoursite.com/tags/Perceptron/"}]},{"title":"统计学习方法笔记(一)","date":"2017-06-02T06:42:52.000Z","path":"2017/06/02/Note-StatisticalML/","text":"本系列文章是个人根据阅读李航博士的《统计学习方法》一书，辅以《机器学习实战》、scikit-learn官方文档等材料整理出来的笔记。 统计学习相关概念定义 统计学习(statistical learning)是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测和分析的一门学科 研究对象数据 研究目的对数据进行预测和分析：学习什么样的模型和如何学习模型（准确、效率） 研究方法构建模型并应用模型进行预测和分析（统计学习方法三要素） 模型：模型的假设空间，即学习模型的集合 策略：模型选择的准则 算法：模型学习的算法，求解最优模型的算法 分类 监督学习 非监督学习 半监督学习 强化学习 监督学习概览 输入空间：输入所有可能取值的集合 输出空间：输出所有可能取值的集合 特征空间： 每个具体的输入是一个实例，通常由特征向量表示，所有特征向量存在的空间称为特征空间 有时输入空间与特征空间为相同的空间，不予区分，有时为不同的空间，需要将实例从输入空间映射到特征空间，模型实际上都是定义在特征空间上 假设空间：学习模型的集合（假设要学习的模型属于某个函数集合），模型属于由输入空间到输出空间的映射的集合，这个集合即假设空间，假设空间的确定意味着学习范围的确定。 监督学习的模型 概率模型：条件概率分布$P(Y|X)$ 非概率模型：决策函数$Y=f(X)$ 联合概率分布监督学习假设输入与输出的随机变量X和Y遵循联合概率分布$P(X, Y)$(监督学习关于数据的基本假设) 监督学习分类 回归问题：输入变量与输出变量均为连续变量的预测问题。 分类问题：输出变量为有限个离散变量的预测问题。 标注问题：输入变量和输出变量均为变量序列的预测问题。 问题形式化学习与预测——训练与测试 学习：利用给定的训练数据集，通过学习（训练）得到一个模型； 预测：对于给定的测试样本中的输入，由所得到的模型给出相应的输出。 统计学习方法三要素(针对监督学习)模型学习什么样的模型，在监督学习过程中，模型指的就是所要学习的条件概率分布或决策函数。 策略（评价标准）模型选择的准则：按照什么样的准则学习或选择最优的模型 损失函数（代价函数）：一次预测的好坏的度量，常用的包括0-1损失函数、平方损失函数、绝对损失函数、对数(似然)损失函数 风险函数（期望损失）：度量平均意义下模型预测的好坏 经验风险 结构风险 将监督学习的问题转化为经验风险或结构风险函数的最优化问题（结构风险最小的模型即最优模型） Note: 在模型选择中，理论上应该使用期望风险函数作为标准，但是由于输入，输出的基本假设$P(X, Y)$联合分布未知（若已知则可直接求解P(Y|X)，不用学习），所以试图使用经验风险（平均损失）来替代，但又由于现实中限于样本容量不会很大，所以要对经验风险进行矫正，于是有了结构风险，在经验风险的基础上添加正则化项来防止过拟合。 最大似然（MLE），最大后验（MAP）都是构造目标函数的方法 算法（求解&amp;优化）模型学习的算法，即学习模型的具体计算方法 用什么样的计算方法求解最优模型（寻找全局最优解） 如何高效实现 Note: 一句话小结：针对某一问题，我们提出了一种假设（模型），然后制订了解决该问题的目标（策略/损失函数）；接下来就是采取某种方式来达成这一目标（算法/优化） 常用优化方法大致可以分为两类，一类使用函数的梯度信息，包括一阶的方法，例如梯度下降，以及二阶的方法，例如牛顿法等。当然，还有和梯度无关的方法，例如 fixed point iteration，坐标下降等等 模型选择模型评估训练误差&amp;测试误差 训练误差：对判断给定的问题是不是一个容易学习的问题是有意义的，但若一味追求减小训练误差，会出现过拟合的情况（所选模型的复杂度比真模型高） 测试误差：反映了学习方法对未知的测试数据集的预测能力，测试误差小的方法具有更好的预测能力所以需要选择复杂度适当的模型，而模型选择常用的两种方法分别是正则化和交叉验证。 正则化结构风险最小化策略的实现，在经验风险的基础上加一个正则化项/罚项 $$ R_{srm}(f)=\\frac{1}{N}\\sum L(y_i,f(x_i))+\\lambda J(f) $$ 作用是为了选择经验风险和模型复杂度同时较小的模型(在所有可能选择的模型中，能够很好地解释已知数据并且十分简单的，才是最好的模型) 交叉验证重复的使用数据 数据集划分 训练集：训练模型，通过输入数据学习函数中的参数值，得到拟合了数据的“分类器/回归器”等 验证集：模型选择，从不同的模型中选择最佳模型 从不同类的模型中选择（比如从SVM,GBM,决策树里选择模型） 从同类模型不同超参数(hyperparameter)组合里选择最优超参数组合。 测试集：最终对学习方法的评估常用方法 简单交叉验证 S折交叉验证 留一交叉验证 Note:关于数据集划分的一点理解理解可以参看交叉验证，而关于模型选择可以进一步了解参数与超参数 学习的泛化能力定义：学习方法的泛化能力(generalization ability)是指由该方法学习到的模型对未知数据的预测能力。评价方法 理论上：泛化误差即期望风险$$ R_{exp}(f)=E_p[L(Y, f(X))]=\\int L(y,f(x))P(x,y)dxdy $$ 实际应用中：常常用测试误差衡量 泛化误差上界通过比较两种学习方法的泛化误差上界来比较其优劣（越小越好） 样本容量越大，泛化误差上界越小 假设空间越大，泛化误差上界越大 训练误差小的模型，其泛化误差也会小 生成模型与判别模型监督学习模型的一般形式 概率模型：条件概率分布$P(Y|X)$ 非概率模型：决策函数$Y=f(X)$ 监督学习方法 - 生成方法 判别方法 定义 由数据学习联合概率分布$P(X, Y)$,然后求出$P(Y&#124;X)$作为预测的模型,$P(Y&#124;X)=\\frac{P(X,Y)}{P(X)}$ 由数据直接学习决策函数$f(x)$或条件概率分布$P(Y&#124;X)$作为预测模型 特点 1.可还原出$P(X, Y)$；2.学习收敛速度更快；3.存在隐变量时仍可用 1.直接面对预测，准确率更高；2.便于数据抽象，特征定义和使用，可简化学习问题 典型模型 朴素贝叶斯法、隐马尔可夫模型 k-近邻、感知机、决策树、逻辑斯谛回归模型、最大熵模型、SVM、提升方法、条件随机场等 Note 模型表示了给定输入X产生输出Y的生成关系 判别方法关心的是对给定的输入X，应该预测什么样的输出Y 监督学习方法的应用分类问题在监督学习中，当输入变量Y取有限个离散值时，预测问题便成为分类问题 多类分类问题，包括二分类 评价指标 分类准确率 精确率&amp;召回率（二分类）——正类、负类、F1值 标注问题可以认为是分类问题的一种推广，输入观测序列，输出标记序列(状态序列) 学习&amp;标注——条件概率分布 评价指标 标注准确率 精确率、召回率 常用统计学习方法 隐马尔可夫模型 条件随机场 应用领域 信息抽取 自然语言处理 回归问题回归用于预测输入变量（自变量）和输出变量（因变量）之间的关系——函数拟合 学习&amp;预测 一元回归 vs 多元回归 线性回归 vs 非线性回归 损失函数：常用平方损失函数—— 最小二乘法 参考资料 李航《统计学习方法》 周志华《机器学习》 知乎-最小二乘、极大似然、梯度下降有何区别？","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/统计学习方法/"},{"name":"Note","slug":"Note","permalink":"http://yoursite.com/tags/Note/"}]},{"title":"关于交叉验证和模型选择的一点思考","date":"2017-06-01T12:28:36.000Z","path":"2017/06/01/cv/","text":"首先按照《统计学习方法》第一章的内容，常用的模型选择方法有两种，按照结构风险最小化思想的具体实现即正则化以及数据驱动的交叉验证方法。 CV是用于模型的(评估)挑选和超参数(关于超参数和参数的区别请见另一篇memo)调整的，首先明确一点，所谓模型，指的是我们用来描述输入数据与最终需要预测的输出数据之间的关联的方法，而不是不同数据拟合的模型得到的实例，比如我们可以说一个线性回归模型，但是不会说用不同数据拟合的线性回归器为不同的模型； 为什么要用CV，这涉及到数据划分的问题，首先我们训练好一个模型后需要评估这个模型的效果，但是如果把全部数据都投入训练就没有数据来进行验证了，常规而言，可以将数据集划分为训练集和验证集比如80%训练、20%验证，但是这样会有一些问题，首先有可能你很不巧的将一些特殊数据划分到这20%的验证集里了(要么效果很好、要么效果很糟糕)，这样的话仅仅这么一组验证集的评估效果就很不稳定也不确切；此外我们划分训练集和验证集就减少了训练数据，而一般而言数据越多训练出的模型方差是越小的，从这个角度来看数据越多一般我们训练的效果更好； 那么有没有方法既可以用上所有数据进行验证和评估、最后又可以用所有数据进行训练呢？所以我们就需要交叉验证，比如五折交叉验证，假设我们仍将数据集8-2划分（80%-20%），对于五折交叉验证，我们就是针对模型进行了五次训练，每次取80%训练数据、20%验证数据，并最终保证每个数据都曾在这五次训练验证中作为20%的验证集对模型进行过评估，这样我们就可以确保我们使用了所有数据对我们的模型进行评估 为什么不用CV中得到的预测器进行预测？一般而言，在做交叉验证时确实可能出现某一组交叉验证的得分较高，我们会试图用这一组数据进行模型的拟合和最终预测，但是这种得分高只是一种表面现象，首先数据量少了，其次这一组验证集并非独立的，是在整个交叉验证中随机生成的，所以这个预测器的结果到底好不好还需要在对额外的数据进行测试，如果数据充足也许你可以用嵌套式的交叉验证进行实验，即第一步用常规内层交叉验证确定最佳模型，然后采用数据驱动的方式(外层交叉验证)拟合最佳的一组预测器 所以CV的作用是用来对不同模型（SVM和生成树等）或者不同(超)参数组合的模型中评估各个模型的效果，而得到最佳(超)参数组合的模型；接下来将所有训练集投入进去拟合预测得到拟合好的预测器最后对测试集进行预测 那么把所有训练集放到模型中会不会导致过拟合呢？答案是不会，过拟合产生的原因是模型过于复杂（模型的参数），而不是数据增加导致，（而不是传入参数的值），增加数据一般而言更有利于训练集的训练，因为一般而言，训练数据越充足，越能反映出真实数据的分布情况，得到一种近似无偏估计的结果 补充：关于生成树模型中的early_stopping，按照《统计学习方法》一书的第12章总结部分P213所述 提升方法没有显示的正则化项，通常通过早停止(early stopping)的方法达到正则化的效果 所以early_stopping属于正则化的范畴，是另一种模型选择的具体方法。 参考资料https://stats.stackexchange.com/a/52277/152084http://scikit-learn.org/stable/modules/cross_validation.htmlhttps://stats.stackexchange.com/a/52312/152084","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"CV","slug":"CV","permalink":"http://yoursite.com/tags/CV/"},{"name":"model selection","slug":"model-selection","permalink":"http://yoursite.com/tags/model-selection/"}]},{"title":"机器学习中模型的参数和超参数","date":"2017-06-01T12:24:11.000Z","path":"2017/06/01/parameter-hyperparameter/","text":"一直以来对于机器学习中的模型训练和模型选择存在一个误区，首先机器学习力的模型通俗来说就是一个函数关系，表明输入数据到输出数据的映射，基本的假设前提是输入数据和输出数据符合某种联合概率分布，而模型训练的过程其实就是在确定函数式的具体参数值的过程，比如假设你要做一个多项式回归分析的模型，比如$f(x)=w_1x_1+w_2x_2+w_3x_3$，那么模型训练的过程中其实就是在学习对应的w的值，那么问题来了，实战中所谓的模型调参来选择模型又指的是什么呢？既然训练已经把参数都确定下来了，那我们调整的参数又是什么？原来这里有个误区在于模型中的parameter和hyperparameter的区别，按照搜集到的资料来看，其实模型中可以分为两种参数，一种是在训练过程中学习到的参数，即parameter也就是上面公式里的w，而另一种参数则是hyperparameter，这种参数是模型中学习不到的，是我们预先定义的，而模型的调参其实指的是调整hyperparameter，而且不同类型的模型的hyperparameter也不尽相同，比如SVM中的C,树模型中的深度、叶子数以及比较常规的学习率等等，这种参数是在模型训练之前预先定义的，所以关于模型的选择其实更多的指的是选择最佳的hyperparameter组合。 参考资料https://datascience.stackexchange.com/a/14234/31117https://www.quora.com/What-are-hyperparameters-in-machine-learning","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"memo","slug":"memo","permalink":"http://yoursite.com/tags/memo/"},{"name":"parameter","slug":"parameter","permalink":"http://yoursite.com/tags/parameter/"},{"name":"hyperparameter","slug":"hyperparameter","permalink":"http://yoursite.com/tags/hyperparameter/"}]},{"title":"计算机与生活","date":"2017-05-30T11:57:01.000Z","path":"2017/05/30/computer-life/","text":"阅读完吴军的《浪潮之巅》，给人的感觉颇有点当初看纪录片《互联网时代》的感觉，简单而言就是你透过作者的眼睛快速浏览了一遍计算机这个行业的发展历程；手头的这本是第二版，看出版时间也是2012年了，到现在算算有点老了，不过对于12年以前发生的一些事情还是能有一个大致的梳理； 这本书上册主要讲述了一些在历史中唱过主角的公司，关于这些公司的兴衰以及作者自己的一些分析判断，下册则更宽泛一点，除了谈谈部分公司，还科普了一些计算机工业界以及商业方面的概念，比如谈信息产业的一些规律，一些公司的商业模式甚至风险投资，金融风暴等等； 下面我想简单谈一谈自己读过这本书的一点点体会，关于计算机和人类的生活。 我粗略的将计算机的发展历程划分为四个大的阶段，分别是计算时代—自动化时代—互联网时代—数据时代；计算时代比较有代表性的是计算机ENIAC，那个时候还主要用作军事用途，用来计算导弹轨迹之类的，离走进我们的生活还比较远，第二阶段自动化时代比较有代表性的就是微机了（当然这里跳过了面向企业的工作站，主要谈与个人生活相关的），按照作者的说法，这一阶段最有代表性的是三个公司，苹果的个人电脑（一体化）和微软与Intel的联盟（WinTel体系），这一阶段计算机真正开始走入普通人的家庭生活，主要是用于协助处理一些日常的办公工作，属于单机阶段；然后是大家比较熟悉的互联网时代，这一阶段的强大之处在于将计算机连接起来，于是每个用户都不再是孤立的，随着因特网的出现，有一部分人将本地的内容放到了网上，这个时候网上的内容不多，但是比较杂乱无序，所以随之诞生了门户网站比如雅虎等等，他们将互联网上的内容分门别类从而使用户便于查看和获取信息；而随着建立网站的门槛降低，互联网上的内容越来越多，单纯靠人工分类的的门户网站已经满足不了人们的需求了，于是搜索引擎（代表是Google）应运而生，帮助用户在浩瀚的互联网中快速找到自己所需的信息； 如果说互联网时代的关键词是‘内容’；那么数据时代的关键词就是平台了。这个时候，人们已经不满足于只是从互联网上获取信息了，就像不满足于只是从电视接受固定的节目一样，用户开始有‘发出自己的声音’的诉求，也就是创作；于是各个平台相继出现，最有代表性的，早期的Blog可以允许用户自己在网上发布自己的文章，然后是Facebook、Twitter不仅为用户构建了虚拟的社交圈，而且让用户可以在这个圈子里随时随地发出自己的声音并与其他人互动，还有YouTube把平台交给用户，让用户自己去发布自己的视频和收看其他用户的视频；这一阶段的特点是‘平台’ ，比较有代表性的公司并不提供内容，而是让用户自己来做内容的创造者。 而随之而来的是数据的爆发，互联网上的信息变得更加庞杂，在不断炒来炒去的概念‘云计算’，‘大数据’的驱动下，计算机的同学们纷纷投身到数据分析/数据挖掘的工作中，希望能够从庞杂的数据中挖掘出与实际业务相关的有价值的信息；那么，下一步人们的需求在哪呢？是通过数据挖掘对一个人建立画像从而实现对每个人的私人定制服务？还是现在炙手可热的基于VR/AR的虚拟社交？按照前段时间看到的一个关于区块链的演讲，在未来每个人都可以被“数字化”，真实世界的你可以投影到虚拟世界成为一个数字化的、独一无二的你，这大概是一种趋势吧。 最后还是推荐下这本书，作为科普类的读物挺不错的。","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"IT","slug":"IT","permalink":"http://yoursite.com/tags/IT/"}]},{"title":"来来来，我教你搭个博客好不好哇","date":"2017-05-30T10:40:58.000Z","path":"2017/05/30/my-blog/","text":"先把我的博客贴一下Lancelot’s Desert 端午节两天时间宅实验室把自己搭建一个博客这么个‘历史遗留问题’解决了下，其实之前也用Hugo搭建过，最后给弄崩了，这次尝试下Hexo，发现倒是异常的顺利，一方面Hexo的整体生态比较完备，另一方面网上找到的教程也很靠谱，下面就把我搭建过程中的一些步骤和踩过的坑记录下。 基础博客搭建首先声明下，基础博客搭建基本上和我所参照的崔斯特的教程是一致的，只是在顺序和语言上重新组织了一下，因为原博主写的教程已经很简明扼要了。 准备工作 git下载 github账号 node.js 另外的话写博客最好熟悉一点markdown的基本语法，常用的指令不多而且很简单，用的熟练是非常不错的工具，这里也推荐一个markdown的不错的在线编辑器Cmd markdown即时预览的，此外，类似简书、SegmentFault这些网站也是支持MarkDown编辑的，所以如果熟悉这个语法的话还是很方便的。 下载和注册基础搭建的话首先需要下载git和node.js(npm)来进行控制台命令的一些输入，以及需要一个github账号，因为我们的博客是挂载在github上的。 在下载好git和node.js并注册好一个github账号后，首先新建一个repository，名称和你的账户名一致，后面添加.github.io域名，比如我的用户名是LancelotHolmes,那么我的repository命名就是LancelotHolmes.github.io 环境配置在完成上述操作并安装好git和node.js之后，我们选择一个路径新建一个文件夹比如我是在D盘的MyBlog，然后执行git bash，可以在开始里搜索，也可以右键然后选择git bash here,在出现的git控制台中输入之前注册的github账号相对应信息，比如1git config --global user.name \"你的账户名\" 1git config --global user.email \"注册github账号时的邮箱\" 如图 然后是安装Hexo,直接输入 1npm install -g hexo-cli 开始搭建同样在MyBlog(或者你命名的目录下)，仍然是刚刚的控制台界面，输入1hexo init blog 成功的时候会显示 1INFO Start blogging with Hexo! 接下来进入blog目录下，输入123hexo cleanhexo ghexo s 或者你也可以新建一个generate.sh脚本文件将上面三条语句写入,因为后面线下测试会多次用到，可以直接在控制台输入./generate.sh，然后在浏览器里输入http://localhost:4000/,这个时候你就可以看到一个网页的基本雏形，这是由于你的\\blog\\source\\_posts路径下已经有一个基本的markdown文件了，而且本身下载的Hexo带了一个landscape的主题文件，在路径\\blog\\themes下可以看到 配置githubSSH回到控制台，现在我们需要生成SSH，仍然是在git控制台里，输入 1ssh-keygen -t rsa -C \"Github的注册邮箱地址\" 基本是一直回车，到出现信息1Your public key has been saved in /c/Users/user/.ssh/id_rsa.pub. 在信息对应的路径下找到这个文件，打开它(我是用sublime text打开的)，在你的github界面，右上角头像里选择Settings,左侧选择SSH and GPG keys然后新建一个SSH key,名称随你定，比如我是设置的blog,内容的话把刚刚id_rsa.pub里的内容全选复制进去就好了。 站点配置在blog目录下，用sublime或者其他编辑器打开_config.yml文件，找到对应的字段修改下面的基本参数信息，这里需要注意一下存在多个_config.yml文件，一个是在blog目录下，作为站点配置文件，一般用来做一些常规的配置，另外在各个主题的目录下还有一个_config.yml文件用来进行特定主体的一些个性化设置，后面会经常用到这两个文件，另外就是下面的配置注意:之后的空格 博客基本信息1234title: 博客名称subtitle: 副标题description: 网页描述author: 作者名 推送设置（这里的repo注意修改为自己的github的对应格式）1234deploy: type: git repo: https://github.com/LancelotHolmes/LancelotHolmes.github.io.git branch: master 新建文章在控制台输入 1hexo n \"文章名称\" 这样会在\\blog\\source\\_posts目录下生成对应的markdown文件，你可以通过编辑器打开它并书写你的文章，比如保存后同样的执行./generate.sh然后打开http://localhost:4000/就可以看到你刚刚写好的的文章了。 当然我这里展示的界面略有不同，因为我这里设置主题，后面会具体介绍。 推送到github线下测试发现没有什么问题我们就可以推送到github了，输入如下命令，或者保存为脚本文件deploy.sh然后执行./deploy.sh第一次部署到github时可能会出错error deployer not found:github，可以在控制台输入,注意--save必不可少1npm install hexo-deployer-git --save 12hexo cleanhexo d -g 过程中会让你输入你的github账号和密码，推送成功后，你就可以通过在浏览器输入你的静态站点名称访问你的博客了，比如LancelotHolmes.github.io,至此一个基本的博客就搭好了，接下来你每次需要写文章只需要经过如下步骤 在blog路径下打开git bash控制台然后输入hexo n &quot;文章名&quot; 在路径\\blog\\source\\_posts中编辑对应的markdown，编辑好后保存 执行generate.sh进行线下预览（可选） 执行deploy.sh推送到github就可以通过你的站点访问啦 配置yilia主题前面也给大家看到了我的博客的截图，这里我使用的是yilia主题，目前Hexo主题里面比较热门的两大主题是Next和yilia,这里我就我所配置的一些功能和踩过的坑记录一下。主要包括一些基本的配置以及优化 基本配置基本的主题下载和安装可以直接参照yilia的github对应的教程，同样在blog目录下执行git bash控制台，输入1git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 然后修改站点配置文件，Hexo根目录下(即blog目录的)的 _config.yml1theme: yilia 然后再该文件末尾添加如下语句123456789101112131415161718jsonContent: meta: false pages: false posts: title: true date: true path: true text: true raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true 添加disqus评论目前使用的评论比较多，我早期Hugo时使用过disqus也就还是用这个了，主要是需要注册一个disque账号，然后修改主题目录下的_config.yml 文件，特别注意这里的路径是\\blog\\themes\\yilia,不再是blog下的文件，后面大部分配置都是针对这个文件1disqus: LancelotHolmes 这里的修改为你注册的disqus的short-name,yilia主题下的配置会优先覆盖blog下的配置，如果你是使用其他的评论比如多说，顺言之类的应该适时修改相应的字段后面的false为对应的值，效果如下 添加menu原始的主题menu是这样的123menu: 主页: / 随笔: /tags/随笔/ ，我们可以修改为我们所需要的‘类别’，注意由于yilia作者没有预先设置类别（category）而是把他当作tags使，所以这里配置时链接到对应的tags下的路径，修改如下1234menu: 主页: / 阅读: /tags/Reading/ 机器学习: /tags/ML/ 可以根据需要在新建文章是设置标签自动生成对应的路径，可以在\\blog\\public\\tags下看到，然后修改menu下的对应字段即可 RSS &amp; Sitemap为什么要用RSS,可以看这篇短文，主要是为了方便对你的博客感兴趣的人将你的博客添加到他的订阅列表中，一旦你有更新他可以在第一时间接收到推送。而sitemap则主要是给搜索引擎用的，方便你的站点能够被google收录，当然这里首先需要绑定一个域名，后续我们会具体介绍。这部分主要是参照voidking和magicwangs的博客，执行如下语句123npm install hexo-generator-feed --savenpm install hexo-generator-sitemap --save 然后照常的执行generate.sh，你可以在路径\\blog\\public下看到生成的文件atom.xml和sitemap.xml,接下来在主题目录下的_config.yml 文件，特别注意这里的路径是\\blog\\themes\\yilia里添加123456# SubNavsubnav: github: \"#\" weibo: \"#\" rss: /atom.xml ... 此外在blog目录下的站点配置文件里(这回是在\\blog路径下的_config.yml)添加如下语句1234567891011# Sitemapsitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xm# RSSfeed: type: atom path: atom.xml limit: 100 头像设置有采用本地图片的，也有将图片传到网上制成外链的，我就是用后面那种方法，这里给大家推荐一个不错的工具sm.ms，方便你把你的本地图片传到网上制成markdown、html、url等格式的外部链接制作好后，修改主题目录下的_config.yml 文件，注意这里的路径是\\blog\\themes\\yilia里 12#你的头像urlavatar: https://ooo.0o0.ooo/2017/05/29/592be5c575c4c.jpg 链接当然是你刚刚生成的url的链接 其他社交外链同样是修改主题目录下的_config.yml 文件，注意这里的路径是\\blog\\themes\\yilia里，位置与之前的rss的地方差不多，根据你想展示的社交平台设置，注意链接是你的社交平台的主页之类的，比如1234# SubNavsubnav: github: \"https://github.com/LancelotHolmes/\" weibo: \"http://weibo.com/2925991784/profile?topnav=1&amp;wvr=6\" 文章截断直接生成的文章在主页会全部显示，如果不处理会占据较大的篇幅，我们可以在文章的特定位置设置文章截断，这样主页展示的就是部分文字，首先仍然是修改主题目录下的_config.yml 文件，注意这里的路径是\\blog\\themes\\yilia里 1234# Content# 文章太长，截断按钮文字excerpt_link: more 然后在你的文章的md文件里你想要阶段的位置插入语句1&lt;!-- more --&gt; 例如 这样就实现了文章的截断，而需要阅读全文只需要点击相应的按钮或者标题即可 favicon这个主要是为了好玩，就是你的网页打开后的在浏览器的上面的一个小图标，比如github和我的博客的favicon 我是在freefavicon上直接选的一个，如果你有兴趣可以自己制作1616的图片就行了，将图片复制到路径\\blog\\public下，然后在*主题目录下的_config.yml 文件里修改即可 1favicon: /favicon.png 文章目录这个是作者目前没有实现的部分，但是有其他的方法，主要参照的是这个post,修改主要涉及这么两个文件 在/themes/yilia/layout/_partial/article.ejs文件里18行左右的位置插入 1234567891011121314&lt;% if (!index)&#123; %&gt; &lt;% if (toc(post.content))&#123; %&gt; &lt;div id=\"toc\" class=\"article-toc\"&gt; &lt;h2&gt;目录&lt;/h2&gt; &lt;%- toc(post.content) %&gt; &lt;/div&gt; &lt;script type=\"text/javascript\"&gt; var _article = document.getElementsByClassName('article')[0]; &lt;!-- setTimeout(\"_article.style.marginRight = '211px'\",0); --&gt; setTimeout(\"_article.className += ' article2'\",0); setTimeout(\"document.getElementById('toc').style.right = '15px'\", 0); &lt;/script&gt; &lt;% &#125; %&gt; &lt;% &#125; %&gt; 在\\themes\\yilia\\source-src\\css\\article.scss文件的末尾添加 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@media (max-width: 1099px)&#123; #toc&#123; display: none; &#125; &#125; @media (min-width: 1100px) &#123; #toc&#123; z-index: 999; background-color: #fff; padding: 0 1em; border:1px solid #ddd; position: fixed; top: 100px; right: -180px; transition: right .5s ease-in; width: 150px; h2&#123; margin-bottom:10px; &#125; ol&#123; padding-left: 0!important; &#125; line-height: 1.3em; font-size: 0.8em; float: right; .toc&#123; padding: 0; li&#123; list-style-type: none; margin: .5em 0 .5em; ol&#123; margin: .5em 0 .5em 1em; &#125; &#125; &#125; &#125; .article2&#123; margin-right: 211px; transition: margin-right .5s ease-in; &#125;&#125;.toc-item span &#123;display: table-cell;&#125;span.toc-text &#123;padding-left: 3px;&#125; 即可实现，如图如果没有效果，可以尝试在需要目录的文章头部添加如下语句123456---title: 使用机器学习识别出拍卖场中作弊的机器人用户date: 2017-05-29 19:27:51tags: [ML, Kaggle, Python]toc: true--- 以上内容基本可以搭建起一个还不错的博客了，当然如果你需要其他的一些好玩的功能比如标签云啊，浏览量、动态特效之类的就多多使用搜索引擎咯，另外，遇到什么问题大部分都能在对应的github的issues找到答案。 域名绑定最后就介绍下域名的绑定，如果你也不满足使用github的二级域名，而是想使用专属于你自己的域名，那么这部分就是为你而准备的。 这部分也是参照了不少文章，比如Setsuna和水瓶座IOSer，还有zhaozhiming下面我就简单的叙述一下。 使用工具 namesilo: 购买域名的地方，经过一番查阅感觉这个相对靠谱 DNSPod: 国内的免费DNS服务，后面将域名的dns转移到这个上面 首先要绑定域名自然需要购买一个域名，国内的话可以试试阿里云、万网，学生的话好像可以试试腾讯云的云服务器，是会送免费的.cn域名，但是国内购买的话需要去公安局备份好象，感觉比较麻烦我就是用了国外的，比如namesilo，其他的介绍可以看看zhaozhiming的对应观点，当然他和我一样也是在知乎上的一个地方看到的； 在namesilo上注册的教程可以看这个,选择好域名并付款后（支持支付宝）的基本设置可以看这个，接下来： CNAME在你的本地站点目录里的source目录下添加一个CNAME文件，不带后缀，用编辑器打开并输入你购买的域名，不要http也不要www,比如我的域名是izhaoyi.top,我就写入izhaoyi.top然后保存以后执行deploy。 DNS设置namesilo登陆namesilo之后，右上角的Manage My Domains点击进入后,选择然后下拉，选择在上面手动添加两条记录，如图然后回到域名管理界面，选中域名那一栏最前面的勾选框，然后选择上面的Change Namesevers图标最后在在NameServer1和NameServer2中填写 DNSPod 的 nameserver 地址f1g1ns1.dnspod.net，f1g1ns2.dnspod.net DNSPod同样完成注册后，在域名注册中需要手动添加你购买的域名，并添加一些记录，最终如图这里的192.30.252.153是github pages的ip地址，固定设置成这个就好，然后稍微等一段时间（也许不用等）应该就可以通过访问你的域名比如izhaoyi.top或者你之前的github站点名比如LancelotHolmes.github.io访问你的博客啦。 最后，欢迎大家来我的博客踩踩，也欢迎跟我互加友链啊~","tags":[{"name":"Coding","slug":"Coding","permalink":"http://yoursite.com/tags/Coding/"},{"name":"blog","slug":"blog","permalink":"http://yoursite.com/tags/blog/"},{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"yilia","slug":"yilia","permalink":"http://yoursite.com/tags/yilia/"}]},{"title":"使用机器学习识别出拍卖场中作弊的机器人用户(二)","date":"2017-05-29T12:52:19.000Z","path":"2017/05/29/HumenOrRobot2/","text":"原创文章，首发于SegmentFault 本文承接上一篇文章:使用机器学习识别出拍卖场中作弊的机器人用户 本项目为kaggle上Facebook举行的一次比赛，地址见数据来源，完整代码见我的github,欢迎来玩~ 代码 数据探索——Data_Exploration.ipynb 数据预处理&amp;特征工程——Feature_Engineering.ipynb &amp; Feature_Engineering2.ipynb 模型设计及评测——Model_Design.ipynb 项目数据来源 kaggle项目所需额外工具包 numpy pandas matplotlib sklearn xgboost lightgbm mlxtend: 含有聚和算法Stacking项目整体运行时间预估为60min左右，在Ubuntu系统，8G内存，运行结果见所提交的jupyter notebook文件 由于文章内容过长，所以分为两篇文章，总共包含四个部分 数据探索 数据预处理及特征工程 模型设计 评估及总结 特征工程(续)12345import numpy as npimport pandas as pdimport pickle%matplotlib inlinefrom IPython.display import display 12# bids = pd.read_csv('bids.csv')bids = pickle.load(open('bids.pkl')) 12print bids.shapedisplay(bids.head()) (7656329, 9) bid_id bidder_id auction merchandise device time country ip url 0 8dac2b259fd1c6d1120e519fb1ac14fbqvax8 ewmzr jewelry phone0 9759243157894736 us 69.166.231.58 vasstdc27m7nks3 1 668d393e858e8126275433046bbd35c6tywop aeqok furniture phone1 9759243157894736 in 50.201.125.84 jmqlhflrzwuay9c 2 aa5f360084278b35d746fa6af3a7a1a5ra3xe wa00e home goods phone2 9759243157894736 py 112.54.208.157 vasstdc27m7nks3 3 3939ac3ef7d472a59a9c5f893dd3e39fh9ofi jefix jewelry phone4 9759243157894736 in 18.99.175.133 vasstdc27m7nks3 4 8393c48eaf4b8fa96886edc7cf27b372dsibi jefix jewelry phone5 9759243157894736 in 145.138.5.37 vasstdc27m7nks3 1bidders = bids.groupby('bidder_id') 针对国家、商品单一特征多类别转换为多个独立特征进行统计1234567891011121314cates = (bids['merchandise'].unique()).tolist()countries = (bids['country'].unique()).tolist()def dummy_coun_cate(group): coun_cate = dict.fromkeys(cates, 0) coun_cate.update(dict.fromkeys(countries, 0)) for cat, value in group['merchandise'].value_counts().iteritems(): coun_cate[cat] = value for c in group['country'].unique(): coun_cate[c] = 1 coun_cate = pd.Series(coun_cate) return coun_cate 1bidder_coun_cate = bidders.apply(dummy_coun_cate) 12display(bidder_coun_cate.describe())bidder_coun_cate.to_csv('coun_cate.csv') - ad ae af ag al am an ao ar at … count 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 6609.0 … mean 0.002724 0.205629 0.054774 0.001059 0.048570 0.023907 0.000303 0.036314 0.120442 0.052655 … std 0.052121 0.404191 0.227555 0.032530 0.214984 0.152770 0.017395 0.187085 0.325502 0.223362 … min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 … 25% 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 … 50% 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 … 75% 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 … max 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 … 同样的，对于每个用户需要统计他对于自己每次竞拍行为的时间间隔情况 12345678910111213141516171819def bidder_interval(group): time_diff = np.ediff1d(group['time']) bidder_interval = &#123;&#125; if len(time_diff) == 0: diff_mean = 0 diff_std = 0 diff_median = 0 diff_zeros = 0 else: diff_mean = np.mean(time_diff) diff_std = np.std(time_diff) diff_median = np.median(time_diff) diff_zeros = time_diff.shape[0] - np.count_nonzero(time_diff) bidder_interval['tmean'] = diff_mean bidder_interval['tstd'] = diff_std bidder_interval['tmedian'] = diff_median bidder_interval['tzeros'] = diff_zeros bidder_interval = pd.Series(bidder_interval) return bidder_interval 1bidder_inv = bidders.apply(bidder_interval) 12display(bidder_inv.describe())bidder_inv.to_csv('bidder_inv.csv') - tmean tmedian tstd tzeros count 6.609000e+03 6.609000e+03 6.609000e+03 6609.0 mean 2.933038e+12 1.860285e+12 3.440901e+12 122.986231 std 8.552343e+12 7.993497e+12 6.512992e+12 3190.805229 min 0.000000e+00 0.000000e+00 0.000000e+00 0.000000 25% 1.192853e+10 2.578947e+09 1.749995e+09 0.000000 50% 2.641139e+11 5.726316e+10 5.510107e+11 0.000000 75% 1.847456e+12 6.339474e+11 2.911282e+12 0.000000 max 7.610295e+13 7.610295e+13 3.800092e+13 231570.000000 按照用户-拍卖场分组进一步分析之前的统计是按照用户进行分组，针对各个用户从整体上针对竞标行为统计其各项特征，下面根据拍卖场来对用户进一步细分，看一看每个用户在不同拍卖场的行为模式,类似上述按照用户分组来统计各个用户的各项特征，针对用户-拍卖场结对分组进行统计以下特征 基本计数统计，针对各个用户在各个拍卖场统计设备、国家、ip、url、商品类别、竞标次数等特征的数目作为新的特征 时间间隔统计：统计各个用户在各个拍卖场每次竞拍的时间间隔的 均值、方差、中位数和0值 针对商品类别、国家进一步转化为多类别进行统计 123456789101112131415161718192021222324252627282930313233343536def auc_features_count(group): time_diff = np.ediff1d(group['time']) if len(time_diff) == 0: diff_mean = 0 diff_std = 0 diff_median = 0 diff_zeros = 0 else: diff_mean = np.mean(time_diff) diff_std = np.std(time_diff) diff_median = np.median(time_diff) diff_zeros = time_diff.shape[0] - np.count_nonzero(time_diff) row = dict.fromkeys(cates, 0) row.update(dict.fromkeys(countries, 0)) row['devices_c'] = group['device'].unique().shape[0] row['countries_c'] = group['country'].unique().shape[0] row['ip_c'] = group['ip'].unique().shape[0] row['url_c'] = group['url'].unique().shape[0]# row['merch_c'] = group['merchandise'].unique().shape[0] row['bids_c'] = group.shape[0] row['tmean'] = diff_mean row['tstd'] = diff_std row['tmedian'] = diff_median row['tzeros'] = diff_zeros for cat, value in group['merchandise'].value_counts().iteritems(): row[cat] = value for c in group['country'].unique(): row[c] = 1 row = pd.Series(row) return row 1bidder_auc = bids.groupby(['bidder_id', 'auction']).apply(auc_features_count) 1bidder_auc.to_csv('bids_auc.csv') 1print bidder_auc.shape (382336, 218) 模型设计与参数评估合并特征对之前生成的各项特征进行合并产生最终的特征空间 1234import numpy as npimport pandas as pd%matplotlib inlinefrom IPython.display import display 首先将之前根据用户分组的统计特征合并起来，然后将其与按照用户-拍卖场结对分组的特征合并起来，最后加上时间特征，分别于训练集、测试集连接生成后续进行训练和预测的特征数据文件 123456789101112131415161718192021222324def merge_data(): train = pd.read_csv('train.csv') test = pd.read_csv('test.csv') time_differences = pd.read_csv('tdiff.csv', index_col=0) bids_auc = pd.read_csv('bids_auc.csv') bids_auc = bids_auc.groupby('bidder_id').mean() bidders = pd.read_csv('cnt_bidder.csv', index_col=0) country_cate = pd.read_csv('coun_cate.csv', index_col=0) bidder_inv = pd.read_csv('bidder_inv.csv', index_col=0) bidders = bidders.merge(country_cate, right_index=True, left_index=True) bidders = bidders.merge(bidder_inv, right_index=True, left_index=True) bidders = bidders.merge(bids_auc, right_index=True, left_index=True) bidders = bidders.merge(time_differences, right_index=True, left_index=True) train = train.merge(bidders, left_on='bidder_id', right_index=True) train.to_csv('train_full.csv', index=False) test = test.merge(bidders, left_on='bidder_id', right_index=True) test.to_csv('test_full.csv', index=False) 1merge_data() 1234train_full = pd.read_csv('train_full.csv')test_full = pd.read_csv('test_full.csv')print train_full.shapeprint test_full.shape (1983, 445) (4626, 444) 123456789train_full['outcome'] = train_full['outcome'].astype(int)ytrain = train_full['outcome']train_full.drop('outcome', 1, inplace=True)test_ids = test_full['bidder_id']labels = ['payment_account', 'address', 'bidder_id']train_full.drop(labels=labels, axis=1, inplace=True)test_full.drop(labels=labels, axis=1, inplace=True) 设计交叉验证模型选择根据之前的分析，由于当前的数据集中存在正负例不均衡的问题，所以考虑选取了RandomForestClassfier, GradientBoostingClassifier, xgboost, lightgbm等四种模型来针对数据及进行训练和预测，确定最终模型的基本思路如下： 对四个模型分别使用评价函数roc_auc进行交叉验证并绘制auc曲线，对各个模型的多轮交叉验证得分取平均值并输出 根据得分确定最终选用的一个或多个模型 若最后发现一个模型的表现大幅度优于其他所有模型，则选择该模型进一步调参 若最后发现多个模型表现都不错，则进行模型的集成，得到聚合模型 使用GridSearchCV来从人为设定的参数列表中选择最佳的参数组合确定最终的模型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from scipy import interpimport matplotlib.pyplot as pltfrom itertools import cycle# from sklearn.cross_validation import StratifiedKFoldfrom sklearn.model_selection import StratifiedKFoldfrom sklearn.metrics import roc_auc_score, roc_curve, aucdef kfold_plot(train, ytrain, model):# kf = StratifiedKFold(y=ytrain, n_folds=5) kf = StratifiedKFold(n_splits=5) scores = [] mean_tpr = 0.0 mean_fpr = np.linspace(0, 1, 100) exe_time = [] colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue']) lw = 2 i=0 for (train_index, test_index), color in zip(kf.split(train, ytrain), colors): X_train, X_test = train.iloc[train_index], train.iloc[test_index] y_train, y_test = ytrain.iloc[train_index], ytrain.iloc[test_index] begin_t = time.time() predictions = model(X_train, X_test, y_train) end_t = time.time() exe_time.append(round(end_t-begin_t, 3))# model = model# model.fit(X_train, y_train) # predictions = model.predict_proba(X_test)[:, 1] scores.append(roc_auc_score(y_test.astype(float), predictions)) fpr, tpr, thresholds = roc_curve(y_test, predictions) mean_tpr += interp(mean_fpr, fpr, tpr) mean_tpr[0] = 0.0 roc_auc = auc(fpr, tpr) plt.plot(fpr, tpr, lw=lw, color=color, label='ROC fold %d (area = %0.2f)' % (i, roc_auc)) i += 1 plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k', label='Luck') mean_tpr /= kf.get_n_splits(train, ytrain) mean_tpr[-1] = 1.0 mean_auc = auc(mean_fpr, mean_tpr) plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--', label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw) plt.xlim([-0.05, 1.05]) plt.ylim([-0.05, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Receiver operating characteristic') plt.legend(loc='lower right') plt.show() # print 'scores: ', scores print 'mean scores: ', np.mean(scores) print 'mean model process time: ', np.mean(exe_time), 's' return scores, np.mean(scores), np.mean(exe_time) 收集各个模型进行交叉验证的结果包括每轮交叉验证的auc得分、auc的平均得分以及模型的训练时间 123dct_scores = &#123;&#125;mean_score = &#123;&#125;mean_time = &#123;&#125; RandomForestClassifier12from sklearn.model_selection import GridSearchCVimport time 12345678910from sklearn.ensemble import RandomForestClassifierdef forest_model(X_train, X_test, y_train):# begin_t = time.time() model = RandomForestClassifier(n_estimators=160, max_features=35, max_depth=8, random_state=7) model.fit(X_train, y_train) # end_t = time.time()# print 'train time of forest model: ',round(end_t-begin_t, 3), 's' predictions = model.predict_proba(X_test)[:, 1] return predictions 12dct_scores['forest'], mean_score['forest'], mean_time['forest'] = kfold_plot(train_full, ytrain, forest_model)# kfold_plot(train_full, ytrain, model_forest) mean scores: 0.909571935157 mean model process time: 0.643 s 123456from sklearn.ensemble import GradientBoostingClassifierdef gradient_model(X_train, X_test, y_train): model = GradientBoostingClassifier(n_estimators=200, random_state=7, max_depth=5, learning_rate=0.03) model.fit(X_train, y_train) predictions = model.predict_proba(X_test)[:, 1] return predictions 1dct_scores['gbm'], mean_score['gbm'], mean_time['gbm'] = kfold_plot(train_full, ytrain, gradient_model) mean scores: 0.911847771023 mean model process time: 4.1948 s 123456789import xgboost as xgbdef xgboost_model(X_train, X_test, y_train): X_train = xgb.DMatrix(X_train.values, label=y_train.values) X_test = xgb.DMatrix(X_test.values) params = &#123;'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': 1, 'seed': 7, 'max_depth': 6, 'eta': 0.01&#125; model = xgb.train(params, X_train, 600) predictions = model.predict(X_test) return predictions /home/lancelot/anaconda2/envs/udacity/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20. &quot;This module will be removed in 0.20.&quot;, DeprecationWarning) 1dct_scores['xgboost'], mean_score['xgboost'], mean_time['xgboost'] = kfold_plot(train_full, ytrain, xgboost_model) mean scores: 0.915372340426 mean model process time: 3.1482 s 1234567import lightgbm as lgbdef lightgbm_model(X_train, X_test, y_train): X_train = lgb.Dataset(X_train.values, y_train.values) params = &#123;'objective': 'binary', 'metric': &#123;'auc'&#125;, 'learning_rate': 0.01, 'max_depth': 6, 'seed': 7&#125; model = lgb.train(params, X_train, num_boost_round=600) predictions = model.predict(X_test) return predictions 1dct_scores['lgbm'], mean_score['lgbm'], mean_time['lgbm'] = kfold_plot(train_full, ytrain, lightgbm_model) mean scores: 0.921512158055 mean model process time: 0.3558 s 模型比较比较四个模型在交叉验证机上的roc_auc平均得分和模型训练的时间 123456789101112131415def plot_model_comp(title, y_label, dct_result): data_source = dct_result.keys() y_pos = np.arange(len(data_source)) # model_auc = [0.910, 0.912, 0.915, 0.922] model_auc = dct_result.values() barlist = plt.bar(y_pos, model_auc, align='center', alpha=0.5) # get the index of highest score max_val = max(model_auc) idx = model_auc.index(max_val) barlist[idx].set_color('r') plt.xticks(y_pos, data_source) plt.ylabel(y_label) plt.title(title) plt.show() print 'The highest auc score is &#123;0&#125; of model: &#123;1&#125;'.format(max_val, data_source[idx]) 1plot_model_comp('Model Performance', 'roc-auc score', mean_score) The highest auc score is 0.921512158055 of model: lgbm 123456789101112131415def plot_time_comp(title, y_label, dct_result): data_source = dct_result.keys() y_pos = np.arange(len(data_source)) # model_auc = [0.910, 0.912, 0.915, 0.922] model_auc = dct_result.values() barlist = plt.bar(y_pos, model_auc, align='center', alpha=0.5) # get the index of highest score min_val = min(model_auc) idx = model_auc.index(min_val) barlist[idx].set_color('r') plt.xticks(y_pos, data_source) plt.ylabel(y_label) plt.title(title) plt.show() print 'The shortest time is &#123;0&#125; of model: &#123;1&#125;'.format(min_val, data_source[idx]) 1plot_time_comp('Time of Building Model', 'time(s)', mean_time) The shortest time is 0.3558 of model: lgbm 12345678910111213141516171819auc_forest = dct_scores['forest']auc_gb = dct_scores['gbm']auc_xgb = dct_scores['xgboost']auc_lgb = dct_scores['lgbm']print 'std of forest auc score: ',np.std(auc_forest)print 'std of gbm auc score: ',np.std(auc_gb)print 'std of xgboost auc score: ',np.std(auc_xgb)print 'std of lightgbm auc score: ',np.std(auc_lgb)data_source = ['roc-fold-1', 'roc-fold-2', 'roc-fold-3', 'roc-fold-4', 'roc-fold-5']y_pos = np.arange(len(data_source))plt.plot(y_pos, auc_forest, 'b-', label='forest')plt.plot(y_pos, auc_gb, 'r-', label='gbm')plt.plot(y_pos, auc_xgb, 'y-', label='xgboost')plt.plot(y_pos, auc_lgb, 'g-', label='lightgbm')plt.title('roc-auc score of each epoch')plt.xlabel('epoch')plt.ylabel('roc-auc score')plt.legend()plt.show() std of forest auc score: 0.0413757504568 std of gbm auc score: 0.027746291638 std of xgboost auc score: 0.0232931322563 std of lightgbm auc score: 0.0287156755513 单从5次交叉验证的各模型roc-auc得分来看，xgboost的得分相对比较稳定 聚合模型由上面的模型比较可以发现，四个模型的经过交叉验证的表现都不错，但是综合而言，xgboost和lightgbm更胜一筹，而且两者的训练时间也相对更短一些，所以接下来考虑进行模型的聚合，思路如下： 先通过GridSearchCV分别针对四个模型在整个训练集上进行调参获得最佳的子模型 针对子模型使用 stacking: 第三方库mlxtend里的stacking方法对子模型进行聚合得到聚合模型，并采用之前相同的cv方法对该模型进行打分评价 voting: 使用sklearn内置的VotingClassifier进行四个模型的聚合 最终对聚合模型在一次进行cv验证评分，根据结果确定最终的模型 先通过交叉验证针对模型选择参数组合 12345678910def choose_xgb_model(X_train, y_train): tuned_params = [&#123;'objective': ['binary:logistic'], 'learning_rate': [0.01, 0.03, 0.05], 'n_estimators': [100, 150, 200], 'max_depth':[4, 6, 8]&#125;] begin_t = time.time() clf = GridSearchCV(xgb.XGBClassifier(seed=7), tuned_params, scoring='roc_auc') clf.fit(X_train, y_train) end_t = time.time() print 'train time: ',round(end_t-begin_t, 3), 's' print 'current best parameters of xgboost: ',clf.best_params_ return clf.best_estimator_ 1bst_xgb = choose_xgb_model(train_full, ytrain) train time: 48.141 s current best parameters of xgboost: {&apos;n_estimators&apos;: 150, &apos;objective&apos;: &apos;binary:logistic&apos;, &apos;learning_rate&apos;: 0.05, &apos;max_depth&apos;: 4} 12345678910def choose_lgb_model(X_train, y_train): tuned_params = [&#123;'objective': ['binary'], 'learning_rate': [0.01, 0.03, 0.05], 'n_estimators': [100, 150, 200], 'max_depth':[4, 6, 8]&#125;] begin_t = time.time() clf = GridSearchCV(lgb.LGBMClassifier(seed=7), tuned_params, scoring='roc_auc') clf.fit(X_train, y_train) end_t = time.time() print 'train time: ',round(end_t-begin_t, 3), 's' print 'current best parameters of lgb: ',clf.best_params_ return clf.best_estimator_ 1bst_lgb = choose_lgb_model(train_full, ytrain) train time: 12.543 s current best parameters of lgb: {&apos;n_estimators&apos;: 150, &apos;objective&apos;: &apos;binary&apos;, &apos;learning_rate&apos;: 0.05, &apos;max_depth&apos;: 4} 先使用stacking集成两个综合表现最佳的模型lgb和xgb，此处元分类器使用较为简单的LR模型来在已经训练好了并且经过参数选择的模型上进一步优化预测结果 12345678910from mlxtend.classifier import StackingClassifierfrom sklearn import linear_modeldef stacking_model(X_train, X_test, y_train): lr = linear_model.LogisticRegression(random_state=7) sclf = StackingClassifier(classifiers=[bst_xgb, bst_lgb], use_probas=True, average_probas=False, meta_classifier=lr) sclf.fit(X_train, y_train) predictions = sclf.predict_proba(X_test)[:, 1] return predictions 1dct_scores['stacking_1'], mean_score['stacking_1'], mean_time['stacking_1'] = kfold_plot(train_full, ytrain, stacking_model) mean scores: 0.92157674772 mean model process time: 0.7022 s 可以看到相对之前的得分最高的模型lightgbm，将lightgbm与xgboost经过stacking集成并且使用lr作为元分类器得到的auc得分有轻微的提升，接下来考虑进一步加入另外的RandomForest和GBDT模型看看增加一点模型的差异性使用Stacking是不是会有所提升 123456789def choose_forest_model(X_train, y_train): tuned_params = [&#123;'n_estimators': [100, 150, 200], 'max_features': [8, 15, 30], 'max_depth':[4, 8, 10]&#125;] begin_t = time.time() clf = GridSearchCV(RandomForestClassifier(random_state=7), tuned_params, scoring='roc_auc') clf.fit(X_train, y_train) end_t = time.time() print 'train time: ',round(end_t-begin_t, 3), 's' print 'current best parameters: ',clf.best_params_ return clf.best_estimator_ 1bst_forest = choose_forest_model(train_full, ytrain) train time: 42.201 s current best parameters: {&apos;max_features&apos;: 15, &apos;n_estimators&apos;: 150, &apos;max_depth&apos;: 8} 12345678910def choose_gradient_model(X_train, y_train): tuned_params = [&#123;'n_estimators': [100, 150, 200], 'learning_rate': [0.03, 0.05, 0.07], 'min_samples_leaf': [8, 15, 30], 'max_depth':[4, 6, 8]&#125;] begin_t = time.time() clf = GridSearchCV(GradientBoostingClassifier(random_state=7), tuned_params, scoring='roc_auc') clf.fit(X_train, y_train) end_t = time.time() print 'train time: ',round(end_t-begin_t, 3), 's' print 'current best parameters: ',clf.best_params_ return clf.best_estimator_ 1bst_gradient = choose_gradient_model(train_full, ytrain) train time: 641.872 s current best parameters: {&apos;n_estimators&apos;: 100, &apos;learning_rate&apos;: 0.03, &apos;max_depth&apos;: 8, &apos;min_samples_leaf&apos;: 30} 1234567def stacking_model2(X_train, X_test, y_train): lr = linear_model.LogisticRegression(random_state=7) sclf = StackingClassifier(classifiers=[bst_xgb, bst_forest, bst_gradient, bst_lgb], use_probas=True, average_probas=False, meta_classifier=lr) sclf.fit(X_train, y_train) predictions = sclf.predict_proba(X_test)[:, 1] return predictions 1dct_scores['stacking_2'], mean_score['stacking_2'], mean_time['stacking_2'] = kfold_plot(train_full, ytrain, stacking_model2) mean scores: 0.92686550152 mean model process time: 4.0878 s 可以看到四个模型的聚合效果比用两个模型的stacking聚合效果要好不少，接下来尝试使用voting对四个模型进行聚合 12345678from sklearn.ensemble import VotingClassifierdef voting_model(X_train, X_test, y_train): vclf = VotingClassifier(estimators=[('xgb', bst_xgb), ('rf', bst_forest), ('gbm',bst_gradient), ('lgb', bst_lgb)], voting='soft', weights=[2, 1, 1, 2]) vclf.fit(X_train, y_train) predictions = vclf.predict_proba(X_test)[:, 1] return predictions 1dct_scores['voting'], mean_score['voting'], mean_time['voting'] = kfold_plot(train_full, ytrain, voting_model) mean scores: 0.926889564336 mean model process time: 4.055 s 再次比较单模型与集成模型的得分 1plot_model_comp('Model Performance', 'roc-auc score', mean_score) The highest auc score is 0.926889564336 of model: voting 由上可以看到最终通过voting将四个模型进行聚合可以得到得分最高的模型，确定为最终用来预测的模型 综合模型，对测试文件进行最终预测12345678910111213141516# predict(train_full, test_full, y_train)def submit(X_train, X_test, y_train, test_ids): predictions = voting_model(X_train, X_test, y_train) sub = pd.read_csv('sampleSubmission.csv') result = pd.DataFrame() result['bidder_id'] = test_ids result['outcome'] = predictions sub = sub.merge(result, on='bidder_id', how='left') # Fill missing values with mean mean_pred = np.mean(predictions) sub.fillna(mean_pred, inplace=True) sub.drop('prediction', 1, inplace=True) sub.to_csv('result.csv', index=False, header=['bidder_id', 'prediction']) 1submit(train_full, test_full, ytrain, test_ids) 最终结果提交到kaggle上进行评分，得分如下 以上就是整个完整的流程，当然还有很多模型可以尝试，很多聚合方法也可以使用，此外，特征工程部分还有很多空间可以挖掘，就留给大家去探索啦~ 参考资料 Chen, K. T., Pao, H. K. K., &amp; Chang, H. C. (2008, October). Game bot identification based on manifold learning. In Proceedings of the 7th ACM SIGCOMM Workshop on Network and System Support for Games (pp. 21-26). ACM. Alayed, H., Frangoudes, F., &amp; Neuman, C. (2013, August). Behavioral-based cheating detection in online first person shooters using machine learning techniques. In Computational Intelligence in Games (CIG), 2013 IEEE Conference on (pp. 1-8). IEEE. https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot/data http://stats.stackexchange.com/a/132832/152084 https://en.wikipedia.org/wiki/Receiver_operating_characteristic https://en.wikipedia.org/wiki/Random_forest https://en.wikipedia.org/wiki/Gradient_boosting https://xgboost.readthedocs.io/en/latest//parameter.html#parameters-for-tree-booster https://github.com/Microsoft/LightGBM https://en.wikipedia.org/wiki/Receiver_operating_characteristic http://stackoverflow.com/questions/29530232/python-pandas-check-if-any-value-is-nan-in-dataframe http://pandas.pydata.org/pandas-docs/stable/missing_data.html http://stackoverflow.com/a/18272653/6653189 http://www.cnblogs.com/jasonfreak/p/5720137.html kaggle ensembling guide","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"使用机器学习识别出拍卖场中作弊的机器人用户","date":"2017-05-29T11:27:51.000Z","path":"2017/05/29/HumenOrRobot/","text":"原创文章，首发于SegmentFault 本项目为kaggle上Facebook举行的一次比赛，地址见数据来源，完整代码见我的github,欢迎来玩~ 代码 数据探索——Data_Exploration.ipynb 数据预处理&amp;特征工程——Feature_Engineering.ipynb &amp; Feature_Engineering2.ipynb 模型设计及评测——Model_Design.ipynb 项目数据来源 kaggle项目所需额外工具包 numpy pandas matplotlib sklearn xgboost lightgbm mlxtend: 含有聚和算法Stacking项目整体运行时间预估为60min左右，在Ubuntu系统，8G内存，运行结果见所提交的jupyter notebook文件 由于文章内容过长，所以分为两篇文章，总共包含四个部分 数据探索 数据预处理及特征工程 模型设计 评估及总结 数据探索1234import numpy as npimport pandas as pd%matplotlib inlinefrom IPython.display import display 123df_bids = pd.read_csv('bids.csv', low_memory=False)df_train = pd.read_csv('train.csv')df_test = pd.read_csv('test.csv') 1df_bids.head(3) bid_id bidder_id auction merchandise device time country ip url 0 8dac2b259fd1c6d1120e519fb1ac14fbqvax8 ewmzr jewelry phone0 9759243157894736 us 69.166.231.58 vasstdc27m7nks3 1 668d393e858e8126275433046bbd35c6tywop aeqok furniture phone1 9759243157894736 in 50.201.125.84 jmqlhflrzwuay9c 2 aa5f360084278b35d746fa6af3a7a1a5ra3xe wa00e home goods phone2 9759243157894736 py 112.54.208.157 vasstdc27m7nks3 12df_train.head(3)# df_train.dtypes bidder_id payment_account address outcome 91a3c57b13234af24875c56fb7e2b2f4rb56a a3d2de7675556553a5f08e4c88d2c228754av a3d2de7675556553a5f08e4c88d2c228vt0u4 0.0 624f258b49e77713fc34034560f93fb3hu3jo a3d2de7675556553a5f08e4c88d2c228v1sga ae87054e5a97a8f840a3991d12611fdcrfbq3 0.0 1c5f4fc669099bfbfac515cd26997bd12ruaj a3d2de7675556553a5f08e4c88d2c2280cybl 92520288b50f03907041887884ba49c0cl0pd 0.0 异常数据检测1234# 查看各表格中是否存在空值print 'Is there any missing value in bids?',df_bids.isnull().any().any()print 'Is there any missing value in train?',df_train.isnull().any().any()print 'Is there any missing value in test?',df_test.isnull().any().any() Is there any missing value in bids? True Is there any missing value in train? False Is there any missing value in test? False 整个对三个数据集进行空值判断，发现用户数据训练集和测试集均无缺失数据，而在竞标行为数据集中存在缺失值的情况，下面便针对bids数据进一步寻找缺失值 123# nan_rows = df_bids[df_bids.isnull().T.any().T]# print nan_rowspd.isnull(df_bids).any() bid_id False bidder_id False auction False merchandise False device False time False country True ip False url False dtype: bool 1234missing_country = df_bids['country'].isnull().sum().sum()print 'No. of missing country: ', missing_countrynormal_country = df_bids['country'].notnull().sum().sum()print 'No. of normal country: ', normal_country No. of missing country: 8859 No. of normal country: 7647475 123456789import matplotlib.pyplot as pltlabels = ['unknown', 'normal']sizes = [missing_country, normal_country]explode = (0.1, 0)fig1, ax1 = plt.subplots()ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)ax1.axis('equal')plt.title('Distribution of missing countries vs. normal countries')plt.show() 综合上述的分析可以发现，在竞标行为用户的country一栏属性中存在很少一部分用户行为是没有country记录的，在预处理部分可以针对这部分缺失数据进行填充操作，有两种思路： 针对原始行为数据按照用户分组后，看看每个对应的用户竞标时经常所位于的国家信息，对缺失值填充常驻国家 针对原始行为数据按照用户分组后，按时间顺序对每组用户中的缺失值前向或后向填充相邻的国家信息 12345678# 查看各个数据的记录数# 看看数据的id是否是唯一标识print df_bids.shape[0]print len(df_bids['bid_id'].unique())print df_train.shape[0]print len(df_train['bidder_id'].unique())print df_test.shape[0]print len(df_test['bidder_id'].unique()) 7656334 7656334 2013 2013 4700 4700 12345678# 简单统计各项基本特征（类别特征）的数目（除去时间）print 'total bidder in bids: ', len(df_bids['bidder_id'].unique())print 'total auction in bids: ', len(df_bids['auction'].unique())print 'total merchandise in bids: ', len(df_bids['merchandise'].unique())print 'total device in bids: ', len(df_bids['device'].unique())print 'total country in bids: ', len(df_bids['country'].unique())print 'total ip in bids: ', len(df_bids['ip'].unique())print 'total url in bids: ', len(df_bids['url'].unique()) total bidder in bids: 6614 total auction in bids: 15051 total merchandise in bids: 10 total device in bids: 7351 total country in bids: 200 total ip in bids: 2303991 total url in bids: 1786351 由上述基本特征可以看到： 竞标行为中的用户总数少于训练集+测试集的用户数，也就是说并不是一一对应的，接下来验证下竞标行为数据中的用户是否完全来自训练集和测试集 商品类别和国家的种类相对其他特征较少，可以作为天然的类别特征提取出来进行处理，而其余的特征可能更多的进行计数统计 12345lst_all_users = (df_train['bidder_id'].unique()).tolist() + (df_test['bidder_id'].unique()).tolist()print 'total bidders of train and test set',len(lst_all_users)lst_bidder = (df_bids['bidder_id'].unique()).tolist()print 'total bidders in bids set',len(lst_bidder)print 'Is bidders in bids are all from train+test set? ',set(lst_bidder).issubset(set(lst_all_users)) total bidders of train and test set 6713 total bidders in bids set 6614 Is bidders in bids are all from train+test set? True 123456lst_nobids = [i for i in lst_all_users if i not in lst_bidder]print 'No. of bidders never bid: ',len(lst_nobids)lst_nobids_train = [i for i in lst_nobids if i in (df_train['bidder_id'].unique()).tolist()]lst_nobids_test = [i for i in lst_nobids if i in (df_test['bidder_id'].unique()).tolist()]print 'No. of bidders never bid in train set: ',len(lst_nobids_train)print 'No. of bidders never bid in test set: ',len(lst_nobids_test) No. of bidders never bid: 99 No. of bidders never bid in train set: 29 No. of bidders never bid in test set: 70 12345678data_source = ['train', 'test']y_pos = np.arange(len(data_source))num_never_bids = [len(lst_nobids_train), len(lst_nobids_test)]plt.bar(y_pos, num_never_bids, align='center', alpha=0.5)plt.xticks(y_pos, data_source)plt.ylabel('bidders no bids')plt.title('Source of no bids bidders')plt.show() 1print df_train[(df_train['bidder_id'].isin(lst_nobids_train)) &amp; (df_train['outcome']==1.0)] Empty DataFrame Columns: [bidder_id, payment_account, address, outcome] Index: [] 由上述计算可知存在99个竞标者无竞标记录，其中29位来自训练集，70位来自测试集，而且这29位来自训练集的竞标者未被标记为机器人用户，所以可以针对测试集中的这70位用户后续标记为人类或者取平均值处理 12# check the partition of bots in trainprint (df_train[df_train['outcome'] == 1].shape[0]*1.0) / df_train.shape[0] * 100,'%' 5.11674118231 % 训练集中的标记为机器人的用户占所有用户数目约5% 12df_train.groupby('outcome').size().plot(labels=['Human', 'Robot'], kind='pie', autopct='%.2f', figsize=(4, 4), title='Distribution of Human vs. Robots', legend=True) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f477135c5d0&gt; 由上述训练集中的正负例分布可以看到本数据集正负例比例失衡，所以后续考虑使用AUC（不受正负例比例影响）作为评价指标，此外尽量采用Gradient Boosting族模型来进行训练 数据预处理与特征工程12345import numpy as npimport pandas as pdimport pickle%matplotlib inlinefrom IPython.display import display 123bids = pd.read_csv('bids.csv')train = pd.read_csv('train.csv')test = pd.read_csv('test.csv') 处理缺失数据针对前面数据探索部分所发现的竞标行为数据中存在的国家属性缺失问题，考虑使用针对原始行为数据按照用户分组后，按时间顺序对每组用户中的缺失值前向或后向填充相邻的国家信息的方法来进行缺失值的填充处理 1display(bids.head(3)) bid_id bidder_id auction merchandise device time country ip url 0 8dac2b259fd1c6d1120e519fb1ac14fbqvax8 ewmzr jewelry phone0 9759243157894736 us 69.166.231.58 vasstdc27m7nks3 1 668d393e858e8126275433046bbd35c6tywop aeqok furniture phone1 9759243157894736 in 50.201.125.84 jmqlhflrzwuay9c 2 aa5f360084278b35d746fa6af3a7a1a5ra3xe wa00e home goods phone2 9759243157894736 py 112.54.208.157 vasstdc27m7nks3 12# pd.algos.is_monotonic_int64(bids.time.values, True)[0]print 'Is the time monotonically non-decreasing? ', pd.Index(bids['time']).is_monotonic Is the time monotonically non-decreasing? False 123# bidder_group = bids.sort_values(['bidder_id', 'time']).groupby('bidder_id')bids['country'] = bids.sort_values(['bidder_id', 'time']).groupby('bidder_id')['country'].ffill()bids['country'] = bids.sort_values(['bidder_id', 'time']).groupby('bidder_id')['country'].bfill() 1display(bids.head(3)) bid_id bidder_id auction merchandise device time country ip url 0 8dac2b259fd1c6d1120e519fb1ac14fbqvax8 ewmzr jewelry phone0 9759243157894736 us 69.166.231.58 vasstdc27m7nks3 1 668d393e858e8126275433046bbd35c6tywop aeqok furniture phone1 9759243157894736 in 50.201.125.84 jmqlhflrzwuay9c 2 aa5f360084278b35d746fa6af3a7a1a5ra3xe wa00e home goods phone2 9759243157894736 py 112.54.208.157 vasstdc27m7nks3 12print 'Is there any missing value in bids?',bids.isnull().any().any()# pickle.dump(bids, open('bids.pkl', 'w')) Is there any missing value in bids? True 1234missing_country = bids['country'].isnull().sum().sum()print 'No. of missing country: ', missing_countrynormal_country = bids['country'].notnull().sum().sum()print 'No. of normal country: ', normal_country No. of missing country: 5 No. of normal country: 7656329 12nan_rows = bids[bids.isnull().T.any().T]print nan_rows bid_id bidder_id auction \\ 1351177 1351177 f3ab8c9ecc0d021ebc81e89f20c8267bn812w jefix 2754184 2754184 88ef9cfdbec4c9e33f6c2e0b512e7a01dp2p2 cc5fs 2836631 2836631 29b8af2fea3881ef61911612372dac41vczqv jqx39 3125892 3125892 df20f216cbb0b0df5a7b2e94b16a7853iyw9g jqx39 5153748 5153748 5e05ec450e2dd64d7996a08bbbca4f126nzzk jqx39 merchandise device time country \\ 1351177 office equipment phone84 9767200789473684 NaN 2754184 mobile phone150 9633363947368421 NaN 2836631 jewelry phone72 9634034894736842 NaN 3125892 books and music phone106 9635755105263157 NaN 5153748 mobile phone267 9645270210526315 NaN ip url 1351177 80.211.119.111 g9pgdfci3yseml5 2754184 20.67.240.88 ctivbfq55rktail 2836631 149.210.107.205 vasstdc27m7nks3 3125892 26.23.62.59 ac9xlqtfg0cx5c5 5153748 145.7.194.40 0em0vg1f0zuxonw 1234# print bids[bids['bid_id']==1351177]nan_bidder = nan_rows['bidder_id'].values.tolist()# print nan_bidderprint bids[bids['bidder_id'].isin(nan_bidder)] bid_id bidder_id auction \\ 1351177 1351177 f3ab8c9ecc0d021ebc81e89f20c8267bn812w jefix 2754184 2754184 88ef9cfdbec4c9e33f6c2e0b512e7a01dp2p2 cc5fs 2836631 2836631 29b8af2fea3881ef61911612372dac41vczqv jqx39 3125892 3125892 df20f216cbb0b0df5a7b2e94b16a7853iyw9g jqx39 5153748 5153748 5e05ec450e2dd64d7996a08bbbca4f126nzzk jqx39 merchandise device time country \\ 1351177 office equipment phone84 9767200789473684 NaN 2754184 mobile phone150 9633363947368421 NaN 2836631 jewelry phone72 9634034894736842 NaN 3125892 books and music phone106 9635755105263157 NaN 5153748 mobile phone267 9645270210526315 NaN ip url 1351177 80.211.119.111 g9pgdfci3yseml5 2754184 20.67.240.88 ctivbfq55rktail 2836631 149.210.107.205 vasstdc27m7nks3 3125892 26.23.62.59 ac9xlqtfg0cx5c5 5153748 145.7.194.40 0em0vg1f0zuxonw 在对整体数据的部分用户缺失国家的按照各个用户分组后在时间上前向和后向填充后，仍然存在5个用户缺失了国家信息，结果发现这5个用户是仅有一次竞标行为，下面看看这5个用户还有什么特征 1234lst_nan_train = [i for i in nan_bidder if i in (train['bidder_id'].unique()).tolist()]lst_nan_test = [i for i in nan_bidder if i in (test['bidder_id'].unique()).tolist()]print 'No. of bidders 1 bid in train set: ',len(lst_nan_train)print 'No. of bidders 1 bid in test set: ',len(lst_nan_test) No. of bidders 1 bid in train set: 1 No. of bidders 1 bid in test set: 4 1print train[train['bidder_id']==lst_nan_train[0]]['outcome'] 546 0.0 Name: outcome, dtype: float64 由于这5个用户仅有一次竞标行为，而且其中1个用户来自训练集，4个来自测试集，由训练集用户的标记为人类，加上行为数太少，所以考虑对这5个用户的竞标行为数据予以舍弃，特别对测试集的4个用户后续操作类似之前对无竞标行为的用户，预测值填充最终模型的平均预测值 123bid_to_drop = nan_rows.index.values.tolist()# print bid_to_dropbids.drop(bids.index[bid_to_drop], inplace=True) 12print 'Is there any missing value in bids?',bids.isnull().any().any()pickle.dump(bids, open('bids.pkl', 'w')) Is there any missing value in bids? False 统计基本的计数特征根据前面的数据探索，由于数据集大部分由类别数据或者离散型数据构成，所以首先针对竞标行为数据按照竞标者分组统计其各项属性的数目，比如使用设备种类，参与竞标涉及国家，ip种类等等 123# group by bidder to do some statisticsbidders = bids.groupby('bidder_id')# pickle.dump(bids, open('bidders.pkl', 'w')) 1234567891011121314# print bidders['device'].count()def feature_count(group): dct_cnt = &#123;&#125; dct_cnt['devices_c'] = group['device'].unique().shape[0] dct_cnt['countries_c'] = group['country'].unique().shape[0] dct_cnt['ip_c'] = group['ip'].unique().shape[0] dct_cnt['url_c'] = group['url'].unique().shape[0] dct_cnt['auction_c'] = group['auction'].unique().shape[0] dct_cnt['auc_mean'] = np.mean(group['auction'].value_counts()) # bids_c/auction_c# dct_cnt['dev_mean'] = np.mean(group['device'].value_counts()) # bids_c/devices_c dct_cnt['merch_c'] = group['merchandise'].unique().shape[0] dct_cnt['bids_c'] = group.shape[0] dct_cnt = pd.Series(dct_cnt) return dct_cnt 1cnt_bidder = bidders.apply(feature_count) 123display(cnt_bidder.describe())# cnt_bidder.to_csv('cnt_bidder.csv')# print cnt_bidder[cnt_bidder['merch_c']==2] - auc_mean auction_c bids_c countries_c devices_c ip_c merch_c url_c count 6609.000000 6609.000000 6609.000000 6609.000000 6609.000000 6609.000000 6609.000000 6609.000000 mean 6.593493 57.850810 1158.470117 12.733848 73.492359 544.507187 1.000151 290.964140 std 30.009242 131.814053 9596.595169 22.556570 172.171106 3370.730666 0.012301 2225.912425 min 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 25% 1.000000 2.000000 3.000000 1.000000 2.000000 2.000000 1.000000 1.000000 50% 1.677419 10.000000 18.000000 3.000000 8.000000 12.000000 1.000000 5.000000 75% 4.142857 47.000000 187.000000 12.000000 57.000000 111.000000 1.000000 36.000000 max 1327.366667 1726.000000 515033.000000 178.000000 2618.000000 111918.000000 2.000000 81376.000000 特征相关性在对竞标行为数据按照用户分组后，对数据集中的每一个产品特征构建一个散布矩阵（scatter matrix），来看看各特征之间的相关性 12# 对于数据中的每一对特征构造一个散布矩阵pd.scatter_matrix(cnt_bidder, alpha = 0.3, figsize = (16,10), diagonal = 'kde'); 在针对竞标行为数据按照竞标用户进行分组基本统计后由上表可以看出，此时并未考虑时间戳的情形下，有以下基本结论： 由各项统计的最大值与中位值，75%值的比较可以看到除了商品类别一项，其他的几项多少都存在一些异常数值，或许可以作为异常行为进行观察 各特征的倾斜度很大，考虑对特征进行取对数的操作，并再次输出散布矩阵看看相关性。 商品类别计数这一特征的方差很小，而且从中位数乃至75%的统计来看，大多数用户仅对同一类别商品进行拍卖，而且因为前面数据探索部分发现商品类别本身适合作为类别数据，所以考虑分多个类别进行单独统计，而在计数特征中舍弃该特征。 1cnt_bidder.drop('merch_c', axis=1, inplace=True) 1cnt_bidder = np.log(cnt_bidder) 1pd.scatter_matrix(cnt_bidder, alpha = 0.3, figsize = (16,10), diagonal = 'kde'); 由上面的散布矩阵可以看到，个行为特征之间并没有表现出很强的相关性，虽然其中的ip计数和竞标计数，设备计数在进行对数操作处理之后表现出轻微的正相关性，但是由于是在做了对数操作之后才体现，而且从图中可以看到并非很强的相关性，所以保留这三个特征。 针对前述的异常行为，先从原train数据集中的机器人、人类中分别挑选几个样本进行追踪观察他们在按照bidders分组后的统计结果，对比看看 1cnt_bidder.to_csv('cnt_bidder.csv') 12345678# trace samples,first 2 bots, last 2 humenindices = ['9434778d2268f1fa2a8ede48c0cd05c097zey','aabc211b4cf4d29e4ac7e7e361371622pockb', 'd878560888b11447e73324a6e263fbd5iydo1','91a3c57b13234af24875c56fb7e2b2f4rb56a']# build a DataFrame for the choosed indicessamples = pd.DataFrame(cnt_bidder.loc[indices], columns = cnt_bidder.keys()).reset_index(drop = True)print \"Chosen samples of training dataset:(first 2 bots, last 2 humen)\"display(samples) Chosen samples of training dataset:(first 2 bots, last 2 humen) bidder_type auc_mean auction_c bids_c countries_c devices_c ip_c url_c robot1 3.190981 5.594711 8.785692 4.174387 6.011267 8.147578 7.557995 robot2 2.780432 4.844187 7.624619 2.639057 3.178054 5.880533 1.609438 human1 0.287682 1.098612 1.386294 1.098612 1.386294 1.386294 0.000000 human2 0.287682 2.890372 3.178054 1.791759 2.639057 2.995732 0.000000 使用seaborn来对上面四个例子的热力图进行可视化，看看percentile的情况 123456789101112import matplotlib.pyplot as pltimport seaborn as sns# look at percentile rankspcts = 100. * cnt_bidder.rank(axis=0, pct=True).loc[indices].round(decimals=3)print pcts# visualize percentiles with heatmapsns.heatmap(pcts, yticklabels=['robot 1', 'robot 2', 'human 1', 'human 2'], annot=True, linewidth=.1, vmax=99, fmt='.1f', cmap='YlGnBu')plt.title('Percentile ranks of\\nsamples\\' feature statistics')plt.xticks(rotation=45, ha='center'); auc_mean auction_c bids_c \\ bidder_id 9434778d2268f1fa2a8ede48c0cd05c097zey 94.9 94.6 97.0 aabc211b4cf4d29e4ac7e7e361371622pockb 92.4 87.2 92.3 d878560888b11447e73324a6e263fbd5iydo1 39.8 30.4 30.2 91a3c57b13234af24875c56fb7e2b2f4rb56a 39.8 60.2 53.0 countries_c devices_c ip_c url_c bidder_id 9434778d2268f1fa2a8ede48c0cd05c097zey 95.4 95.6 96.7 97.4 aabc211b4cf4d29e4ac7e7e361371622pockb 77.3 63.8 84.8 50.3 d878560888b11447e73324a6e263fbd5iydo1 48.8 38.7 34.2 13.4 91a3c57b13234af24875c56fb7e2b2f4rb56a 63.7 56.8 56.2 13.4 由上面的热力图对比可以看到，机器人的各项统计指标除去商品类别上的统计以外，均比人类用户要高，所以考虑据此设计基于基本统计指标规则的基准模型，其中最显著的特征差异应该是在auc_mean一项即用户在各个拍卖场的平均竞标次数，不妨先按照异常值处理的方法来找出上述基础统计中的异常情况 设计朴素分类器由于最终目的是从竞标者中寻找到机器人用户，而根据常识，机器人用户的各项竞标行为的操作应该比人类要频繁许多，所以可以从异常值检验的角度来设计朴素分类器，根据之前针对不同用户统计的基本特征计数情况，可以先针对每一个特征找出其中的疑似异常用户列表，最后整合各个特征生成的用户列表，认为超过多个特征异常的用户为机器人用户。 123456789101112# find the outliers for each featurelst_outlier = []for feature in cnt_bidder.keys(): # percentile 25th Q1 = np.percentile(cnt_bidder[feature], 25) # percentile 75th Q3 = np.percentile(cnt_bidder[feature], 75) step = 1.5 * (Q3 - Q1) # show outliers # print \"Data points considered outliers for the feature '&#123;&#125;':\".format(feature) display(cnt_bidder[~((cnt_bidder[feature] &gt;= Q1 - step) &amp; (cnt_bidder[feature] &lt;= Q3 + step))]) lst_outlier += cnt_bidder[~((cnt_bidder[feature] &gt;= Q1 - step) &amp; (cnt_bidder[feature] &lt;= Q3 + step))].index.values.tolist() 再找到各种特征的所有可能作为‘异常值’的用户id之后，可以对其做一个基本统计，进一步找出其中超过某几个特征值均异常的用户，经过测试，考虑到原始train集合里bots用户不到5%，所以最终确定以至少存在1个特征值异常的用户作为异常用户的一个假设，由此与test集合里的用户进行判断，可以得到一个用户子集，将这部分用户判定为朴素分类器的作弊用户判定结果。 12345# print len(lst_outlier)from collections import Counterfreq_outlier = dict(Counter(lst_outlier))perhaps_outlier = [i for i in freq_outlier if freq_outlier[i] &gt;= 1]print len(perhaps_outlier) 214 123# basic_pred = test[test['bidder_id'].isin(perhaps_outlier)]['bidder_id'].tolist()train_pred = train[train['bidder_id'].isin(perhaps_outlier)]['bidder_id'].tolist()print len(train_pred) 76 设计评价指标根据前面数据探索知本实验中的数据集的正负例比例约为19:1，有些失衡，所以考虑使用auc这种不受正负例比例影响的评价指标作为衡量标准，现针对所涉及的朴素分类器在原始训练集上的表现得到一个基准得分 1234567from sklearn.metrics import roc_auc_scorey_true = train['outcome']naive_pred = pd.DataFrame(columns=['bidder_id', 'prediction'])naive_pred['bidder_id'] = train['bidder_id']naive_pred['prediction'] = np.where(naive_pred['bidder_id'].isin(train_pred), 1.0, 0.0)basic_pred = naive_pred['prediction']print roc_auc_score(y_true, basic_pred) 0.54661464952 在经过上述对基本计数特征的统计之后，目前尚未针对非类别特征：时间戳进行处理，而在之前的数据探索过程中，针对商品类别和国家这两个类别属性，可以将原始的单一特征转换为多个特征分别统计，此外，在上述分析过程中，我们发现针对用户分组可以进一步对于拍卖场进行分组统计。 对时间戳进行处理 针对商品类别、国家转换为多个类别分别进行统计 按照用户-拍卖场进行分组进一步统计 对时间戳进行处理主要是分析各个竞标行为的时间间隔，即统计竞标行为表中在同一拍卖场的各个用户之间的竞标行为间隔 然后针对每个用户对其他用户的时间间隔计算 时间间隔均值 时间间隔最大值 时间间隔最小值 1234567891011121314151617181920212223from collections import defaultdictdef generate_timediff(): bids_grouped = bids.groupby('auction') bds = defaultdict(list) last_row = None for bids_auc in bids_grouped: for i, row in bids_auc[1].iterrows(): if last_row is None: last_row = row continue time_difference = row['time'] - last_row['time'] bds[row['bidder_id']].append(time_difference) last_row = row df = [] for key in bds.keys(): df.append(&#123;'bidder_id': key, 'mean': np.mean(bds[key]), 'min': np.min(bds[key]), 'max': np.max(bds[key])&#125;) pd.DataFrame(df).to_csv('tdiff.csv', index=False) 1generate_timediff() 后续内容请移步使用机器学习识别出拍卖场中作弊的机器人用户(二)","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"平坦世界","date":"2017-05-29T08:30:23.000Z","path":"2017/05/29/世界是平的/","text":"这次给大家推荐一本书——“The world is flat”（《世界是平的》），刚刚看完了一遍，感觉这本书的视角很广阔，让我看到自己受限于地理或者意识因素所看不到的一些其实就实实在在发生在身边的变化，私以为，作为即将毕业的大学生而言，是很有必要一读的。 【这本书面世其实快10年了，但是书中所述的一些变化我们或经历过，或忽略掉了，所以即使放在现在读，也是很有前瞻性的。特别由于作者本身是记者，写作风格还是颇为轻松地，所以，读起来感觉像是跟着作者开着上帝视角实现了所谓“世界那么大，我想去看看”的成就一般。】 此书整体大概是由世界变平坦的种种例证、现象到作者所认为使世界变平坦的一些因素再到各个“成员”大至国家公司、小至每一个个体在这平坦世界里的角色和相互影响，最后是平坦世界所带来的或即将造成的可能负面影响和威胁来逐层叙述的。我想先就宏观角度简单记叙下书中的一些观点，然后谈一下书中所说而我也感觉到的身边的一些变化，最后就自己而言随意谈一下想法。 就宏观而言，书中让我印象最深的是全球化的重中体现，首先是全球合作的一些案例，这个曾经在《互联网时代》里也看到过，比如现在的波音飞机的组装以及各零件的制造是分散到世界各个国家各个地区去做的，然后又井然有序的由各个地方汇聚到一个点组装成型，书中还举得一个例子是沃尔玛的生产线，和前者类似，也是一种像分布式的结构，这种做法极大地提高了效率并降低了成本，而其中最让我目眩神迷的是这么个巨大的整体居然能够这么一致的展开协同工作；另外一点是关于离岸外包，这里就不得不提到一个国家是印度，或者说印度的城市班加罗尔，看了这本书我感觉像亲身去看一下，所谓外包，就是将本国家的一些相对而言中低端而要耗费大量人力的基础工作交给其他国家去做，这里你也许会想到所谓的”Made in China”，诚然，中国也是外包的一个很好的例子，换做以前我会觉得嗤之以鼻，感觉我们国家就老是像给美国之类的国家打工一样，其实，分析一下，就目前而言，这个是双赢的，美国那边自不用说，花一分美国工人的钱在中国或者印度雇佣到6~8个同样水平的技术工人，极大地降低了成本，而且这里面不得不说有个很有意思的东西是——时差，因为有些工作在美国的白天又美国人做，晚上就交给印度人（印度正在白天）做，极大地提升了效率，这个我在电影《贫民窟的百万富翁》里看到过的一个场景就是印度人通过远程监控摄像头替美国人监控停车场让我印象深刻，那么对于印度和中国这些国家呢，好处在哪？其一就是增多了工作岗位，就印度而言的话，由于现在印度培养了大量的理工学生，而毕业后以前只能去国外谋出路，否则在国内一般只能成为出租车司机，而经过外包，他们可以在本土就进行软件开发的工作，虽然工资不如美国人，但是相对于自己本身而言，他们的生活已经得到不小的提升了，另外，通过这种形式，印度和中国还可以学习一些国外的技术，甚至是抢占国外的市场，书中后面有个例子是中国生产的埃及传统的灯几乎快要垄断埃及的市场，因为在技术上有所突破，而且价格也低廉。 当然，以上只是从书中看到的知识，也许我以后还需要实地考察一番看看实际情况是否真如作者所述，不过，对于我们国家，我还是希望抱有乐观的态度去看他的发展，相信他会越变越好。 就身边的所见所闻而言，我印象比较深的是一个 距离 的问题，和书中作者一次下飞机乘出租车的经历一般，在乘车那段时间里，司机一直在通过蓝牙耳机聊天，车上开着导航，播放着电影，而作者在自己的笔记本上整理文章以及收发e-mail，用作者的话说，在那一个小时里他们同时做了很多事，却几乎没有交流，作者甚至猜想，也许司机正和远在另一个国度的父母通话呢。这大概就是目前发生在我们身边的一个尴尬境地了，一方面，技术的发展拉近了我们的距离，而另一面却又使我们的距离变得遥远，它拉近了我们和处在不同空间的亲人之间的距离，却又在此时此地的就在你眼前的我面前树起了一道屏障。我想起了高中语文老师发的一篇文章里对动车、高铁上的年轻人乘客们的描述，确确实实的感受到这一真切的现象，大家一上车就戴上耳机，或插入ipad，或插入iphone，而彼此之间却没有交流，这和绿皮火车上的情景完全不同，也是值得我们反思的一件事。 最后我想简单谈一下自己的一些想法。首先关于教育和竞争方面，这里还得扯下印度，书中说道，班加罗尔的接线员晚上为美国（白天）的乘客们进行咨询和失物找回工作，晚上还会自己学习一些知识，攻读一些学位什么的，顿时感到一种压迫感。从前经常听到政治老师说美国家长告诫自己的孩子说快吃饭，不然中国和印度的孩子就把你的饭给吃了或者快努力学习，中国或印度的孩子快把你的饭碗抢走了之类的。而现在是大家都站在几乎同一个平台上竞争，前面我们需要追赶美国，而同时，印度的青年们却也在旁虎视眈眈。而身处计算机专业这一日新月异，竞争更加残酷的环境下，我一面感到威胁，一面感到兴奋，兴奋的是，试想一下你即将站在一个大舞台上，和来自不同城市乃至不同国家的人角逐，而威胁在于，你们本身的水平是不一样的，就我而言，在我将要踏上这个舞台的那一天，我不仅会和同班同学竞争，稍远一点还有来自全国各大高校的强者，再远一点还有世界其他角落的高手出没，而要想不被碾成炮灰，或者说不被这一变平的趋势所冲倒的话，我就得不断地去吸收和学习新的技术、新的知识，还要进一步强化自己的全局意识，做到让自己所做的事无可替代，也就不会被”外包”掉。另外一点就是之前实习时听到负责人说现在的联系方式太多了，又是电话、又是微信、QQ的，反而造成了联系上的障碍，以前我们只有电话时一般就通过电话联系，而现在常常是不同人有不同的习惯，你得把这些一股脑全开着，否则搞不好会错过重要讯息，我常常在想，世界是多元化的，但本意是方便我们交流的工具最后却慢慢成了束缚我们的枷锁，从这个角度来看，人类到底是进步了，还是退化了？","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"Economy","slug":"Economy","permalink":"http://yoursite.com/tags/Economy/"}]},{"title":"面壁者，我是你的破壁人","date":"2017-05-29T08:28:53.000Z","path":"2017/05/29/三体/","text":"对一个人的评判若不结合他所处的时代都是不公允的。 终于看完了《三体》全集，这部科幻小说给人的感觉真像作者自己命名的“地球往事”，像一本历史书，以宇宙为坐标，以光年为刻度，读罢，借用一句话“科幻小说虽然尽是对于未来的想象，但我们探索的一直是人的内心”。 情节上我就不剧透太多了，我想谈谈《三体》里的几个角色。 首先是罗辑，面壁者、执剑人、守墓人是他不同时期的身份，他是一个充满传奇色彩的人，用他自己的话是一个及时行乐的人，对其他的人、其他的事都不怎么上心的人。但是我看到的是他受人胁迫而终于接受面壁者身份时的无奈和挣扎；在冬眠复苏时被人当做笑话时的淡然、洒脱，再一次被推上历史转折点时的坚韧以及作为执剑人半个多世纪面壁时的狠厉。我得承认全书给我印象最深的是《黑暗森林》最高潮的部分，当罗辑站在自己挖掘的坟墓边对着虚空中的智子亮出最后底牌的那一刻；我真的有一种作为人类终于得救了、终于又有了希望的感觉。命运喜欢捉弄罗辑，在他吊儿郎当、无所事事时对他施以面壁者的诅咒，在他拯救了全人类时却让人类对他报以敌视；但罗辑不在乎这些，他只在乎自己所爱的那个女孩的幸福。 然后是《死神永生》里的女主角程心。之前有看一个《x分钟读完三体》的短片，当时发现弹幕上对于程心大多数是一片骂声，随处可见诸如“程圣母毁灭世界”的字眼。不得不说在看《死神永生》的中后段我也一直对于程心是恨得咬牙切齿的，实话说，我讨厌她的不作为和自以为是爱与善的化身所做的所有决定，讨厌她毁灭了地球、害人类灭亡，但后来一想，我其实是开了上帝视角在看这个角色，其实回过头来看，程心是当前普通人类的典型，甚至，是大众中向善的群体的典型，她一直秉持着自己的责任，逆来顺受；她是勇敢的，敢于牺牲自己，但是，她的能力是不够的。整部小说看下来程心大部分时间处于冬眠状态，而经常是其他人帮她做足了准备，然后突然一下把她推到全人类命运决策的位置，可是却没有想过她准备好了没有，结果她只能依据自己当前的认识做出最佳选择，然后成了公众的替罪羊，其实换位思考，我们估计在那个节点做的更差；不信你把程心放到不同的时间段看公众对于她作为执剑人的那几分钟的态度的不停反复就可以看出来了，大众只是需要一个为自己顶包的人，然后好像自己就可以置身事外了。 如果说程心更多的依靠感性来做判断，那么就不得不提到维德了，这个极端理性到冷酷的男人。最开始他对于程心的捉弄确实让我很不爽，从发射云天明的大脑一直到后面他不择手段的想要暗杀程心竞争执剑人。但读到后面你会发现维德其实是个表里如一的人，对于自己认定的事他能坚持到底，不择手段的坚持到底。 所以不存在这么一个人，把他放到任何一个时空他的判断都是对的，或者说大众对于个体的判断其实也不能代表什么，一个人只需坚持自己所相信的，在乎自己所爱的，到最后一刻，就够了。 各位面壁者们，准备好做自己的破壁人了么？","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"Novel","slug":"Novel","permalink":"http://yoursite.com/tags/Novel/"}]},{"title":"平凡的世界，不平凡的凡人","date":"2017-05-29T08:27:42.000Z","path":"2017/05/29/平凡的世界/","text":"一直想读这套书，在临近毕业时还专门从朋友那收藏了一套，借着在MEB的机会终于是忙里偷闲的看完了，虽意犹未尽，却也感到一种平和和满足。 如果要用一句话概括，《平凡的世界》写的是爱情和面包。 书中描述的爱情种类比较多，我就简单挑给我印象最深的三个来说说吧。首先是润叶和少安的爱情，那句诗里写的“郎骑竹马来，绕床弄青梅”大抵描述的就是这么一种爱情。少安和润叶是两小无猜的一对童年玩伴，随着两家各自发展出的家庭背景的差异却也慢慢在双方之间产生了巨大的鸿沟，一个留校任教成了城里人，另一个却回家务农，给家里分担压力。诚然，你可以说是少安主观上的懦弱导致了最终两人的分离和三个家庭的阴影和痛苦，但是个人认为，在那么一种背景下，就一个男人而言是不得不考虑这些东西的。书中虽说如果两人身份对调也许就没什么问题，但我觉得即使两人身份对调而彼此相爱也还是要考虑双方的家庭背景，这里我倒不是在推崇门当户对的老观念，只是客观的分析一下，毕竟，我一直认为，爱情是建立在一定的物质基础之上，不是简单的彼此相爱，毕竟，爱情可能产生在一瞬间，但是维系这份感情却需要长期的努力，也就需要一定的物质作为基础，而若双方各自条件不对等，时间长了或多或少会产生并积累一些问题。由此看来，古人所推崇的门当户对确有一定道理。 当然，我不否认灰姑娘和王子的童话故事，但是即使在灰姑娘的故事里，灰姑娘也是有仙子为她做后盾的而且她本身有相对应的素质作为支撑。 扯远了，其实书中倒也有这么一对，就是少平和晓霞。我对这一对的定义是充满理想和浪漫主义色彩的情侣，两人由最初的共同爱好或说理想发展出深厚的友谊，继而在不同的人生道路上产生的共鸣引导出爱情，说实话，看到这里我已经觉得有点过于理想化了，到后来晓霞拒绝高富帅依然坚定地爱着少平时，我也深深被打动，从心理上我是希望看到这一对能有美好的结局的，我原希望会是少平从煤炭场最后走出来完成逆袭然后迎娶白富美的励志故事（原谅程序员的屌丝气息），不过感觉依作者的套路势必会再给这对有情人制作一些波折，可是，万万没想到，情节发展居然是晓霞的牺牲，当时感觉很难接受这一事实，毕竟这朴实的书中的一朵充满浪漫主义色彩的绚丽花朵竟以这种方式枯萎。 最后一对就是润叶和向前，很难用爱情定义这一对，因为故事前段一直是向前的单恋，即使两人的结合也是润叶的自我牺牲和自我放逐，结果是长期的互相折磨到最后，然后转折点在向前的事故所带来的残疾，润叶因为内疚而突然回头，两人复合（个人觉得此处作者不够深入），然后两人开始真正的婚姻生活，不过也很难说是爱情，连文中也提及此时的润叶更像是一个虔诚的修女，给予向前的除了妻子的责任另外确是一种变相的母性关怀，姑且称之为——怜悯或者同情吧，其实我是很喜欢润叶这么个姑娘的，敢爱敢恨，在和少安的爱情中表现出勇敢，却又太懂事，这样矛盾的性格造成她最终的屈服，与其说她是嫁给了一个自己不爱的男人，倒不如说她是嫁给了生活。但我又不能怪罪向前，爱情这东西说不清道不明的，很多时候其实就是，我喜欢你然而我并不能理解你为何不喜欢我，其结果至少有一人受伤。 以上是对书中描述的几种爱情的粗浅理解，当然，作为一个纯理论家，请相信，我说的每一句话都是谎话。 还是来谈谈另一条线——面包吧。 全书主要描述了双水村这么个平凡的小天地里一个又一个像你像他像那路边野花的平凡人的平凡而又不平凡的生活。总体来说，大的背景是变化莫测的，从文革末的动乱年代到改革开放的小康生活，立足在这么一个大环境下，每个人的命运都不能孤立的来看，刚开始看这本书的时候我常常随着故事的情节发展给故事中的人下定义是好是坏是对是错，后来随着作者时不时的客观分析才发现自己的浅薄，其实就像我们所生活中的生活，它不像我们做的卷子上的题目，没有什么绝对的对错，每个人只是在给他演出的那么一段镜头里或优雅谢幕或落荒而逃，导演却是生活。也许我仍不能从简单的评判一个人是好人坏人，他做的一件事是对的还是错的这么一种观念里走出来，但是从这本书里我看到了生活这辆无缰马车上的形形色色人物的身不由己，同时，我也看到他们的奋斗，一次一次被生活喊cut却又一次一次带着微笑或者含着泪水伤痕累累的跑龙套，在这场戏里，没有主角，或者说每个人都是主角，轮番上场。 每一个人物都有其鲜明特色又有其局限性，之前有一朋友说他觉得书中把任务刻画得太脸谱化了，我觉得是有这么一点，但是大体而言路遥对人物的刻画还是很鲜明，很丰满而有血有肉的。我欣赏少安在物质生活追求上的不屈不挠精神，却又为他有时的偏执扼腕，对于他在爱情上的胆怯以及一意孤行而叹息；我喜欢少平这么一个脚踏实地的为生活战斗，为自己为他人着想的‘精神贵族’。他太可爱了，看这本书真的就是看着这个少年的成长，看着他咀嚼生活，消化生活，不过我希望他能更勇敢，有时能多为自己考虑一点，勇敢的追逐自己的幸福；至于晓霞，自不用说，感觉书中几乎刻画成女神一般的存在了，可惜也由于她的冒险精神和舍己为人的精神而香消玉殒；润叶，真是一个让人忍不住心疼却又忍不住尊敬的女性，她是那么为大家着想，甚至于可以牺牲自己；还有孙玉厚，这个伟大的老父亲，虽然书中没太多专门的笔墨，但是仍可以看见少安、少平甚至兰花从他身上传承的坚韧的基因，也许从早期发狠供玉亭念书也能窥见一二，另外，这位老人在儿子追求自己目标时默默支持，在孩子们遭受不幸时却总是挺身而出，实在令人感动；其他还有用情专一的向前，逛鬼王满银，懂事的兰香等等，像你我身边的每一个人一样在这个平凡的世界里献上了这精彩的演出，然后帷幕缓缓落下，他们又消失在人群中，从此你我看到的身边的每一个人都似曾相识。 以上大概就是这粗略的第一次阅读《平凡的世界》的一点不吐不快的想法，虽然书有一点点小瑕疵比如各个主角的结局实在难以让我这么一个习惯了happy ending的人满意，比如中间外星人的情节有点乱入的感觉，不过还是强力推荐这本书，怎么说呢，在看这本书的过程中，我常常有一种踏实感，活着的踏实感，在我合上书页，回想起书中一些情节时情不自禁的感受到生活，从某种程度上，它减轻了我若有若无的虚无感。","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"Novel","slug":"Novel","permalink":"http://yoursite.com/tags/Novel/"}]},{"title":"韶华倾负，难舍皮囊","date":"2017-05-29T08:22:21.000Z","path":"2017/05/29/韶华倾负，难舍皮囊/","text":"利用在动车上的四个小时看完了《皮囊》（除去中间15分钟和邻座美女搭讪的时间），感觉是很不错的一本书，不同的章节写的不同的故事，如果你有时间不妨通篇阅读一下，如果没有时间不妨听我说一说。 个人觉得，前五章的主题关于生离，关于死别。 第一章和书同名是为《皮囊》，讲述的是99岁的阿太。 我们的生命本来多轻盈，都是被这肉体和各种欲望的污浊给拖住。阿太，我记住了。“肉体是拿来用的，不是拿来伺候的。” 没文化的神婆阿太穷极一生都在利用自己的那副皮囊，甚至要求周围的人也学会去利用好这幅皮囊，所以她把年幼的孩子扔进海里让他学游泳，所以她即使白发人送黑发人也保持一副让人不解的嘲弄表情，或许在她看来，失掉这皮囊，灵魂最终的归宿反而是自由吧。 但这并不意味着阿太是那种不在乎生死的人，相反，我觉得她是站在更高的角度看待生死这个问题的人，而这也许隐隐约约影响着身边的人。所以作者后来写下这么一句话： 从小我就喜欢闻泥土的味道，也因此其实从小我不怕死，一直觉得死是回家，是入土。我反而觉得生才是问题，人学会站立，是任性地想脱离这土地，因此不断向上攀爬，不断抓取任何理由——欲望、理想、追求。然而，我们终究需要脚踏着黄土。在我看来，生是更激烈的索取，或许太激烈的生活本身就是一种任性。 然后是第二章——《母亲的房子》写的是母亲，房子还有爱情。 每个人都会有自己的执着，而那种执着最后就物化成某种具体的事物，然后进一步，有的化成妥协后的一道疤痕，有的成为穷极一生的执念。而文中的母亲显然是后者，而她所执着的是那所房子，是年轻时的父亲曾允诺的房子，以一种执着的近乎任性的方式。从最开始的一修再修，这所母亲执意要修建的房子联系着一家人的生活和命运，从拮据的生活到后来周围人的不解最后甚至家人的埋怨，母亲一度支撑不下去却又不甘心，最难过的日子里甚至一家人想要一起求死，直到后来身为“不合格的一家之主”的我终于理解了母亲： 事实上，知道母亲坚持要建好这房子的那一刻。我才明白过来，前两次建房，为的不是她或者我的脸面，而是父亲的脸面——她想让父亲发起的这个家庭看上去是那么健全和完整。 这是母亲从老没表达过，也不可能说出口的爱情。 另外，关于母亲的故事在《我的神明朋友》里会继续讲述，你会看到更多母亲身上的坚韧。 然后《残疾》讲述的是父亲中风后的家里的种种艰辛和一家人和生活的抗争。 不同于以前语文课本里父亲那种高大、沉默的形象，《皮囊》里的父亲首先是一个中风而偏瘫了的父亲，而后是一个曾经混黑社会呼风唤雨的混混头子，中风前后的心理落差造成了父亲接下来的一系列变化，他从假装坚强，希望靠对时间的严苛要求和每天的活动恢复到以前的健壮身体，于是一家人默契的相互演着戏，却也过了一段幸福的时间，直到被一场意料之中的台风摧毁。 虽然知道根本不是台风的错。那结局是注定的，生活中很多事情，该来的会来，不以这个形式，就会以那样的形式。但把事情简单归咎于我们无能为力的某个点，会让我们的内心可以稍微自我安慰一下，所以，我至今仍愿意诅咒那次台风。 那场台风刮倒的不仅仅是父亲，更打碎了父亲伪装的坚强和一家人匆忙写就的生活的剧本。自此，父亲进入了第二个状态，先是放弃了坚强，呈现出一副自暴自弃的样子期盼着死亡。而后又进入下一阶段，甚至于舍弃了父亲的身份，像个不懂事的孩子一样撒娇、任性，却不再期盼死亡。 但是父亲的故事并没有完，在《重症病房里的圣诞节》，作者以在重症病房里的看护家属的视角半写实半轻描淡写地描述了退去浮华身份后的种种生离死别。 疾病在不同的地方找到了他们，即使他们当时身处不同的生活，但疾病一眼看出他们共同的地方，统一把他们赶到这么一个地方圈养。 在这一章里，作者刻画了几个不同的人物，有乐观的病人，有刻意显得刻薄的医护人员，有早熟的同龄孩子。在医院这么一个特殊的小世界里，赤裸的生长着。有几个事件的刻画让我印象深刻，比如‘我’对于医院里电梯的描写，对于走过一间间病房核查之前的病人是否还在；比如和同龄的孩子交流，发现相同点，大家拥有相同的眼睛，那是经历过生命消逝后才看得到的眼睛，所以会被一眼看透，而无法交到同龄的朋友，因为他们只有一次也只能有一次痛彻心扉的谈话；比如乐观的病友最后还是被夺走生命后‘我’心态上的变化，印象最深的是那个为父亲在圣诞节违禁燃放烟花的年轻人，只因他父亲要进行手术了，然后他被三个保安团团围住…… 比如，在帮父亲换输液瓶时，会发觉他手上密密麻麻的针孔，找不到哪一寸可以用来插针；比如医生会时常拿着两种药让我选择，这个是进口的贵点的，这个是国产的便宜的，你要哪种？我问了问进口的价钱，想了很久。“国产的会有副作用吗？”“会，吃完后会有疼痛，进口的就不会。”我算了算剩下的钱和可能要住院的时间，“还是国产的吧。” 然后看着父亲疼痛了一个晚上，怎么都睡不着。 在这里，你一不小心留出空当，就会被悲伤占领——这是疾病最廉价、最恼人的雇佣兵。 这种笔触没在重症病房待过的人是写不出来的，那种切实的压抑感和空闲下来被对未来的恐惧和迷茫逼到绝路的走投无路感。 接下来的几章个人觉得是关于青春和梦想的。 首先《张美丽》这一章从这么一个略显俗气的名字开始，从迷信的桃色传说开始记叙了传说中自杀的殉情张美丽，而后记叙了现实中被夹在理想和世俗之间压垮的张美丽，同样是一副让荷尔蒙萌动的青春期少年燥起来的皮囊，演绎了不同的故事。这故事一面让我想起了《搜索》这部电影里舆论的可怕，人们对于未知的事物的不理解演化成的嫉妒和驱逐和以讹传讹造成的悲剧上演。另一面看到的是被遗忘的先驱们，为了自己的理想而头破血流，然后若干年后在残次不齐的字里行间被遗忘或变成谈资。只是人们一同忘掉的，是现在所呈现在眼前的似曾相识。 接下来《阿小和阿小》、《天才文展》记叙的算是童年和成长吧。 关于城市，那是不在城市长大的孩子们小时候的天堂，是他们在电视屏幕上看到的模样，而两个阿小，一个土生土长在小城镇；另一个为即将开始的大城市生活过度而寄居在小城镇。同样的名字，也沉浸在同样的幻想里，只是他们误读了城市，他们以为的城市就真的是电视屏幕上的那样，数不过来的高楼，四六分的香港发饰，衬衫，洁白的牙齿……于是，他们开始注重外表上的模仿和行为上的接近，在我看来，这正是一种青春期的迷茫，对于寻找自我定位时的种种探索，追求那些很酷很与众不同的感觉。但是结局却不尽如意： 大部分人都困倦到睡着了——他们都是一早七点准时在家门口等着这车到市区，他们出发前各自化妆、精心穿着，等着到这城市的各个角落，扮演起维修工、洗碗工、电器行销售、美发店小弟……时间一到，又仓皇地一路小跑赶这趟车，搭一两个小时回所谓的家，准备第二天的演出。 他们都是这城市的组成部分。而这城市，曾经是我们在小镇以为的，最美的天堂。他们是我们曾经认为的，活在天堂里的人。 而《天才文展》里的文展则代表着一种有远大抱负却囿于家庭环境的早熟的年轻人，也许我们都碰到过像这样的人，以一种居高临下的姿态看着你，对于自我有着严苛的要求，希望通过一项项计划通来向别人证明自己，来狠狠地扇曾经看不起自己的人一耳光。 我才明白，那封信里，我向文展说的“小时候的玩伴真该一起聚聚了”，真是个天真的提议。每个人都已经过上不同的生活，不同的生活让许多人在这个时空里没法相处在共同的状态中，除非等彼此都老了，年迈再次抹去其他，构成我们每个人最重要的标志，或许那时候的聚会才能成真。 然而结局却是悲剧的，‘我’最终过上了文展所希冀的生活而遭到文展的嫉妒和排斥，令人吃惊的是‘我’却也产生了一些负罪感。就像电视剧里经常碰到的桥段，“如果不是因为当初XXX，你现在所有的一切本该是我的。”其实这何尝不是自我欺骗呢，就算自我假设的前提条件不成立，结局却不见得会不一样，失败者把失败原因归结到自己以外的事物上注定还是会重蹈覆辙。 然后是《厚朴》描述的这么一个可爱的男生，英文名又是hope，伴着激情和叛逆的形象出场，不得不说让我想起身边一些充满朝气和正能量的人，平时还是很羡慕他们的，不过厚朴是不同的，开始我也很希望看看他所谓的“务虚”能走出一片天地来，俗话说就是理想主义者，因为我感觉自己更类似于作者。不过可惜的是最后结局让人痛心，像作者所形容的 不清楚真实的标准时，越用力就越让人觉得可笑。 不合时宜的东西，如果自己虚弱，终究会成为人们嘲笑的对象，但有力量了，或坚持久了，或许反而能成为众人追捧的魅力和个性——让我修正自己想法，产生这个判断的，是厚朴。 他以为自己做着摧毁一切规矩的事情，但其实一直活在规矩里。我以为自己战战兢兢地以活在规矩里为生活方式，但其实却对规矩有着将其彻底摧毁的欲望。 所以厚朴所表现出来的其实是一种沉浸在自己构造的幻想世界里而最终难以回到现实，他不愿接受自己失败了的现实，只是表现的叛逆。但话说回来我是希望能看到一种理想主义者最终实现理想的故事，我相信如果有那么一定会是精彩的。 说到这里我甚至有一种奇怪的感觉，是否厚朴其实就是作者的另一面，只是最终发现自己其实是在规则里画着圈而随着屈服而死掉的那部分。 最后几章主要简单谈了些关于城市、理想之类的话题，印象较之之前的几章倒是没那么深了，截取一段如下 只是我觉得城市不好。特别是中国的城市不好。厦门和中国大部分城市的建设都有个基础——人家国外的城市是怎么样的，以及人们该怎么被组织的，然后再依据这样的标准建设。中国近代的城市不是长出来的，不是培植出来的，不是催生出来的，而是一种安排。因为初期必然要混乱，所以中国的城市也表现出强大的秩序意识，人要干吗，路要怎么样。生长在这样环境里的人，除了维护秩序或者反抗秩序，似乎也难接受第二层次的思维了。 回头来看，几篇文章倒确实很契合《皮囊》这么一个主题，就像开头所述的，灵魂离不开皮囊，无论你如何讨厌自己的这幅皮囊，你的灵魂也得附庸其下。皮囊往小的说就是阿太所谓的驱壳和父亲半瘫的残疾，往大了说是一个人所处的位置，他背后的故事，就像母亲的那所房子，阿小的香港梦，厚朴的架子鼓。过去我常常看不懂电视剧里有些人的无可奈何，一厢情愿的认为这么容易的事换做是我直接三下五除二就解决了，实际情况却是不要以好坏善恶对错来划分人群，你看到的只是他当前展现给你的，你看不到的是他这一举动后面的挣扎，人是不能孤立和剥离来看的，当他站在你面前时，你得看到他的背景。 我常对朋友说，理解是对他人最大的善举。当你坐在一个人面前，听他开口说话，看得到各种复杂、精密的境况和命运，如何最终雕刻出这样的性格、思想、做法、长相，这才是理解。而有了这样的眼睛，你才算真正“看见”那个人，也才会发觉，这世界最美的风景，是一个个活出各自模样和体系的人。 如果每个人都是一段程序的话，那么我们在和不同的人交往的过程中就有意无意的修改了他们的代码，从此也许他们身上就带着你的痕迹；而身边亲近的人，更像是写进你基因里的代码段，在某个特定的时间里被激活，让你似曾相识，让你视线模糊。 无论如何，请带着这副皮囊好好生活。","tags":[{"name":"Reading","slug":"Reading","permalink":"http://yoursite.com/tags/Reading/"},{"name":"Novel","slug":"Novel","permalink":"http://yoursite.com/tags/Novel/"}]}]